{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9f75e7",
   "metadata": {
    "papermill": {
     "duration": 0.050641,
     "end_time": "2021-12-28T14:41:56.802315",
     "exception": false,
     "start_time": "2021-12-28T14:41:56.751674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Import of Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19605e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: statsmodels in ./.local/lib/python3.10/site-packages (0.13.5)\n",
      "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.5.2)\n",
      "Requirement already satisfied: patsy>=0.5.2 in ./.local/lib/python3.10/site-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (21.3)\n",
      "Requirement already satisfied: scipy>=1.3 in ./.local/lib/python3.10/site-packages (from statsmodels) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from statsmodels) (1.23.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging>=21.3->statsmodels) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->statsmodels) (2022.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e631f1b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-28T14:41:56.910598Z",
     "iopub.status.busy": "2021-12-28T14:41:56.908741Z",
     "iopub.status.idle": "2021-12-28T14:41:58.883849Z",
     "shell.execute_reply": "2021-12-28T14:41:58.882967Z",
     "shell.execute_reply.started": "2021-12-28T13:57:06.245140Z"
    },
    "papermill": {
     "duration": 2.030257,
     "end_time": "2021-12-28T14:41:58.884045",
     "exception": false,
     "start_time": "2021-12-28T14:41:56.853788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a077b8c",
   "metadata": {
    "papermill": {
     "duration": 0.048486,
     "end_time": "2021-12-28T14:41:58.982944",
     "exception": false,
     "start_time": "2021-12-28T14:41:58.934458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Import of the CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c578232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.python_history',\n",
       " 'datalogQMLOLD.csv',\n",
       " '.cache',\n",
       " 'summarizer.ipynb',\n",
       " '.jupyter',\n",
       " 'datalog.csv',\n",
       " '.bashrc',\n",
       " 'EDA-EV-ML-Project',\n",
       " '.config',\n",
       " 'Untitled.ipynb',\n",
       " '.bash_logout',\n",
       " '.keras',\n",
       " 'Paraphrase.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " '.ipython',\n",
       " 'QML-benchmarking',\n",
       " 'Untitled Folder',\n",
       " '.bash_history',\n",
       " 'datasets',\n",
       " '.local',\n",
       " '.profile',\n",
       " 'OPenAI.ipynb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path=\"../ej/\"\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681c9fcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:41:59.087350Z",
     "iopub.status.busy": "2021-12-28T14:41:59.086613Z",
     "iopub.status.idle": "2021-12-28T14:41:59.105407Z",
     "shell.execute_reply": "2021-12-28T14:41:59.104749Z",
     "shell.execute_reply.started": "2021-12-28T13:57:08.462419Z"
    },
    "papermill": {
     "duration": 0.0739,
     "end_time": "2021-12-28T14:41:59.105577",
     "exception": false,
     "start_time": "2021-12-28T14:41:59.031677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(path+'/datasets/evCarsData/ElectricCarData_Clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5341b7",
   "metadata": {
    "papermill": {
     "duration": 0.048651,
     "end_time": "2021-12-28T14:41:59.203199",
     "exception": false,
     "start_time": "2021-12-28T14:41:59.154548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Top five rows of the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd63b9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Model</th>\n",
       "      <th>AccelSec</th>\n",
       "      <th>TopSpeed_KmH</th>\n",
       "      <th>Range_Km</th>\n",
       "      <th>Efficiency_WhKm</th>\n",
       "      <th>FastCharge_KmH</th>\n",
       "      <th>RapidCharge</th>\n",
       "      <th>PowerTrain</th>\n",
       "      <th>PlugType</th>\n",
       "      <th>BodyStyle</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Seats</th>\n",
       "      <th>PriceEuro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tesla</td>\n",
       "      <td>Model 3 Long Range Dual Motor</td>\n",
       "      <td>4.6</td>\n",
       "      <td>233</td>\n",
       "      <td>450</td>\n",
       "      <td>161</td>\n",
       "      <td>940</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>D</td>\n",
       "      <td>5</td>\n",
       "      <td>55480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>ID.3 Pure</td>\n",
       "      <td>10.0</td>\n",
       "      <td>160</td>\n",
       "      <td>270</td>\n",
       "      <td>167</td>\n",
       "      <td>250</td>\n",
       "      <td>Yes</td>\n",
       "      <td>RWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>Hatchback</td>\n",
       "      <td>C</td>\n",
       "      <td>5</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polestar</td>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>210</td>\n",
       "      <td>400</td>\n",
       "      <td>181</td>\n",
       "      <td>620</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>Liftback</td>\n",
       "      <td>D</td>\n",
       "      <td>5</td>\n",
       "      <td>56440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BMW</td>\n",
       "      <td>iX3</td>\n",
       "      <td>6.8</td>\n",
       "      <td>180</td>\n",
       "      <td>360</td>\n",
       "      <td>206</td>\n",
       "      <td>560</td>\n",
       "      <td>Yes</td>\n",
       "      <td>RWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>SUV</td>\n",
       "      <td>D</td>\n",
       "      <td>5</td>\n",
       "      <td>68040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Honda</td>\n",
       "      <td>e</td>\n",
       "      <td>9.5</td>\n",
       "      <td>145</td>\n",
       "      <td>170</td>\n",
       "      <td>168</td>\n",
       "      <td>190</td>\n",
       "      <td>Yes</td>\n",
       "      <td>RWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>Hatchback</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>32997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Brand                          Model  AccelSec  TopSpeed_KmH  \\\n",
       "0       Tesla   Model 3 Long Range Dual Motor       4.6           233   \n",
       "1  Volkswagen                       ID.3 Pure      10.0           160   \n",
       "2    Polestar                               2       4.7           210   \n",
       "3         BMW                            iX3        6.8           180   \n",
       "4       Honda                              e        9.5           145   \n",
       "\n",
       "   Range_Km  Efficiency_WhKm FastCharge_KmH RapidCharge PowerTrain  \\\n",
       "0       450              161            940         Yes        AWD   \n",
       "1       270              167            250         Yes        RWD   \n",
       "2       400              181            620         Yes        AWD   \n",
       "3       360              206            560         Yes        RWD   \n",
       "4       170              168            190         Yes        RWD   \n",
       "\n",
       "     PlugType  BodyStyle Segment  Seats  PriceEuro  \n",
       "0  Type 2 CCS      Sedan       D      5      55480  \n",
       "1  Type 2 CCS  Hatchback       C      5      30000  \n",
       "2  Type 2 CCS   Liftback       D      5      56440  \n",
       "3  Type 2 CCS        SUV       D      5      68040  \n",
       "4  Type 2 CCS  Hatchback       B      4      32997  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ebe8454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:41:59.310817Z",
     "iopub.status.busy": "2021-12-28T14:41:59.310095Z",
     "iopub.status.idle": "2021-12-28T14:41:59.331260Z",
     "shell.execute_reply": "2021-12-28T14:41:59.331763Z",
     "shell.execute_reply.started": "2021-12-28T13:57:08.486312Z"
    },
    "papermill": {
     "duration": 0.079729,
     "end_time": "2021-12-28T14:41:59.331933",
     "exception": false,
     "start_time": "2021-12-28T14:41:59.252204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Model</th>\n",
       "      <th>AccelSec</th>\n",
       "      <th>TopSpeed_KmH</th>\n",
       "      <th>Range_Km</th>\n",
       "      <th>Efficiency_WhKm</th>\n",
       "      <th>FastCharge_KmH</th>\n",
       "      <th>RapidCharge</th>\n",
       "      <th>PowerTrain</th>\n",
       "      <th>PlugType</th>\n",
       "      <th>BodyStyle</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Seats</th>\n",
       "      <th>PriceEuro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tesla</td>\n",
       "      <td>Model 3 Long Range Dual Motor</td>\n",
       "      <td>4.6</td>\n",
       "      <td>233</td>\n",
       "      <td>450</td>\n",
       "      <td>161</td>\n",
       "      <td>940</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>D</td>\n",
       "      <td>5</td>\n",
       "      <td>55480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>ID.3 Pure</td>\n",
       "      <td>10.0</td>\n",
       "      <td>160</td>\n",
       "      <td>270</td>\n",
       "      <td>167</td>\n",
       "      <td>250</td>\n",
       "      <td>Yes</td>\n",
       "      <td>RWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>Hatchback</td>\n",
       "      <td>C</td>\n",
       "      <td>5</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polestar</td>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>210</td>\n",
       "      <td>400</td>\n",
       "      <td>181</td>\n",
       "      <td>620</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>Liftback</td>\n",
       "      <td>D</td>\n",
       "      <td>5</td>\n",
       "      <td>56440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BMW</td>\n",
       "      <td>iX3</td>\n",
       "      <td>6.8</td>\n",
       "      <td>180</td>\n",
       "      <td>360</td>\n",
       "      <td>206</td>\n",
       "      <td>560</td>\n",
       "      <td>Yes</td>\n",
       "      <td>RWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>SUV</td>\n",
       "      <td>D</td>\n",
       "      <td>5</td>\n",
       "      <td>68040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Honda</td>\n",
       "      <td>e</td>\n",
       "      <td>9.5</td>\n",
       "      <td>145</td>\n",
       "      <td>170</td>\n",
       "      <td>168</td>\n",
       "      <td>190</td>\n",
       "      <td>Yes</td>\n",
       "      <td>RWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>Hatchback</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>32997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Brand                          Model  AccelSec  TopSpeed_KmH  \\\n",
       "0       Tesla  Model 3 Long Range Dual Motor       4.6           233   \n",
       "1  Volkswagen                      ID.3 Pure      10.0           160   \n",
       "2    Polestar                              2       4.7           210   \n",
       "3         BMW                            iX3       6.8           180   \n",
       "4       Honda                              e       9.5           145   \n",
       "\n",
       "   Range_Km  Efficiency_WhKm FastCharge_KmH RapidCharge PowerTrain  \\\n",
       "0       450              161            940         Yes        AWD   \n",
       "1       270              167            250         Yes        RWD   \n",
       "2       400              181            620         Yes        AWD   \n",
       "3       360              206            560         Yes        RWD   \n",
       "4       170              168            190         Yes        RWD   \n",
       "\n",
       "     PlugType  BodyStyle Segment  Seats  PriceEuro  \n",
       "0  Type 2 CCS      Sedan       D      5      55480  \n",
       "1  Type 2 CCS  Hatchback       C      5      30000  \n",
       "2  Type 2 CCS   Liftback       D      5      56440  \n",
       "3  Type 2 CCS        SUV       D      5      68040  \n",
       "4  Type 2 CCS  Hatchback       B      4      32997  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# trim spaces from values of a column\n",
    "df['Brand'] = df['Brand'].str.strip()\n",
    "df['Model'] = df['Model'].str.strip()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb483147",
   "metadata": {
    "papermill": {
     "duration": 0.04929,
     "end_time": "2021-12-28T14:41:59.432619",
     "exception": false,
     "start_time": "2021-12-28T14:41:59.383329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Finding out the number of null values**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d72eb",
   "metadata": {
    "papermill": {
     "duration": 0.077595,
     "end_time": "2021-12-28T14:42:23.421313",
     "exception": false,
     "start_time": "2021-12-28T14:42:23.343718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Majority of cars have 5 seats "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08186afb",
   "metadata": {
    "papermill": {
     "duration": 0.078401,
     "end_time": "2021-12-28T14:42:23.578470",
     "exception": false,
     "start_time": "2021-12-28T14:42:23.500069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Putting independent variables as x and dependent variable as y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4453e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cells with a particular value\n",
    "# replace a particilar value with another value in all coulmns\n",
    "df.replace(to_replace='-', value=np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11fd9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe65b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Model</th>\n",
       "      <th>AccelSec</th>\n",
       "      <th>TopSpeed_KmH</th>\n",
       "      <th>Range_Km</th>\n",
       "      <th>Efficiency_WhKm</th>\n",
       "      <th>FastCharge_KmH</th>\n",
       "      <th>RapidCharge</th>\n",
       "      <th>PowerTrain</th>\n",
       "      <th>PlugType</th>\n",
       "      <th>...</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Seats</th>\n",
       "      <th>PriceEuro</th>\n",
       "      <th>BrandCode</th>\n",
       "      <th>ModelCode</th>\n",
       "      <th>RapidChargeCode</th>\n",
       "      <th>PowerTrainCode</th>\n",
       "      <th>PlugTypeCode</th>\n",
       "      <th>BodyStyleCode</th>\n",
       "      <th>SegmentCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tesla</td>\n",
       "      <td>Model 3 Long Range Dual Motor</td>\n",
       "      <td>4.6</td>\n",
       "      <td>233</td>\n",
       "      <td>450</td>\n",
       "      <td>161.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>5</td>\n",
       "      <td>55480</td>\n",
       "      <td>30</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>ID.3 Pure</td>\n",
       "      <td>10.0</td>\n",
       "      <td>160</td>\n",
       "      <td>270</td>\n",
       "      <td>167.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>RWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>5</td>\n",
       "      <td>30000</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polestar</td>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>210</td>\n",
       "      <td>400</td>\n",
       "      <td>181.0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>5</td>\n",
       "      <td>56440</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BMW</td>\n",
       "      <td>iX3</td>\n",
       "      <td>6.8</td>\n",
       "      <td>180</td>\n",
       "      <td>360</td>\n",
       "      <td>206.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>RWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>5</td>\n",
       "      <td>68040</td>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Honda</td>\n",
       "      <td>e</td>\n",
       "      <td>9.5</td>\n",
       "      <td>145</td>\n",
       "      <td>170</td>\n",
       "      <td>168.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>RWD</td>\n",
       "      <td>Type 2 CCS</td>\n",
       "      <td>...</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>32997</td>\n",
       "      <td>9</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Brand                          Model  AccelSec  TopSpeed_KmH  \\\n",
       "0       Tesla  Model 3 Long Range Dual Motor       4.6           233   \n",
       "1  Volkswagen                      ID.3 Pure      10.0           160   \n",
       "2    Polestar                              2       4.7           210   \n",
       "3         BMW                            iX3       6.8           180   \n",
       "4       Honda                              e       9.5           145   \n",
       "\n",
       "   Range_Km  Efficiency_WhKm  FastCharge_KmH RapidCharge PowerTrain  \\\n",
       "0       450            161.0           940.0         Yes        AWD   \n",
       "1       270            167.0           250.0         Yes        RWD   \n",
       "2       400            181.0           620.0         Yes        AWD   \n",
       "3       360            206.0           560.0         Yes        RWD   \n",
       "4       170            168.0           190.0         Yes        RWD   \n",
       "\n",
       "     PlugType  ... Segment Seats  PriceEuro  BrandCode  ModelCode  \\\n",
       "0  Type 2 CCS  ...       D     5      55480         30         46   \n",
       "1  Type 2 CCS  ...       C     5      30000         31         33   \n",
       "2  Type 2 CCS  ...       D     5      56440         23          0   \n",
       "3  Type 2 CCS  ...       D     5      68040          2        101   \n",
       "4  Type 2 CCS  ...       B     4      32997          9         78   \n",
       "\n",
       "   RapidChargeCode  PowerTrainCode  PlugTypeCode  BodyStyleCode  SegmentCode  \n",
       "0                1               0             2              7            3  \n",
       "1                1               2             2              1            2  \n",
       "2                1               0             2              2            3  \n",
       "3                1               2             2              6            3  \n",
       "4                1               2             2              1            1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the categorical variables  as numbers for machine leanring algorithms\n",
    "df['BrandCode'] = df['Brand'].astype('category').cat.codes\n",
    "df['ModelCode'] = df['Model'].astype('category').cat.codes\n",
    "df[\"RapidChargeCode\"] = df[\"RapidCharge\"].astype('category').cat.codes\n",
    "df[\"PowerTrainCode\"] = df[\"PowerTrain\"].astype('category').cat.codes\n",
    "df[\"PlugTypeCode\"] = df[\"PlugType\"].astype('category').cat.codes\n",
    "df[\"BodyStyleCode\"] = df[\"BodyStyle\"].astype('category').cat.codes\n",
    "df[\"SegmentCode\"] = df[\"Segment\"].astype('category').cat.codes\n",
    "\n",
    "# change data type of a column from object to float\n",
    "df[\"Efficiency_WhKm\"] = df[\"Efficiency_WhKm\"].astype('float')\n",
    "df[\"FastCharge_KmH\"] = df[\"FastCharge_KmH\"].astype('float')\n",
    "# df['Brand'] = df['Brand'].astype('category')\n",
    "# df['Brand'].astype('category')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91b2c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ 'AccelSec', 'TopSpeed_KmH', 'Range_Km',\n",
    "#        'Efficiency_WhKm', 'FastCharge_KmH', 'RapidCharge',  'Seats', 'PriceEuro', 'BrandCode',\n",
    "#        'ModelCode', 'RapidChargeCode', 'PowerTrainCode', 'PlugTypeCode',\n",
    "#        'BodyStyleCode', 'SegmentCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dde76141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:42:23.742086Z",
     "iopub.status.busy": "2021-12-28T14:42:23.741138Z",
     "iopub.status.idle": "2021-12-28T14:42:23.743608Z",
     "shell.execute_reply": "2021-12-28T14:42:23.744104Z",
     "shell.execute_reply.started": "2021-12-28T13:57:27.872142Z"
    },
    "papermill": {
     "duration": 0.087176,
     "end_time": "2021-12-28T14:42:23.744308",
     "exception": false,
     "start_time": "2021-12-28T14:42:23.657132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x=df[['AccelSec',\n",
    " 'TopSpeed_KmH',\n",
    " 'Range_Km',\n",
    " 'Efficiency_WhKm',\n",
    " 'FastCharge_KmH',\n",
    " 'Seats',\n",
    " 'BrandCode',\n",
    " 'ModelCode',\n",
    " 'RapidChargeCode',\n",
    " 'PowerTrainCode',\n",
    " 'PlugTypeCode',\n",
    " 'BodyStyleCode',\n",
    " 'SegmentCode']]\n",
    "y=df['PriceEuro']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e29e9",
   "metadata": {
    "papermill": {
     "duration": 0.077689,
     "end_time": "2021-12-28T14:42:23.899486",
     "exception": false,
     "start_time": "2021-12-28T14:42:23.821797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Finding out the linear regression using OLS method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5b48fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103 entries, 0 to 102\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Brand            103 non-null    object \n",
      " 1   Model            103 non-null    object \n",
      " 2   AccelSec         103 non-null    float64\n",
      " 3   TopSpeed_KmH     103 non-null    int64  \n",
      " 4   Range_Km         103 non-null    int64  \n",
      " 5   Efficiency_WhKm  103 non-null    float64\n",
      " 6   FastCharge_KmH   103 non-null    float64\n",
      " 7   RapidCharge      103 non-null    object \n",
      " 8   PowerTrain       103 non-null    object \n",
      " 9   PlugType         103 non-null    object \n",
      " 10  BodyStyle        103 non-null    object \n",
      " 11  Segment          103 non-null    object \n",
      " 12  Seats            103 non-null    int64  \n",
      " 13  PriceEuro        103 non-null    int64  \n",
      " 14  BrandCode        103 non-null    int8   \n",
      " 15  ModelCode        103 non-null    int8   \n",
      " 16  RapidChargeCode  103 non-null    int8   \n",
      " 17  PowerTrainCode   103 non-null    int8   \n",
      " 18  PlugTypeCode     103 non-null    int8   \n",
      " 19  BodyStyleCode    103 non-null    int8   \n",
      " 20  SegmentCode      103 non-null    int8   \n",
      "dtypes: float64(3), int64(4), int8(7), object(7)\n",
      "memory usage: 12.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e94edad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:42:24.065703Z",
     "iopub.status.busy": "2021-12-28T14:42:24.065001Z",
     "iopub.status.idle": "2021-12-28T14:42:24.073882Z",
     "shell.execute_reply": "2021-12-28T14:42:24.074416Z",
     "shell.execute_reply.started": "2021-12-28T13:57:27.883536Z"
    },
    "papermill": {
     "duration": 0.097369,
     "end_time": "2021-12-28T14:42:24.074633",
     "exception": false,
     "start_time": "2021-12-28T14:42:23.977264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x= sm.add_constant(x)\n",
    "results = sm.OLS(y,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a186548",
   "metadata": {
    "papermill": {
     "duration": 0.078042,
     "end_time": "2021-12-28T14:42:24.233492",
     "exception": false,
     "start_time": "2021-12-28T14:42:24.155450",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Fitting the model and summarizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bfa7d1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:42:24.397633Z",
     "iopub.status.busy": "2021-12-28T14:42:24.396851Z",
     "iopub.status.idle": "2021-12-28T14:42:24.418903Z",
     "shell.execute_reply": "2021-12-28T14:42:24.418254Z",
     "shell.execute_reply.started": "2021-12-28T13:57:27.904519Z"
    },
    "papermill": {
     "duration": 0.106371,
     "end_time": "2021-12-28T14:42:24.419095",
     "exception": false,
     "start_time": "2021-12-28T14:42:24.312724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>PriceEuro</td>    <th>  R-squared:         </th> <td>   0.831</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   33.60</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 15 Apr 2023</td> <th>  Prob (F-statistic):</th> <td>1.03e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:28:27</td>     <th>  Log-Likelihood:    </th> <td> -1129.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   103</td>      <th>  AIC:               </th> <td>   2287.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    89</td>      <th>  BIC:               </th> <td>   2323.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td> 3.412e+04</td> <td> 3.37e+04</td> <td>    1.011</td> <td> 0.315</td> <td>-3.29e+04</td> <td> 1.01e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AccelSec</th>        <td> -390.8667</td> <td> 1157.720</td> <td>   -0.338</td> <td> 0.736</td> <td>-2691.231</td> <td> 1909.498</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TopSpeed_KmH</th>    <td>  293.0374</td> <td>   86.848</td> <td>    3.374</td> <td> 0.001</td> <td>  120.472</td> <td>  465.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Range_Km</th>        <td>   38.2874</td> <td>   21.751</td> <td>    1.760</td> <td> 0.082</td> <td>   -4.932</td> <td>   81.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Efficiency_WhKm</th> <td>  -67.8685</td> <td>   75.062</td> <td>   -0.904</td> <td> 0.368</td> <td> -217.015</td> <td>   81.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FastCharge_KmH</th>  <td>  -11.2537</td> <td>   14.580</td> <td>   -0.772</td> <td> 0.442</td> <td>  -40.223</td> <td>   17.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Seats</th>           <td>-1.269e+04</td> <td> 2697.447</td> <td>   -4.705</td> <td> 0.000</td> <td>-1.81e+04</td> <td>-7331.658</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BrandCode</th>       <td>   24.8927</td> <td>  186.169</td> <td>    0.134</td> <td> 0.894</td> <td> -345.021</td> <td>  394.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ModelCode</th>       <td>  127.8068</td> <td>   58.840</td> <td>    2.172</td> <td> 0.033</td> <td>   10.893</td> <td>  244.720</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RapidChargeCode</th> <td> 1892.3517</td> <td> 1.15e+04</td> <td>    0.164</td> <td> 0.870</td> <td> -2.1e+04</td> <td> 2.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PowerTrainCode</th>  <td>-6499.3712</td> <td> 2449.677</td> <td>   -2.653</td> <td> 0.009</td> <td>-1.14e+04</td> <td>-1631.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PlugTypeCode</th>    <td> 4386.9908</td> <td> 5259.981</td> <td>    0.834</td> <td> 0.406</td> <td>-6064.479</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BodyStyleCode</th>   <td>  114.4619</td> <td>  783.718</td> <td>    0.146</td> <td> 0.884</td> <td>-1442.769</td> <td> 1671.692</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SegmentCode</th>     <td> 1.017e+04</td> <td> 1841.911</td> <td>    5.519</td> <td> 0.000</td> <td> 6505.808</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>45.142</td> <th>  Durbin-Watson:     </th> <td>   2.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 147.824</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.497</td> <th>  Prob(JB):          </th> <td>7.95e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.048</td> <th>  Cond. No.          </th> <td>1.50e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.5e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              PriceEuro   R-squared:                       0.831\n",
       "Model:                            OLS   Adj. R-squared:                  0.806\n",
       "Method:                 Least Squares   F-statistic:                     33.60\n",
       "Date:                Sat, 15 Apr 2023   Prob (F-statistic):           1.03e-28\n",
       "Time:                        21:28:27   Log-Likelihood:                -1129.3\n",
       "No. Observations:                 103   AIC:                             2287.\n",
       "Df Residuals:                      89   BIC:                             2323.\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const            3.412e+04   3.37e+04      1.011      0.315   -3.29e+04    1.01e+05\n",
       "AccelSec         -390.8667   1157.720     -0.338      0.736   -2691.231    1909.498\n",
       "TopSpeed_KmH      293.0374     86.848      3.374      0.001     120.472     465.602\n",
       "Range_Km           38.2874     21.751      1.760      0.082      -4.932      81.507\n",
       "Efficiency_WhKm   -67.8685     75.062     -0.904      0.368    -217.015      81.278\n",
       "FastCharge_KmH    -11.2537     14.580     -0.772      0.442     -40.223      17.715\n",
       "Seats           -1.269e+04   2697.447     -4.705      0.000   -1.81e+04   -7331.658\n",
       "BrandCode          24.8927    186.169      0.134      0.894    -345.021     394.806\n",
       "ModelCode         127.8068     58.840      2.172      0.033      10.893     244.720\n",
       "RapidChargeCode  1892.3517   1.15e+04      0.164      0.870    -2.1e+04    2.48e+04\n",
       "PowerTrainCode  -6499.3712   2449.677     -2.653      0.009   -1.14e+04   -1631.914\n",
       "PlugTypeCode     4386.9908   5259.981      0.834      0.406   -6064.479    1.48e+04\n",
       "BodyStyleCode     114.4619    783.718      0.146      0.884   -1442.769    1671.692\n",
       "SegmentCode      1.017e+04   1841.911      5.519      0.000    6505.808    1.38e+04\n",
       "==============================================================================\n",
       "Omnibus:                       45.142   Durbin-Watson:                   2.174\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              147.824\n",
       "Skew:                           1.497   Prob(JB):                     7.95e-33\n",
       "Kurtosis:                       8.048   Cond. No.                     1.50e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.5e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=results.fit()\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "336e1d47",
   "metadata": {
    "papermill": {
     "duration": 0.078687,
     "end_time": "2021-12-28T14:42:24.578765",
     "exception": false,
     "start_time": "2021-12-28T14:42:24.500078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Only Top Speed and Range ,BrandCode,ModelCode,RapidChargeCode,PlugTypeCode,BodyStyleCode are important parameters since high coeeficient values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d75374c",
   "metadata": {
    "papermill": {
     "duration": 0.076868,
     "end_time": "2021-12-28T14:42:24.733199",
     "exception": false,
     "start_time": "2021-12-28T14:42:24.656331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Importing train test split from Scikit Learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5cd3db0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:42:24.894856Z",
     "iopub.status.busy": "2021-12-28T14:42:24.894135Z",
     "iopub.status.idle": "2021-12-28T14:42:25.026064Z",
     "shell.execute_reply": "2021-12-28T14:42:25.025434Z",
     "shell.execute_reply.started": "2021-12-28T13:57:27.934162Z"
    },
    "papermill": {
     "duration": 0.215071,
     "end_time": "2021-12-28T14:42:25.026224",
     "exception": false,
     "start_time": "2021-12-28T14:42:24.811153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d138ebf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:42:25.194281Z",
     "iopub.status.busy": "2021-12-28T14:42:25.193556Z",
     "iopub.status.idle": "2021-12-28T14:42:25.197508Z",
     "shell.execute_reply": "2021-12-28T14:42:25.196755Z",
     "shell.execute_reply.started": "2021-12-28T13:57:28.070781Z"
    },
    "papermill": {
     "duration": 0.092252,
     "end_time": "2021-12-28T14:42:25.197666",
     "exception": false,
     "start_time": "2021-12-28T14:42:25.105414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc91ca",
   "metadata": {
    "papermill": {
     "duration": 0.077192,
     "end_time": "2021-12-28T14:42:25.354054",
     "exception": false,
     "start_time": "2021-12-28T14:42:25.276862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Importing Linear regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "580be326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:42:25.518055Z",
     "iopub.status.busy": "2021-12-28T14:42:25.517322Z",
     "iopub.status.idle": "2021-12-28T14:42:25.607750Z",
     "shell.execute_reply": "2021-12-28T14:42:25.606941Z",
     "shell.execute_reply.started": "2021-12-28T13:57:28.082739Z"
    },
    "papermill": {
     "duration": 0.174836,
     "end_time": "2021-12-28T14:42:25.607919",
     "exception": false,
     "start_time": "2021-12-28T14:42:25.433083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr= LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4cc0472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:42:25.776119Z",
     "iopub.status.busy": "2021-12-28T14:42:25.775288Z",
     "iopub.status.idle": "2021-12-28T14:42:25.777475Z",
     "shell.execute_reply": "2021-12-28T14:42:25.776966Z",
     "shell.execute_reply.started": "2021-12-28T13:57:28.185749Z"
    },
    "papermill": {
     "duration": 0.091219,
     "end_time": "2021-12-28T14:42:25.777634",
     "exception": false,
     "start_time": "2021-12-28T14:42:25.686415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)\n",
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c34667",
   "metadata": {
    "papermill": {
     "duration": 0.07873,
     "end_time": "2021-12-28T14:42:25.935844",
     "exception": false,
     "start_time": "2021-12-28T14:42:25.857114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Finding out the R-squared value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "817384a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:42:26.104670Z",
     "iopub.status.busy": "2021-12-28T14:42:26.103638Z",
     "iopub.status.idle": "2021-12-28T14:42:26.112308Z",
     "shell.execute_reply": "2021-12-28T14:42:26.112831Z",
     "shell.execute_reply.started": "2021-12-28T13:57:28.197278Z"
    },
    "papermill": {
     "duration": 0.094442,
     "end_time": "2021-12-28T14:42:26.113075",
     "exception": false,
     "start_time": "2021-12-28T14:42:26.018633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.20355108484323\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2=(r2_score(y_test,pred))\n",
    "print(r2*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017b4e3",
   "metadata": {
    "papermill": {
     "duration": 0.080259,
     "end_time": "2021-12-28T14:42:26.273291",
     "exception": false,
     "start_time": "2021-12-28T14:42:26.193032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Around 78% of the dependant variable has been explained by the independant variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e700b5",
   "metadata": {
    "papermill": {
     "duration": 0.078285,
     "end_time": "2021-12-28T14:42:26.432072",
     "exception": false,
     "start_time": "2021-12-28T14:42:26.353787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**classification and car finding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11b9a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df[['AccelSec',\n",
    " 'TopSpeed_KmH',\n",
    " 'Range_Km',\n",
    " 'Efficiency_WhKm',\n",
    " 'FastCharge_KmH',\n",
    " 'Seats',\n",
    " 'BrandCode',\n",
    " 'RapidChargeCode',\n",
    " 'PowerTrainCode',\n",
    " 'PlugTypeCode',\n",
    " 'BodyStyleCode',\n",
    " 'SegmentCode','PriceEuro']]\n",
    "y=df['ModelCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79eb44e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:42:26.968547Z",
     "iopub.status.busy": "2021-12-28T14:42:26.966706Z",
     "iopub.status.idle": "2021-12-28T14:42:26.971290Z",
     "shell.execute_reply": "2021-12-28T14:42:26.970711Z",
     "shell.execute_reply.started": "2021-12-28T13:57:28.262613Z"
    },
    "papermill": {
     "duration": 0.089762,
     "end_time": "2021-12-28T14:42:26.971456",
     "exception": false,
     "start_time": "2021-12-28T14:42:26.881694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(x, y, test_size=0.2,random_state=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05426e6c",
   "metadata": {
    "papermill": {
     "duration": 0.078355,
     "end_time": "2021-12-28T14:42:27.130274",
     "exception": false,
     "start_time": "2021-12-28T14:42:27.051919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Importing Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "125dbe37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:42:27.293686Z",
     "iopub.status.busy": "2021-12-28T14:42:27.292933Z",
     "iopub.status.idle": "2021-12-28T14:42:27.295144Z",
     "shell.execute_reply": "2021-12-28T14:42:27.294489Z",
     "shell.execute_reply.started": "2021-12-28T13:57:28.277685Z"
    },
    "papermill": {
     "duration": 0.0861,
     "end_time": "2021-12-28T14:42:27.295312",
     "exception": false,
     "start_time": "2021-12-28T14:42:27.209212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "301f427e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:42:27.495055Z",
     "iopub.status.busy": "2021-12-28T14:42:27.494296Z",
     "iopub.status.idle": "2021-12-28T14:42:27.495672Z",
     "shell.execute_reply": "2021-12-28T14:42:27.496487Z",
     "shell.execute_reply.started": "2021-12-28T13:57:28.306834Z"
    },
    "papermill": {
     "duration": 0.10977,
     "end_time": "2021-12-28T14:42:27.496691",
     "exception": false,
     "start_time": "2021-12-28T14:42:27.386921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "log= LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f852a6bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:42:27.663487Z",
     "iopub.status.busy": "2021-12-28T14:42:27.662802Z",
     "iopub.status.idle": "2021-12-28T14:42:27.683698Z",
     "shell.execute_reply": "2021-12-28T14:42:27.684457Z",
     "shell.execute_reply.started": "2021-12-28T13:57:28.320756Z"
    },
    "papermill": {
     "duration": 0.105843,
     "end_time": "2021-12-28T14:42:27.684703",
     "exception": false,
     "start_time": "2021-12-28T14:42:27.578860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([28, 93, 13, 88, 58, 42,  2, 54, 24, 28, 34, 27, 54, 67,  9, 88, 79,\n",
       "       85, 58, 50, 33], dtype=int8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.fit(X1_train, y1_train)\n",
    "pred1 = log.predict(X1_test)\n",
    "pred1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d64211a",
   "metadata": {
    "papermill": {
     "duration": 0.08061,
     "end_time": "2021-12-28T14:42:27.847230",
     "exception": false,
     "start_time": "2021-12-28T14:42:27.766620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Confusion Matrix of the regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e130a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:42:28.018526Z",
     "iopub.status.busy": "2021-12-28T14:42:28.017654Z",
     "iopub.status.idle": "2021-12-28T14:42:28.021774Z",
     "shell.execute_reply": "2021-12-28T14:42:28.021137Z",
     "shell.execute_reply.started": "2021-12-28T13:57:28.350791Z"
    },
    "papermill": {
     "duration": 0.094225,
     "end_time": "2021-12-28T14:42:28.021932",
     "exception": false,
     "start_time": "2021-12-28T14:42:27.927707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y1_test, pred1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f78e0b",
   "metadata": {
    "papermill": {
     "duration": 0.079933,
     "end_time": "2021-12-28T14:42:28.183488",
     "exception": false,
     "start_time": "2021-12-28T14:42:28.103555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Finding out the accuracy score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "348bc156",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T14:42:28.352248Z",
     "iopub.status.busy": "2021-12-28T14:42:28.351169Z",
     "iopub.status.idle": "2021-12-28T14:42:28.355830Z",
     "shell.execute_reply": "2021-12-28T14:42:28.355288Z",
     "shell.execute_reply.started": "2021-12-28T13:57:28.364923Z"
    },
    "papermill": {
     "duration": 0.092364,
     "end_time": "2021-12-28T14:42:28.356020",
     "exception": false,
     "start_time": "2021-12-28T14:42:28.263656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.761904761904762"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score=accuracy_score(y1_test,pred1)\n",
    "score*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a6734",
   "metadata": {
    "papermill": {
     "duration": 0.081175,
     "end_time": "2021-12-28T14:42:28.518215",
     "exception": false,
     "start_time": "2021-12-28T14:42:28.437040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The data is accurate upto 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "284fb4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpsklearn import HyperoptEstimator, all_classifiers\n",
    "from hyperopt import tpe\n",
    "import numpy as np\n",
    "np.random.seed( 42 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f2e6bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpsklearn import random_forest_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1b55234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_classifiers(name):\n",
    "#     \"\"\"\n",
    "#     All classifiers\n",
    "#     \"\"\"\n",
    "#     classifiers = [\n",
    "#         random_forest_classifier(name + \".random_forest\"),\n",
    "#         extra_trees_classifier(name + \".extra_trees\"),\n",
    "#         bagging_classifier(name + \".bagging\"),\n",
    "#         sgd_classifier(name + \".sgd\"),\n",
    "#         ridge_classifier(name + \".ridge\"),\n",
    "#         logistic_regression(name + \".logistic_regression\"),\n",
    "#         passive_aggressive_classifier(name + \".passive_aggressive\"),\n",
    "#         perceptron(name + \".perceptron\"),\n",
    "#         dummy_classifier(name + \".dummy\"),\n",
    "#         gaussian_process_classifier(name + \".gaussian_process\"),\n",
    "#         mlp_classifier(name + \".mlp\"),\n",
    "#         linear_svc(name + \".linear_svc\"),\n",
    "#         nu_svc(name + \".nu_svc\"),\n",
    "#         svc(name + \".svc\"),\n",
    "#         decision_tree_classifier(name + \".decision_tree\"),\n",
    "#         extra_tree_classifier(name + \".extra_tree\"),\n",
    "#         k_neighbors_classifier(name + \".knn\"),\n",
    "#         nearest_centroid(name + \".nearest_centroid\"),\n",
    "#     ]\n",
    "#     return hp.choice(name, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2bb66d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpsklearn import HyperoptEstimator,random_forest_classifier,k_neighbors_classifier,mlp_classifier,logistic_regression,svc\n",
    "from hyperopt import hp\n",
    "name=\"car_model\"\n",
    "clf = hp.pchoice( 'car_model', \n",
    "          [ ( 0.2, random_forest_classifier(name+'.random_forest') ),\n",
    "            ( 0.2, k_neighbors_classifier(name+'.knn') ),\n",
    "            (0.2,mlp_classifier(name + \".mlp\")),\n",
    "            (0.2,logistic_regression(name + \".logistic_regression\"))\n",
    "               ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "23597574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 3/3 [00:00<00:00,  6.39trial/s, best loss: 1.0]\n",
      " 75%|███████▌  | 3/4 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  6.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 7/7 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 8/8 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 9/9 [00:00<00:00,  5.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 10/10 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 11/11 [00:00<00:00, 13.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 12/12 [00:00<00:00,  3.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 13/13 [00:00<00:00,  2.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 14/14 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 15/15 [00:00<00:00, 14.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 16/16 [00:00<00:00,  8.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 17/17 [00:00<00:00,  6.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 18/18 [00:00<00:00, 11.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 19/19 [00:00<00:00, 11.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 20/20 [00:00<00:00, 14.86trial/s, best loss: 1.0]\n",
      " 95%|█████████▌| 20/21 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:16<00:00, 16.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22/22 [00:00<00:00,  8.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 23/23 [00:00<00:00, 10.31trial/s, best loss: 1.0]\n",
      " 96%|█████████▌| 23/24 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:14<00:00, 14.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 25/25 [00:00<00:00,  8.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 26/26 [00:00<00:00,  2.41trial/s, best loss: 1.0]\n",
      " 96%|█████████▋| 26/27 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:13<00:00, 13.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 28/28 [00:00<00:00, 10.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 29/29 [00:00<00:00,  9.13trial/s, best loss: 1.0]\n",
      " 97%|█████████▋| 29/30 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:12<00:00, 12.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 31/31 [00:00<00:00, 10.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 32/32 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      " 97%|█████████▋| 32/33 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:10<00:00, 10.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 34/34 [00:00<00:00,  8.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 35/35 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 36/36 [00:00<00:00,  7.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 37/37 [00:00<00:00,  7.21trial/s, best loss: 1.0]\n",
      " 97%|█████████▋| 37/38 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:13<00:00, 13.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 39/39 [00:00<00:00,  4.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 40/40 [00:00<00:00,  7.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 41/41 [00:00<00:00,  6.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 42/42 [00:00<00:00,  9.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 43/43 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      " 98%|█████████▊| 43/44 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:10<00:00, 10.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 45/45 [00:00<00:00,  8.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 46/46 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 47/47 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 48/48 [00:00<00:00,  8.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 49/49 [00:00<00:00,  9.43trial/s, best loss: 1.0]\n",
      " 98%|█████████▊| 49/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00, 12.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 51/51 [00:00<00:00,  9.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 52/52 [00:00<00:00,  9.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 53/53 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 54/54 [00:00<00:00,  5.16trial/s, best loss: 1.0]\n",
      " 98%|█████████▊| 54/55 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:11<00:00, 11.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 56/56 [00:13<00:00, 13.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 57/57 [00:00<00:00,  9.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 58/58 [00:00<00:00,  9.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 59/59 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 60/60 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 61/61 [00:00<00:00,  8.43trial/s, best loss: 1.0]\n",
      " 98%|█████████▊| 61/62 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:06<00:00,  6.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 63/63 [00:00<00:00,  9.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 64/64 [00:00<00:00, 10.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 65/65 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 66/66 [00:00<00:00,  2.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 67/67 [00:00<00:00,  5.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 68/68 [00:00<00:00,  6.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 69/69 [00:00<00:00,  7.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 70/70 [00:00<00:00,  7.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 71/71 [00:00<00:00,  4.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 72/72 [00:00<00:00,  9.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 73/73 [00:00<00:00,  2.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 74/74 [00:00<00:00,  8.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 75/75 [00:00<00:00,  9.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 76/76 [00:00<00:00,  4.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 77/77 [00:00<00:00,  4.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 78/78 [00:00<00:00,  5.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 79/79 [00:12<00:00, 12.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 80/80 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 81/81 [00:00<00:00,  9.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 82/82 [00:00<00:00,  9.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 83/83 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 84/84 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 85/85 [00:00<00:00,  8.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 86/86 [00:00<00:00,  7.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 87/87 [00:00<00:00,  8.13trial/s, best loss: 1.0]\n",
      " 99%|█████████▉| 87/88 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:06<00:00,  6.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 89/89 [00:00<00:00,  9.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 90/90 [00:00<00:00,  9.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 91/91 [00:00<00:00,  6.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 92/92 [00:00<00:00,  7.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 93/93 [00:00<00:00,  2.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 94/94 [00:00<00:00,  3.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 95/95 [00:00<00:00,  5.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 96/96 [00:00<00:00,  5.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 97/97 [00:00<00:00,  7.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 98/98 [00:00<00:00,  6.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 99/99 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 100/100 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 101/101 [00:00<00:00,  8.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 102/102 [00:00<00:00,  9.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 103/103 [00:00<00:00,  9.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 104/104 [00:00<00:00,  8.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 105/105 [00:00<00:00,  5.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 106/106 [00:00<00:00,  5.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 107/107 [00:00<00:00,  4.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 108/108 [00:00<00:00,  4.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 109/109 [00:19<00:00, 19.49s/trial, best loss: 1.0]\n",
      " 99%|█████████▉| 109/110 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 111/111 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 112/112 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 113/113 [00:00<00:00,  9.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 114/114 [00:00<00:00,  8.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 115/115 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 116/116 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 117/117 [00:00<00:00,  7.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 118/118 [00:00<00:00,  8.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 119/119 [00:00<00:00,  7.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 120/120 [00:00<00:00,  8.16trial/s, best loss: 1.0]\n",
      " 99%|█████████▉| 120/121 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:06<00:00,  6.48s/trial, best loss: 1.0]\n",
      " 99%|█████████▉| 121/122 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [00:04<00:00,  4.03s/trial, best loss: 1.0]\n",
      " 99%|█████████▉| 122/123 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:06<00:00,  6.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 124/124 [00:00<00:00,  8.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 125/125 [00:00<00:00,  6.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 126/126 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 127/127 [00:00<00:00,  6.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 128/128 [00:00<00:00,  6.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 129/129 [00:00<00:00,  3.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 130/130 [00:00<00:00,  4.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 131/131 [00:00<00:00,  2.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 132/132 [00:00<00:00,  5.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 133/133 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 134/134 [00:00<00:00,  4.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 135/135 [00:00<00:00,  6.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 136/136 [00:00<00:00,  6.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 137/137 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 138/138 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 139/139 [00:00<00:00,  6.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 140/140 [00:00<00:00,  8.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 141/141 [00:00<00:00,  5.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 142/142 [00:00<00:00,  8.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 143/143 [00:00<00:00,  6.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 144/144 [00:00<00:00,  5.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 145/145 [00:00<00:00,  3.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 146/146 [00:00<00:00,  2.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 147/147 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 148/148 [00:00<00:00,  4.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 149/149 [00:00<00:00,  4.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 150/150 [00:00<00:00,  4.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 151/151 [00:00<00:00,  6.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 152/152 [00:00<00:00,  6.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 153/153 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 154/154 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 155/155 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 156/156 [00:00<00:00,  8.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 157/157 [00:00<00:00,  8.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 158/158 [00:00<00:00,  8.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 159/159 [00:00<00:00,  8.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 160/160 [00:00<00:00,  8.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 161/161 [00:00<00:00,  3.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 162/162 [00:00<00:00,  6.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 163/163 [00:00<00:00,  4.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 164/164 [00:00<00:00,  4.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 165/165 [00:00<00:00,  4.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 166/166 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 167/167 [00:00<00:00,  4.89trial/s, best loss: 1.0]\n",
      " 99%|█████████▉| 167/168 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (194) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 169/169 [00:00<00:00,  4.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 170/170 [00:00<00:00,  4.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 171/171 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 172/172 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 173/173 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 174/174 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 175/175 [00:00<00:00,  8.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 176/176 [00:00<00:00,  8.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 177/177 [00:00<00:00,  8.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 178/178 [00:00<00:00,  8.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 179/179 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 180/180 [00:00<00:00,  2.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 181/181 [00:00<00:00,  7.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 182/182 [00:00<00:00,  7.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 183/183 [00:00<00:00,  7.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 184/184 [00:00<00:00,  7.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 185/185 [00:00<00:00,  7.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 186/186 [00:00<00:00,  2.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 187/187 [00:00<00:00,  6.69trial/s, best loss: 1.0]\n",
      " 99%|█████████▉| 187/188 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      " 99%|█████████▉| 188/189 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      " 99%|█████████▉| 189/190 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:05<00:00,  5.78s/trial, best loss: 1.0]\n",
      " 99%|█████████▉| 190/191 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:08<00:00,  8.51s/trial, best loss: 1.0]\n",
      " 99%|█████████▉| 191/192 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [00:04<00:00,  4.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 193/193 [00:00<00:00,  2.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 194/194 [00:00<00:00,  8.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 195/195 [00:00<00:00,  6.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 196/196 [00:00<00:00,  6.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 197/197 [00:00<00:00,  5.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 198/198 [00:00<00:00,  6.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 199/199 [00:00<00:00,  5.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 200/200 [00:00<00:00,  3.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 201/201 [00:00<00:00,  4.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 202/202 [00:00<00:00,  4.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 203/203 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 204/204 [00:00<00:00,  2.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 205/205 [00:00<00:00,  5.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 206/206 [00:00<00:00,  2.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 207/207 [00:00<00:00,  4.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 208/208 [00:00<00:00,  5.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 209/209 [00:00<00:00,  4.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 210/210 [00:00<00:00,  5.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 211/211 [00:00<00:00,  4.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 212/212 [00:00<00:00,  6.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 213/213 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 214/214 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 215/215 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 216/216 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 217/217 [00:00<00:00,  5.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 218/218 [00:00<00:00,  5.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 219/219 [00:00<00:00,  5.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 220/220 [00:00<00:00,  5.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 221/221 [00:00<00:00,  5.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 222/222 [00:00<00:00,  5.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 223/223 [00:00<00:00,  6.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 224/224 [00:00<00:00,  5.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 225/225 [00:00<00:00,  5.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 226/226 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 227/227 [00:00<00:00,  5.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 228/228 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 229/229 [00:00<00:00,  3.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 230/230 [00:00<00:00,  2.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 231/231 [00:00<00:00,  5.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 232/232 [00:00<00:00,  5.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 233/233 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 234/234 [00:00<00:00,  4.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 235/235 [00:00<00:00,  4.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 236/236 [00:00<00:00,  6.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 237/237 [00:00<00:00,  6.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 238/238 [00:00<00:00,  6.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 239/239 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 240/240 [00:05<00:00,  5.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 241/241 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 242/242 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 243/243 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 244/244 [00:00<00:00,  7.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 245/245 [00:00<00:00,  8.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 246/246 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 247/247 [00:00<00:00,  7.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 248/248 [00:00<00:00,  7.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 249/249 [00:00<00:00,  7.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 250/250 [00:00<00:00,  7.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 251/251 [00:00<00:00,  3.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 252/252 [00:00<00:00,  3.91trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 252/253 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (215) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253/253 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 253/254 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (245) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 255/255 [00:00<00:00,  4.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 256/256 [00:00<00:00,  4.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 257/257 [00:00<00:00,  4.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 258/258 [00:00<00:00,  4.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 259/259 [00:00<00:00,  4.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 260/260 [00:00<00:00,  4.70trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 260/261 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [00:00<00:00,  4.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 262/262 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 263/263 [00:00<00:00,  4.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 264/264 [00:00<00:00,  4.84trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 264/265 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 266/266 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 266/267 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267/267 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 268/268 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 269/269 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 270/270 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 271/271 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 272/272 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 273/273 [00:00<00:00,  2.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 274/274 [00:00<00:00,  7.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 275/275 [00:00<00:00,  7.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 276/276 [00:00<00:00,  7.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 277/277 [00:00<00:00,  7.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 278/278 [00:00<00:00,  7.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 279/279 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 280/280 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 281/281 [00:00<00:00,  1.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 282/282 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 283/283 [00:00<00:00,  7.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 284/284 [00:00<00:00,  6.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 285/285 [00:00<00:00,  7.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 286/286 [00:00<00:00,  6.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 287/287 [00:00<00:00,  7.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 288/288 [00:00<00:00,  7.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 289/289 [00:00<00:00,  5.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 290/290 [00:00<00:00,  5.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 291/291 [00:00<00:00,  5.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 292/292 [00:00<00:00,  5.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 293/293 [00:00<00:00,  2.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 294/294 [00:00<00:00,  5.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 295/295 [00:00<00:00,  3.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 296/296 [00:00<00:00,  5.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 297/297 [00:00<00:00,  4.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 298/298 [00:00<00:00,  4.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 299/299 [00:00<00:00,  4.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 300/300 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 301/301 [00:00<00:00,  2.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 302/302 [00:00<00:00,  3.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 303/303 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 304/304 [00:00<00:00,  5.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 305/305 [00:00<00:00,  5.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 306/306 [00:00<00:00,  5.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 307/307 [00:00<00:00,  4.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 308/308 [00:00<00:00,  4.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 309/309 [00:00<00:00,  4.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 310/310 [00:00<00:00,  4.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 311/311 [00:00<00:00,  4.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 312/312 [00:00<00:00,  4.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 313/313 [00:00<00:00,  4.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 314/314 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 315/315 [00:00<00:00,  6.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 316/316 [00:00<00:00,  5.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 317/317 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 318/318 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 319/319 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 320/320 [00:05<00:00,  5.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 321/321 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 322/322 [00:00<00:00,  5.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 323/323 [00:00<00:00,  5.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 324/324 [00:00<00:00,  5.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 325/325 [00:00<00:00,  5.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 326/326 [00:00<00:00,  4.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 327/327 [00:00<00:00,  5.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 328/328 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 329/329 [00:00<00:00,  5.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 330/330 [00:00<00:00,  5.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 331/331 [00:00<00:00,  5.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 332/332 [00:00<00:00,  7.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 333/333 [00:00<00:00,  7.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 334/334 [00:00<00:00,  7.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 335/335 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 336/336 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 337/337 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 338/338 [00:00<00:00,  3.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 339/339 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 340/340 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 341/341 [00:00<00:00,  4.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 342/342 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 343/343 [00:00<00:00,  4.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 344/344 [00:00<00:00,  4.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 345/345 [00:00<00:00,  4.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 346/346 [00:00<00:00,  4.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 347/347 [00:00<00:00,  4.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 348/348 [00:00<00:00,  4.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 349/349 [00:00<00:00,  2.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 350/350 [00:00<00:00,  5.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 351/351 [00:00<00:00,  5.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 352/352 [00:00<00:00,  5.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 353/353 [00:00<00:00,  5.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 354/354 [00:00<00:00,  5.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 355/355 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 356/356 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 357/357 [00:05<00:00,  5.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 358/358 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 359/359 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 360/360 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 361/361 [00:00<00:00,  6.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 362/362 [00:00<00:00,  6.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 363/363 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 364/364 [00:00<00:00,  6.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 365/365 [00:00<00:00,  6.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 366/366 [00:00<00:00,  6.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 367/367 [00:00<00:00,  7.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 368/368 [00:00<00:00,  6.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 369/369 [00:00<00:00,  6.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 370/370 [00:00<00:00,  2.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 371/371 [00:00<00:00,  3.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 372/372 [00:00<00:00,  3.32trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 372/373 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (216) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 373/374 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 375/375 [00:00<00:00,  4.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 376/376 [00:00<00:00,  4.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 377/377 [00:00<00:00,  1.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 378/378 [00:00<00:00,  3.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 379/379 [00:00<00:00,  4.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 380/380 [00:00<00:00,  4.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 381/381 [00:00<00:00,  4.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 382/382 [00:00<00:00,  4.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 383/383 [00:00<00:00,  4.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 384/384 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 385/385 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 386/386 [00:00<00:00,  4.68trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 386/387 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [00:00<00:00,  3.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 388/388 [00:00<00:00,  4.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 389/389 [00:00<00:00,  3.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 390/390 [00:00<00:00,  2.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 391/391 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 392/392 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 393/393 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 394/394 [00:00<00:00,  3.98trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 394/395 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 396/396 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 397/397 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 398/398 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 399/399 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 400/400 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 401/401 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 402/402 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 403/403 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 404/404 [00:00<00:00,  6.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 405/405 [00:00<00:00,  2.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 406/406 [00:00<00:00,  6.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 407/407 [00:00<00:00,  6.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 408/408 [00:00<00:00,  6.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 409/409 [00:00<00:00,  6.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 410/410 [00:00<00:00,  6.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 411/411 [00:00<00:00,  6.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 412/412 [00:00<00:00,  2.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 413/413 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 414/414 [00:00<00:00,  1.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 415/415 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 416/416 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 417/417 [00:00<00:00,  6.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 418/418 [00:00<00:00,  6.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 419/419 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 420/420 [00:00<00:00,  6.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 421/421 [00:00<00:00,  6.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 422/422 [00:00<00:00,  6.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 423/423 [00:00<00:00,  6.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 424/424 [00:00<00:00,  6.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 425/425 [00:00<00:00,  6.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 426/426 [00:00<00:00,  6.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 427/427 [00:00<00:00,  7.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 428/428 [00:00<00:00,  5.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 429/429 [00:00<00:00,  5.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 430/430 [00:00<00:00,  5.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 431/431 [00:00<00:00,  5.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 432/432 [00:00<00:00,  5.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 433/433 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 434/434 [00:00<00:00,  5.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 435/435 [00:00<00:00,  4.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 436/436 [00:00<00:00,  4.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 437/437 [00:00<00:00,  4.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 438/438 [00:00<00:00,  3.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 439/439 [00:00<00:00,  5.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 440/440 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 441/441 [00:00<00:00,  3.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 442/442 [00:00<00:00,  4.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 443/443 [00:00<00:00,  2.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 444/444 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 445/445 [00:00<00:00,  2.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 446/446 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 447/447 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 448/448 [00:00<00:00,  6.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 449/449 [00:00<00:00,  4.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 450/450 [00:00<00:00,  5.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 451/451 [00:00<00:00,  4.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 452/452 [00:00<00:00,  4.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 453/453 [00:00<00:00,  4.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 454/454 [00:00<00:00,  4.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 455/455 [00:00<00:00,  4.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 456/456 [00:00<00:00,  4.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 457/457 [00:00<00:00,  4.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 458/458 [00:00<00:00,  4.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 459/459 [00:00<00:00,  4.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 460/460 [00:00<00:00,  3.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 461/461 [00:00<00:00,  4.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 462/462 [00:00<00:00,  4.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 463/463 [00:00<00:00,  4.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 464/464 [00:00<00:00,  5.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 465/465 [00:00<00:00,  5.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 466/466 [00:00<00:00,  5.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 467/467 [00:00<00:00,  5.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 468/468 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 469/469 [00:05<00:00,  5.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 470/470 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 471/471 [00:04<00:00,  4.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 472/472 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 473/473 [00:00<00:00,  5.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 474/474 [00:00<00:00,  6.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 475/475 [00:00<00:00,  5.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 476/476 [00:00<00:00,  5.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 477/477 [00:00<00:00,  4.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 478/478 [00:00<00:00,  5.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 479/479 [00:00<00:00,  4.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 480/480 [00:00<00:00,  5.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 481/481 [00:00<00:00,  4.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 482/482 [00:00<00:00,  5.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 483/483 [00:00<00:00,  4.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 484/484 [00:00<00:00,  5.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 485/485 [00:00<00:00,  4.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 486/486 [00:00<00:00,  5.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 487/487 [00:00<00:00,  5.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 488/488 [00:00<00:00,  6.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 489/489 [00:00<00:00,  6.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 490/490 [00:00<00:00,  6.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 491/491 [00:00<00:00,  6.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 492/492 [00:00<00:00,  6.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 493/493 [00:00<00:00,  6.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 494/494 [00:00<00:00,  6.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 495/495 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 496/496 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 497/497 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 498/498 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 499/499 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 500/500 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 501/501 [00:00<00:00,  6.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 502/502 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 503/503 [00:00<00:00,  4.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 504/504 [00:00<00:00,  4.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 505/505 [00:00<00:00,  3.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 506/506 [00:00<00:00,  3.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 507/507 [00:00<00:00,  4.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 508/508 [00:00<00:00,  4.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 509/509 [00:00<00:00,  4.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 510/510 [00:00<00:00,  4.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 511/511 [00:00<00:00,  3.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 512/512 [00:00<00:00,  4.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 513/513 [00:00<00:00,  5.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 514/514 [00:00<00:00,  5.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 515/515 [00:00<00:00,  5.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 516/516 [00:00<00:00,  5.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 517/517 [00:00<00:00,  5.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 518/518 [00:00<00:00,  5.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 519/519 [00:00<00:00,  5.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 520/520 [00:00<00:00,  5.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 521/521 [00:00<00:00,  5.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 522/522 [00:00<00:00,  5.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 523/523 [00:00<00:00,  5.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 524/524 [00:05<00:00,  5.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 525/525 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 526/526 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 527/527 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 528/528 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 529/529 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 530/530 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 531/531 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 532/532 [00:00<00:00,  6.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 533/533 [00:00<00:00,  6.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 534/534 [00:00<00:00,  6.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 535/535 [00:00<00:00,  6.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 536/536 [00:00<00:00,  6.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 537/537 [00:00<00:00,  6.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 538/538 [00:00<00:00,  6.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 539/539 [00:00<00:00,  6.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 540/540 [00:00<00:00,  6.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 541/541 [00:00<00:00,  6.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 542/542 [00:00<00:00,  6.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 543/543 [00:00<00:00,  6.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 544/544 [00:00<00:00,  6.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 545/545 [00:00<00:00,  3.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 546/546 [00:00<00:00,  3.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 547/547 [00:00<00:00,  3.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 548/548 [00:00<00:00,  3.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 549/549 [00:00<00:00,  4.01trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 549/550 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (219) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550/550 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 551/551 [00:00<00:00,  3.97trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 551/552 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (181) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 552/552 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 553/553 [00:00<00:00,  4.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 554/554 [00:00<00:00,  4.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 555/555 [00:00<00:00,  4.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 556/556 [00:00<00:00,  4.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 557/557 [00:00<00:00,  4.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 558/558 [00:00<00:00,  3.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 559/559 [00:00<00:00,  4.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 560/560 [00:00<00:00,  4.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 561/561 [00:00<00:00,  4.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 562/562 [00:00<00:00,  4.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 563/563 [00:00<00:00,  4.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 564/564 [00:00<00:00,  4.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 565/565 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 565/566 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (227) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 566/566 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 567/567 [00:00<00:00,  2.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 568/568 [00:00<00:00,  3.58trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 568/569 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 569/569 [00:00<00:00,  3.85trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 569/570 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 570/570 [00:00<00:00,  3.73trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 570/571 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 571/571 [00:00<00:00,  3.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 572/572 [00:00<00:00,  2.54trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 572/573 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 573/574 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 574/574 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 574/575 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 576/576 [00:00<00:00,  3.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 577/577 [00:00<00:00,  3.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 578/578 [00:00<00:00,  3.81trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 578/579 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (238) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 579/579 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 580/580 [00:00<00:00,  3.42trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 580/581 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 581/581 [00:00<00:00,  3.74trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 581/582 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 582/582 [00:00<00:00,  3.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 583/583 [00:00<00:00,  3.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 584/584 [00:00<00:00,  3.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 585/585 [00:00<00:00,  3.43trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 585/586 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 587/587 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 588/588 [00:00<00:00,  4.05trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 588/589 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 589/589 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 590/590 [00:00<00:00,  2.76trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 590/591 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 591/591 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 591/592 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 592/592 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 592/593 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 593/593 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 593/594 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 594/594 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 595/595 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 596/596 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 597/597 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 598/598 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 599/599 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 600/600 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 601/601 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 602/602 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 603/603 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 604/604 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 605/605 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 606/606 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 607/607 [00:00<00:00,  6.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 608/608 [00:00<00:00,  5.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 609/609 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 610/610 [00:00<00:00,  6.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 611/611 [00:00<00:00,  5.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 612/612 [00:00<00:00,  6.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 613/613 [00:00<00:00,  5.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 614/614 [00:00<00:00,  5.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 615/615 [00:00<00:00,  6.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 616/616 [00:00<00:00,  5.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 617/617 [00:00<00:00,  6.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 618/618 [00:00<00:00,  6.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 619/619 [00:00<00:00,  5.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 620/620 [00:00<00:00,  5.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 621/621 [00:00<00:00,  6.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 622/622 [00:00<00:00,  5.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 623/623 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 624/624 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 625/625 [00:00<00:00,  1.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 626/626 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 627/627 [00:00<00:00,  5.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 628/628 [00:00<00:00,  5.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 629/629 [00:00<00:00,  5.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 630/630 [00:00<00:00,  5.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 631/631 [00:00<00:00,  5.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 632/632 [00:00<00:00,  5.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 633/633 [00:00<00:00,  5.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 634/634 [00:00<00:00,  5.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 635/635 [00:00<00:00,  5.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 636/636 [00:00<00:00,  5.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 637/637 [00:00<00:00,  5.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 638/638 [00:00<00:00,  6.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 639/639 [00:00<00:00,  5.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 640/640 [00:00<00:00,  5.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 641/641 [00:00<00:00,  5.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 642/642 [00:00<00:00,  5.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 643/643 [00:00<00:00,  5.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 644/644 [00:00<00:00,  5.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 645/645 [00:00<00:00,  4.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 646/646 [00:00<00:00,  4.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 647/647 [00:00<00:00,  4.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 648/648 [00:00<00:00,  4.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 649/649 [00:00<00:00,  4.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 650/650 [00:00<00:00,  4.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 651/651 [00:00<00:00,  4.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 652/652 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 653/653 [00:00<00:00,  4.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 654/654 [00:00<00:00,  4.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 655/655 [00:00<00:00,  4.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 656/656 [00:00<00:00,  4.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 657/657 [00:00<00:00,  4.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 658/658 [00:00<00:00,  3.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 659/659 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 660/660 [00:00<00:00,  3.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 661/661 [00:00<00:00,  3.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 662/662 [00:00<00:00,  3.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 663/663 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 664/664 [00:00<00:00,  3.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 665/665 [00:00<00:00,  3.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 666/666 [00:00<00:00,  2.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 667/667 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 668/668 [00:00<00:00,  3.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 669/669 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 670/670 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 671/671 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 672/672 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 673/673 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 674/674 [00:00<00:00,  1.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 675/675 [00:00<00:00,  5.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 676/676 [00:00<00:00,  4.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 677/677 [00:00<00:00,  4.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 678/678 [00:00<00:00,  4.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 679/679 [00:00<00:00,  4.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 680/680 [00:00<00:00,  4.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 681/681 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 682/682 [00:00<00:00,  4.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 683/683 [00:00<00:00,  4.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 684/684 [00:00<00:00,  4.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 685/685 [00:00<00:00,  3.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 686/686 [00:00<00:00,  3.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 687/687 [00:00<00:00,  3.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 688/688 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 689/689 [00:00<00:00,  4.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 690/690 [00:00<00:00,  3.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 691/691 [00:00<00:00,  3.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 692/692 [00:00<00:00,  3.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 693/693 [00:00<00:00,  3.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 694/694 [00:00<00:00,  3.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 695/695 [00:00<00:00,  3.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 696/696 [00:00<00:00,  3.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 697/697 [00:00<00:00,  3.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 698/698 [00:00<00:00,  3.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 699/699 [00:00<00:00,  4.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 700/700 [00:00<00:00,  4.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 701/701 [00:00<00:00,  4.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 702/702 [00:00<00:00,  4.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 703/703 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 704/704 [00:00<00:00,  4.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 705/705 [00:00<00:00,  4.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 706/706 [00:00<00:00,  4.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 707/707 [00:05<00:00,  5.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 708/708 [00:04<00:00,  4.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 709/709 [00:05<00:00,  5.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 710/710 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 711/711 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 712/712 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 713/713 [00:00<00:00,  5.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 714/714 [00:00<00:00,  5.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 715/715 [00:00<00:00,  4.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 716/716 [00:00<00:00,  4.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 717/717 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 718/718 [00:00<00:00,  4.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 719/719 [00:00<00:00,  4.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 720/720 [00:00<00:00,  4.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 721/721 [00:00<00:00,  4.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 722/722 [00:00<00:00,  4.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 723/723 [00:00<00:00,  4.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 724/724 [00:00<00:00,  4.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 725/725 [00:00<00:00,  4.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 726/726 [00:00<00:00,  4.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 727/727 [00:00<00:00,  4.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 728/728 [00:00<00:00,  4.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 729/729 [00:00<00:00,  4.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 730/730 [00:00<00:00,  4.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 731/731 [00:00<00:00,  4.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 732/732 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 733/733 [00:00<00:00,  5.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 734/734 [00:00<00:00,  5.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 735/735 [00:00<00:00,  5.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 736/736 [00:00<00:00,  5.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 737/737 [00:00<00:00,  5.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 738/738 [00:00<00:00,  5.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 739/739 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 740/740 [00:00<00:00,  5.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 741/741 [00:00<00:00,  5.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 742/742 [00:00<00:00,  5.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 743/743 [00:00<00:00,  5.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 744/744 [00:00<00:00,  5.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 745/745 [00:00<00:00,  5.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 746/746 [00:00<00:00,  5.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 747/747 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 748/748 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 749/749 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 750/750 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 751/751 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 752/752 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 753/753 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 754/754 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 755/755 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 756/756 [00:00<00:00,  5.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 757/757 [00:00<00:00,  4.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 758/758 [00:00<00:00,  4.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 759/759 [00:00<00:00,  4.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 760/760 [00:00<00:00,  4.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 761/761 [00:00<00:00,  3.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 762/762 [00:00<00:00,  4.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 763/763 [00:00<00:00,  4.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 764/764 [00:00<00:00,  3.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 765/765 [00:00<00:00,  3.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 766/766 [00:00<00:00,  4.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 767/767 [00:00<00:00,  3.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 768/768 [00:00<00:00,  3.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 769/769 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 770/770 [00:00<00:00,  4.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 771/771 [00:00<00:00,  4.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 772/772 [00:00<00:00,  4.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 773/773 [00:00<00:00,  5.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 774/774 [00:00<00:00,  4.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 775/775 [00:00<00:00,  4.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 776/776 [00:00<00:00,  5.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 777/777 [00:00<00:00,  5.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 778/778 [00:00<00:00,  4.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 779/779 [00:00<00:00,  4.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 780/780 [00:00<00:00,  4.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 781/781 [00:00<00:00,  4.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 782/782 [00:00<00:00,  4.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 783/783 [00:00<00:00,  4.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 784/784 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 785/785 [00:00<00:00,  4.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 786/786 [00:00<00:00,  4.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 787/787 [00:00<00:00,  4.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 788/788 [00:00<00:00,  4.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 789/789 [00:04<00:00,  4.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 790/790 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 791/791 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 792/792 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 793/793 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 794/794 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 795/795 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 796/796 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 797/797 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 798/798 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 799/799 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 800/800 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 801/801 [00:03<00:00,  3.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 802/802 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 803/803 [00:00<00:00,  5.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 804/804 [00:00<00:00,  5.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 805/805 [00:00<00:00,  5.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 806/806 [00:00<00:00,  5.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 807/807 [00:00<00:00,  5.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 808/808 [00:00<00:00,  5.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 809/809 [00:00<00:00,  5.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 810/810 [00:00<00:00,  5.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 811/811 [00:00<00:00,  5.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 812/812 [00:00<00:00,  5.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 813/813 [00:00<00:00,  5.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 814/814 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 815/815 [00:00<00:00,  5.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 816/816 [00:00<00:00,  5.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 817/817 [00:00<00:00,  5.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 818/818 [00:00<00:00,  5.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 819/819 [00:00<00:00,  5.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 820/820 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 821/821 [00:00<00:00,  3.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 822/822 [00:00<00:00,  3.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 823/823 [00:00<00:00,  2.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 824/824 [00:00<00:00,  3.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 825/825 [00:00<00:00,  3.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 826/826 [00:00<00:00,  2.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 827/827 [00:00<00:00,  3.23trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 827/828 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (192) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 828/828 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 828/829 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (188) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 829/829 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 830/830 [00:00<00:00,  3.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 831/831 [00:00<00:00,  3.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 832/832 [00:00<00:00,  3.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 833/833 [00:00<00:00,  3.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 834/834 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 835/835 [00:00<00:00,  3.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 836/836 [00:00<00:00,  3.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 837/837 [00:00<00:00,  3.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 838/838 [00:00<00:00,  3.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 839/839 [00:00<00:00,  3.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 840/840 [00:00<00:00,  3.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 841/841 [00:00<00:00,  3.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 842/842 [00:00<00:00,  3.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 843/843 [00:00<00:00,  3.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 844/844 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 845/845 [00:00<00:00,  3.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 846/846 [00:00<00:00,  3.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 847/847 [00:00<00:00,  3.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 848/848 [00:00<00:00,  3.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 849/849 [00:00<00:00,  3.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 850/850 [00:00<00:00,  3.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 851/851 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 852/852 [00:00<00:00,  3.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 853/853 [00:00<00:00,  3.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 854/854 [00:00<00:00,  3.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 855/855 [00:00<00:00,  3.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 856/856 [00:00<00:00,  3.41trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 856/857 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 857/857 [00:00<00:00,  3.43trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 857/858 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 858/858 [00:00<00:00,  3.36trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 858/859 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 859/859 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 859/860 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 860/860 [00:00<00:00,  3.18trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 860/861 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 861/861 [00:00<00:00,  3.28trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 861/862 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 862/862 [00:00<00:00,  3.32trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 862/863 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 863/863 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 863/864 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 864/864 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 864/865 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 865/865 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 866/866 [00:00<00:00,  2.86trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 866/867 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 867/867 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 867/868 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 868/868 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 869/869 [00:00<00:00,  2.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 870/870 [00:00<00:00,  2.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 871/871 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 872/872 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 872/873 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (227) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 873/873 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 874/874 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 875/875 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 875/876 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 876/876 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 876/877 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 877/877 [00:00<00:00,  3.20trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 877/878 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [00:00<00:00,  3.35trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 878/879 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 879/879 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 879/880 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 880/880 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 881/881 [00:00<00:00,  3.24trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 881/882 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 882/882 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 883/883 [00:00<00:00,  2.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 884/884 [00:00<00:00,  3.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 885/885 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 885/886 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 886/886 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 886/887 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 887/887 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 888/888 [00:00<00:00,  3.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 889/889 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 890/890 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 890/891 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 891/891 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 892/892 [00:00<00:00,  2.97trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 892/893 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 893/893 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 894/894 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 894/895 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 895/895 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 896/896 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 897/897 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 898/898 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 899/899 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 900/900 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 901/901 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 902/902 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 903/903 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 904/904 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 905/905 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 906/906 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 907/907 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 908/908 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 909/909 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 910/910 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 911/911 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 912/912 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 913/913 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 914/914 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 915/915 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 916/916 [00:00<00:00,  5.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 917/917 [00:00<00:00,  5.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 918/918 [00:00<00:00,  5.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 919/919 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 920/920 [00:00<00:00,  5.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 921/921 [00:00<00:00,  5.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 922/922 [00:00<00:00,  5.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 923/923 [00:00<00:00,  5.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 924/924 [00:00<00:00,  5.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 925/925 [00:00<00:00,  5.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 926/926 [00:00<00:00,  5.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 927/927 [00:00<00:00,  5.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 928/928 [00:00<00:00,  5.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 929/929 [00:00<00:00,  4.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 930/930 [00:00<00:00,  5.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 931/931 [00:00<00:00,  4.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 932/932 [00:00<00:00,  5.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 933/933 [00:00<00:00,  5.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 934/934 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 935/935 [00:00<00:00,  5.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 936/936 [00:00<00:00,  5.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 937/937 [00:00<00:00,  5.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 938/938 [00:00<00:00,  5.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 939/939 [00:00<00:00,  5.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 940/940 [00:00<00:00,  5.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 941/941 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 942/942 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 943/943 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 944/944 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 945/945 [00:00<00:00,  5.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 946/946 [00:00<00:00,  5.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 947/947 [00:00<00:00,  5.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 948/948 [00:00<00:00,  5.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 949/949 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 950/950 [00:00<00:00,  5.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 951/951 [00:00<00:00,  5.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 952/952 [00:00<00:00,  5.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 953/953 [00:00<00:00,  4.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 954/954 [00:00<00:00,  4.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 955/955 [00:00<00:00,  4.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 956/956 [00:00<00:00,  4.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 957/957 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 958/958 [00:00<00:00,  4.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 959/959 [00:00<00:00,  4.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 960/960 [00:00<00:00,  4.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 961/961 [00:00<00:00,  4.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 962/962 [00:00<00:00,  4.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 963/963 [00:00<00:00,  4.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 964/964 [00:00<00:00,  4.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 965/965 [00:00<00:00,  5.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 966/966 [00:00<00:00,  4.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 967/967 [00:00<00:00,  4.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 968/968 [00:00<00:00,  4.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 969/969 [00:00<00:00,  4.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 970/970 [00:00<00:00,  4.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 971/971 [00:00<00:00,  5.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 972/972 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 973/973 [00:00<00:00,  5.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 974/974 [00:00<00:00,  5.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 975/975 [00:00<00:00,  4.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 976/976 [00:00<00:00,  4.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 977/977 [00:00<00:00,  4.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 978/978 [00:00<00:00,  4.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 979/979 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 980/980 [00:00<00:00,  4.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 981/981 [00:00<00:00,  3.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 982/982 [00:00<00:00,  3.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 983/983 [00:00<00:00,  3.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 984/984 [00:00<00:00,  3.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 985/985 [00:00<00:00,  4.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 986/986 [00:00<00:00,  4.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 987/987 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 988/988 [00:00<00:00,  3.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 989/989 [00:00<00:00,  3.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 990/990 [00:00<00:00,  3.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 991/991 [00:00<00:00,  3.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 992/992 [00:00<00:00,  3.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 993/993 [00:00<00:00,  3.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 994/994 [00:00<00:00,  3.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 995/995 [00:00<00:00,  3.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 996/996 [00:00<00:00,  3.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 997/997 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 998/998 [00:00<00:00,  3.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 999/999 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 1000/1000 [00:00<00:00,  2.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 1001/1001 [00:00<00:00,  3.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 1002/1002 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 1003/1003 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 1004/1004 [00:00<00:00,  3.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 1005/1005 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 1006/1006 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1007/1007 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1008/1008 [00:00<00:00,  2.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 1009/1009 [00:00<00:00,  1.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 1010/1010 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1011/1011 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 1012/1012 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 1013/1013 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 1014/1014 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 1015/1015 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 1016/1016 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 1017/1017 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 1018/1018 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 1019/1019 [00:00<00:00,  4.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 1020/1020 [00:00<00:00,  4.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 1021/1021 [00:00<00:00,  3.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1022/1022 [00:00<00:00,  3.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 1023/1023 [00:00<00:00,  3.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 1024/1024 [00:00<00:00,  3.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 1025/1025 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 1026/1026 [00:00<00:00,  3.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 1027/1027 [00:00<00:00,  3.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 1028/1028 [00:00<00:00,  4.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 1029/1029 [00:00<00:00,  3.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 1030/1030 [00:00<00:00,  3.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1031/1031 [00:00<00:00,  3.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 1032/1032 [00:00<00:00,  3.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 1033/1033 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 1034/1034 [00:00<00:00,  3.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 1035/1035 [00:00<00:00,  3.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 1036/1036 [00:00<00:00,  3.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 1037/1037 [00:00<00:00,  3.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 1038/1038 [00:00<00:00,  3.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 1039/1039 [00:00<00:00,  3.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 1040/1040 [00:00<00:00,  3.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 1041/1041 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 1042/1042 [00:00<00:00,  3.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 1043/1043 [00:00<00:00,  3.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 1044/1044 [00:00<00:00,  3.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 1045/1045 [00:00<00:00,  3.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1046/1046 [00:00<00:00,  3.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 1047/1047 [00:00<00:00,  3.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1048/1048 [00:00<00:00,  3.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 1049/1049 [00:00<00:00,  3.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 1050/1050 [00:00<00:00,  3.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 1051/1051 [00:00<00:00,  3.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 1052/1052 [00:00<00:00,  3.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 1053/1053 [00:00<00:00,  3.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 1054/1054 [00:00<00:00,  3.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 1055/1055 [00:00<00:00,  3.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 1056/1056 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 1057/1057 [00:00<00:00,  4.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 1058/1058 [00:00<00:00,  3.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 1059/1059 [00:00<00:00,  4.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 1060/1060 [00:00<00:00,  4.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 1061/1061 [00:00<00:00,  4.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 1062/1062 [00:00<00:00,  4.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 1063/1063 [00:00<00:00,  4.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 1064/1064 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 1065/1065 [00:00<00:00,  4.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 1066/1066 [00:00<00:00,  4.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 1067/1067 [00:00<00:00,  4.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 1068/1068 [00:05<00:00,  5.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 1069/1069 [00:05<00:00,  5.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 1070/1070 [00:04<00:00,  4.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 1071/1071 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 1072/1072 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 1073/1073 [00:05<00:00,  5.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 1074/1074 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 1075/1075 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 1076/1076 [00:00<00:00,  4.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 1077/1077 [00:00<00:00,  4.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 1078/1078 [00:00<00:00,  4.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 1079/1079 [00:00<00:00,  4.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 1080/1080 [00:00<00:00,  4.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 1081/1081 [00:00<00:00,  4.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 1082/1082 [00:00<00:00,  4.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 1083/1083 [00:00<00:00,  4.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 1084/1084 [00:00<00:00,  4.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 1085/1085 [00:00<00:00,  4.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1086/1086 [00:00<00:00,  4.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 1087/1087 [00:00<00:00,  4.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 1088/1088 [00:00<00:00,  4.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 1089/1089 [00:00<00:00,  4.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 1090/1090 [00:00<00:00,  3.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1091/1091 [00:00<00:00,  4.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 1092/1092 [00:00<00:00,  4.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 1093/1093 [00:00<00:00,  4.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 1094/1094 [00:00<00:00,  4.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 1095/1095 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 1096/1096 [00:00<00:00,  4.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 1097/1097 [00:00<00:00,  4.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 1098/1098 [00:00<00:00,  4.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 1099/1099 [00:00<00:00,  4.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 1100/1100 [00:00<00:00,  3.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 1101/1101 [00:00<00:00,  4.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 1102/1102 [00:00<00:00,  4.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 1103/1103 [00:00<00:00,  4.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1104/1104 [00:00<00:00,  4.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 1105/1105 [00:00<00:00,  4.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 1106/1106 [00:00<00:00,  4.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 1107/1107 [00:00<00:00,  4.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 1108/1108 [00:00<00:00,  4.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 1109/1109 [00:00<00:00,  4.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 1110/1110 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 1111/1111 [00:00<00:00,  4.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 1112/1112 [00:00<00:00,  4.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 1113/1113 [00:00<00:00,  4.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 1114/1114 [00:00<00:00,  4.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 1115/1115 [00:00<00:00,  4.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 1116/1116 [00:00<00:00,  4.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 1117/1117 [00:00<00:00,  4.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1118/1118 [00:00<00:00,  4.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 1119/1119 [00:00<00:00,  4.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 1120/1120 [00:00<00:00,  4.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 1121/1121 [00:00<00:00,  4.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 1122/1122 [00:00<00:00,  4.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 1123/1123 [00:00<00:00,  4.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 1124/1124 [00:00<00:00,  4.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 1125/1125 [00:00<00:00,  5.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 1126/1126 [00:00<00:00,  4.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 1127/1127 [00:00<00:00,  4.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 1128/1128 [00:00<00:00,  4.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 1129/1129 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 1130/1130 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 1131/1131 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 1132/1132 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 1133/1133 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1134/1134 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 1135/1135 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 1136/1136 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 1137/1137 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 1138/1138 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 1139/1139 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 1140/1140 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 1141/1141 [00:00<00:00,  4.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 1142/1142 [00:00<00:00,  4.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 1143/1143 [00:00<00:00,  3.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 1144/1144 [00:00<00:00,  3.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 1145/1145 [00:00<00:00,  3.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 1146/1146 [00:00<00:00,  3.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 1147/1147 [00:00<00:00,  3.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 1148/1148 [00:00<00:00,  3.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 1149/1149 [00:00<00:00,  3.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 1150/1150 [00:00<00:00,  3.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 1151/1151 [00:00<00:00,  3.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 1152/1152 [00:00<00:00,  3.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 1153/1153 [00:00<00:00,  3.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 1154/1154 [00:00<00:00,  3.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 1155/1155 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 1156/1156 [00:00<00:00,  3.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 1157/1157 [00:00<00:00,  4.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 1158/1158 [00:00<00:00,  4.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 1159/1159 [00:00<00:00,  4.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 1160/1160 [00:00<00:00,  4.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 1161/1161 [00:00<00:00,  4.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 1162/1162 [00:00<00:00,  4.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 1163/1163 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 1164/1164 [00:00<00:00,  4.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 1165/1165 [00:00<00:00,  4.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 1166/1166 [00:00<00:00,  4.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 1167/1167 [00:00<00:00,  4.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 1168/1168 [00:00<00:00,  4.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 1169/1169 [00:00<00:00,  4.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 1170/1170 [00:00<00:00,  4.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 1171/1171 [00:00<00:00,  4.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 1172/1172 [00:00<00:00,  4.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 1173/1173 [00:00<00:00,  4.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 1174/1174 [00:00<00:00,  4.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 1175/1175 [00:00<00:00,  4.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 1176/1176 [00:00<00:00,  3.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 1177/1177 [00:00<00:00,  4.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 1178/1178 [00:00<00:00,  4.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 1179/1179 [00:00<00:00,  4.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 1180/1180 [00:00<00:00,  3.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 1181/1181 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 1182/1182 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 1183/1183 [00:04<00:00,  4.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 1184/1184 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 1185/1185 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 1186/1186 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 1187/1187 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 1188/1188 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 1189/1189 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 1190/1190 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 1191/1191 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 1192/1192 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 1193/1193 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 1194/1194 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 1195/1195 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 1196/1196 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 1197/1197 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 1198/1198 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 1199/1199 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 1200/1200 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 1201/1201 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 1202/1202 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 1203/1203 [00:00<00:00,  4.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 1204/1204 [00:00<00:00,  4.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 1205/1205 [00:00<00:00,  4.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 1206/1206 [00:00<00:00,  4.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 1207/1207 [00:00<00:00,  4.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 1208/1208 [00:00<00:00,  4.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 1209/1209 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 1210/1210 [00:00<00:00,  4.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 1211/1211 [00:00<00:00,  4.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 1212/1212 [00:00<00:00,  4.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 1213/1213 [00:00<00:00,  4.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 1214/1214 [00:00<00:00,  4.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 1215/1215 [00:00<00:00,  4.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 1216/1216 [00:00<00:00,  4.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 1217/1217 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 1218/1218 [00:00<00:00,  4.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 1219/1219 [00:00<00:00,  4.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 1220/1220 [00:00<00:00,  4.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 1221/1221 [00:00<00:00,  4.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 1222/1222 [00:00<00:00,  4.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 1223/1223 [00:00<00:00,  4.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 1224/1224 [00:00<00:00,  4.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 1225/1225 [00:00<00:00,  4.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 1226/1226 [00:00<00:00,  4.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 1227/1227 [00:00<00:00,  4.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 1228/1228 [00:00<00:00,  4.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 1229/1229 [00:00<00:00,  2.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 1230/1230 [00:00<00:00,  2.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 1231/1231 [00:00<00:00,  2.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 1232/1232 [00:00<00:00,  2.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 1233/1233 [00:00<00:00,  2.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 1234/1234 [00:00<00:00,  2.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 1235/1235 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 1236/1236 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 1237/1237 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 1238/1238 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 1239/1239 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 1240/1240 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1241/1241 [00:00<00:00,  3.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 1242/1242 [00:00<00:00,  3.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 1243/1243 [00:00<00:00,  3.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 1244/1244 [00:00<00:00,  3.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 1245/1245 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1246/1246 [00:00<00:00,  3.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 1247/1247 [00:00<00:00,  3.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 1248/1248 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 1249/1249 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1250/1250 [00:00<00:00,  3.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 1251/1251 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 1252/1252 [00:00<00:00,  3.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 1253/1253 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1254/1254 [00:00<00:00,  3.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 1255/1255 [00:00<00:00,  3.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 1256/1256 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1257/1257 [00:00<00:00,  3.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 1258/1258 [00:00<00:00,  3.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 1259/1259 [00:00<00:00,  3.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 1260/1260 [00:00<00:00,  3.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 1261/1261 [00:00<00:00,  3.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 1262/1262 [00:00<00:00,  3.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 1263/1263 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1264/1264 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 1265/1265 [00:00<00:00,  3.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 1266/1266 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1267/1267 [00:00<00:00,  3.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 1268/1268 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1269/1269 [00:00<00:00,  3.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 1270/1270 [00:00<00:00,  3.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 1271/1271 [00:00<00:00,  3.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 1272/1272 [00:00<00:00,  3.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 1273/1273 [00:00<00:00,  3.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 1274/1274 [00:00<00:00,  3.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 1275/1275 [00:00<00:00,  3.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 1276/1276 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 1277/1277 [00:00<00:00,  2.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 1278/1278 [00:00<00:00,  2.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 1279/1279 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 1280/1280 [00:00<00:00,  3.15trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1280/1281 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1281/1281 [00:00<00:00,  2.93trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1281/1282 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1282/1282 [00:00<00:00,  2.85trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1282/1283 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [00:00<00:00,  2.98trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1283/1284 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1284/1284 [00:00<00:00,  2.81trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1284/1285 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1285/1285 [00:00<00:00,  2.89trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1285/1286 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1286/1286 [00:00<00:00,  2.90trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1286/1287 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1287/1287 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1287/1288 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1288/1288 [00:00<00:00,  2.81trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1288/1289 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1289/1289 [00:00<00:00,  2.88trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1289/1290 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1290/1290 [00:00<00:00,  2.91trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1290/1291 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1291/1291 [00:00<00:00,  2.80trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1291/1292 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 1293/1293 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1293/1294 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1294/1294 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 1295/1295 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1295/1296 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1296/1296 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 1297/1297 [00:00<00:00,  2.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 1298/1298 [00:00<00:00,  2.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1299/1299 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 1300/1300 [00:00<00:00,  2.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 1301/1301 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 1302/1302 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 1303/1303 [00:00<00:00,  2.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 1304/1304 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 1305/1305 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 1306/1306 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 1307/1307 [00:00<00:00,  2.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 1308/1308 [00:00<00:00,  2.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 1309/1309 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 1310/1310 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1310/1311 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1311/1311 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 1312/1312 [00:00<00:00,  3.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 1313/1313 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1313/1314 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1314/1314 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 1315/1315 [00:00<00:00,  2.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 1316/1316 [00:00<00:00,  2.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 1317/1317 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1317/1318 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1318/1318 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 1319/1319 [00:00<00:00,  2.78trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1319/1320 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1320/1320 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1320/1321 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1321/1321 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1321/1322 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1322/1322 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 1323/1323 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1323/1324 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1324/1324 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 1325/1325 [00:00<00:00,  2.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 1326/1326 [00:00<00:00,  2.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 1327/1327 [00:00<00:00,  2.81trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1327/1328 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1328/1328 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 1329/1329 [00:00<00:00,  2.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 1330/1330 [00:00<00:00,  2.28trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1330/1331 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1331/1331 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 1332/1332 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1332/1333 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1333/1333 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 1334/1334 [00:00<00:00,  3.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 1335/1335 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1335/1336 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1336/1336 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 1337/1337 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 1338/1338 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 1339/1339 [00:00<00:00,  2.90trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1339/1340 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1340/1340 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 1341/1341 [00:00<00:00,  2.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 1342/1342 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 1343/1343 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 1344/1344 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 1345/1345 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 1346/1346 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 1347/1347 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 1348/1348 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 1349/1349 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 1350/1350 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 1351/1351 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 1352/1352 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 1353/1353 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 1354/1354 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 1355/1355 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 1356/1356 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 1357/1357 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 1358/1358 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 1359/1359 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 1360/1360 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 1361/1361 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 1362/1362 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 1363/1363 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 1364/1364 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 1365/1365 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 1366/1366 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 1367/1367 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 1368/1368 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 1369/1369 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 1370/1370 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 1371/1371 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 1372/1372 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 1373/1373 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 1374/1374 [00:00<00:00,  4.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 1375/1375 [00:00<00:00,  4.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 1376/1376 [00:00<00:00,  4.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1377/1377 [00:00<00:00,  4.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 1378/1378 [00:00<00:00,  4.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 1379/1379 [00:00<00:00,  4.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 1380/1380 [00:00<00:00,  4.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 1381/1381 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 1382/1382 [00:00<00:00,  4.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1383/1383 [00:00<00:00,  4.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 1384/1384 [00:00<00:00,  4.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 1385/1385 [00:00<00:00,  4.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 1386/1386 [00:00<00:00,  4.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 1387/1387 [00:00<00:00,  4.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 1388/1388 [00:00<00:00,  4.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 1389/1389 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 1390/1390 [00:00<00:00,  4.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 1391/1391 [00:00<00:00,  4.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 1392/1392 [00:00<00:00,  4.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 1393/1393 [00:00<00:00,  4.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 1394/1394 [00:00<00:00,  4.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 1395/1395 [00:00<00:00,  4.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 1396/1396 [00:00<00:00,  3.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 1397/1397 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 1398/1398 [00:00<00:00,  4.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 1399/1399 [00:00<00:00,  4.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 1400/1400 [00:00<00:00,  4.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 1401/1401 [00:00<00:00,  4.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 1402/1402 [00:00<00:00,  4.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 1403/1403 [00:00<00:00,  4.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 1404/1404 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 1405/1405 [00:00<00:00,  4.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 1406/1406 [00:00<00:00,  4.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 1407/1407 [00:00<00:00,  4.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 1408/1408 [00:00<00:00,  4.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 1409/1409 [00:00<00:00,  4.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 1410/1410 [00:00<00:00,  4.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 1411/1411 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 1412/1412 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 1413/1413 [00:00<00:00,  4.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 1414/1414 [00:00<00:00,  4.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 1415/1415 [00:00<00:00,  4.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 1416/1416 [00:00<00:00,  4.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 1417/1417 [00:00<00:00,  4.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 1418/1418 [00:00<00:00,  4.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 1419/1419 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 1420/1420 [00:00<00:00,  4.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 1421/1421 [00:00<00:00,  4.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 1422/1422 [00:00<00:00,  4.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 1423/1423 [00:00<00:00,  3.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 1424/1424 [00:00<00:00,  4.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 1425/1425 [00:00<00:00,  4.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 1426/1426 [00:00<00:00,  4.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 1427/1427 [00:00<00:00,  3.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 1428/1428 [00:00<00:00,  3.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 1429/1429 [00:00<00:00,  3.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 1430/1430 [00:00<00:00,  3.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 1431/1431 [00:00<00:00,  3.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1432/1432 [00:00<00:00,  3.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 1433/1433 [00:00<00:00,  3.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 1434/1434 [00:00<00:00,  4.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 1435/1435 [00:00<00:00,  4.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 1436/1436 [00:00<00:00,  4.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 1437/1437 [00:00<00:00,  3.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 1438/1438 [00:00<00:00,  3.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 1439/1439 [00:00<00:00,  3.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 1440/1440 [00:00<00:00,  3.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1441/1441 [00:00<00:00,  4.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 1442/1442 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 1443/1443 [00:00<00:00,  3.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 1444/1444 [00:00<00:00,  4.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 1445/1445 [00:00<00:00,  4.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 1446/1446 [00:00<00:00,  4.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 1447/1447 [00:00<00:00,  4.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 1448/1448 [00:00<00:00,  4.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 1449/1449 [00:00<00:00,  4.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 1450/1450 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 1451/1451 [00:00<00:00,  3.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1452/1452 [00:00<00:00,  3.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 1453/1453 [00:00<00:00,  3.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 1454/1454 [00:00<00:00,  4.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 1455/1455 [00:00<00:00,  3.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 1456/1456 [00:00<00:00,  4.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 1457/1457 [00:00<00:00,  4.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 1458/1458 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 1459/1459 [00:00<00:00,  4.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 1460/1460 [00:00<00:00,  3.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 1461/1461 [00:00<00:00,  3.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 1462/1462 [00:00<00:00,  3.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 1463/1463 [00:00<00:00,  3.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 1464/1464 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 1465/1465 [00:00<00:00,  3.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 1466/1466 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 1467/1467 [00:00<00:00,  3.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 1468/1468 [00:00<00:00,  3.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 1469/1469 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 1470/1470 [00:00<00:00,  3.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 1471/1471 [00:00<00:00,  3.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 1472/1472 [00:00<00:00,  3.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 1473/1473 [00:00<00:00,  3.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 1474/1474 [00:00<00:00,  3.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 1475/1475 [00:00<00:00,  3.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 1476/1476 [00:00<00:00,  3.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 1477/1477 [00:00<00:00,  3.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 1478/1478 [00:00<00:00,  3.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 1479/1479 [00:00<00:00,  3.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 1480/1480 [00:00<00:00,  3.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 1481/1481 [00:00<00:00,  2.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 1482/1482 [00:00<00:00,  3.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 1483/1483 [00:00<00:00,  2.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 1484/1484 [00:00<00:00,  2.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 1485/1485 [00:00<00:00,  2.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 1486/1486 [00:00<00:00,  2.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 1487/1487 [00:00<00:00,  2.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1488/1488 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 1489/1489 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1490/1490 [00:00<00:00,  2.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 1491/1491 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 1492/1492 [00:00<00:00,  2.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 1493/1493 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 1494/1494 [00:00<00:00,  2.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 1495/1495 [00:00<00:00,  2.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1496/1496 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 1497/1497 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 1498/1498 [00:00<00:00,  2.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 1499/1499 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 1500/1500 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 1501/1501 [00:00<00:00,  2.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 1502/1502 [00:00<00:00,  2.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 1503/1503 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1504/1504 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 1505/1505 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 1506/1506 [00:00<00:00,  2.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 1507/1507 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 1508/1508 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 1509/1509 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 1510/1510 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 1511/1511 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 1512/1512 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 1513/1513 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 1514/1514 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 1515/1515 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 1516/1516 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 1517/1517 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 1518/1518 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 1519/1519 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1520/1520 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 1521/1521 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 1522/1522 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 1523/1523 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 1524/1524 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 1525/1525 [00:00<00:00,  3.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 1526/1526 [00:00<00:00,  3.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1527/1527 [00:00<00:00,  3.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1528/1528 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 1529/1529 [00:00<00:00,  3.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 1530/1530 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1531/1531 [00:00<00:00,  3.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 1532/1532 [00:00<00:00,  3.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 1533/1533 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 1534/1534 [00:00<00:00,  3.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 1535/1535 [00:00<00:00,  3.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1536/1536 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 1537/1537 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 1538/1538 [00:00<00:00,  3.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 1539/1539 [00:00<00:00,  3.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 1540/1540 [00:00<00:00,  3.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 1541/1541 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1542/1542 [00:00<00:00,  3.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 1543/1543 [00:00<00:00,  3.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 1544/1544 [00:00<00:00,  3.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 1545/1545 [00:00<00:00,  3.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 1546/1546 [00:00<00:00,  2.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 1547/1547 [00:00<00:00,  2.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1548/1548 [00:00<00:00,  2.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 1549/1549 [00:00<00:00,  3.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 1550/1550 [00:00<00:00,  3.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 1551/1551 [00:00<00:00,  3.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 1552/1552 [00:00<00:00,  3.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 1553/1553 [00:00<00:00,  2.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 1554/1554 [00:00<00:00,  2.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1555/1555 [00:00<00:00,  2.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 1556/1556 [00:00<00:00,  2.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1557/1557 [00:00<00:00,  2.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 1558/1558 [00:00<00:00,  2.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 1559/1559 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 1560/1560 [00:00<00:00,  2.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 1561/1561 [00:00<00:00,  2.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 1562/1562 [00:00<00:00,  2.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 1563/1563 [00:00<00:00,  2.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 1564/1564 [00:00<00:00,  2.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 1565/1565 [00:00<00:00,  2.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 1566/1566 [00:00<00:00,  2.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 1567/1567 [00:00<00:00,  2.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 1568/1568 [00:00<00:00,  3.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 1569/1569 [00:00<00:00,  2.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 1570/1570 [00:00<00:00,  2.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 1571/1571 [00:00<00:00,  2.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 1572/1572 [00:00<00:00,  2.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 1573/1573 [00:00<00:00,  2.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 1574/1574 [00:00<00:00,  2.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 1575/1575 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 1576/1576 [00:00<00:00,  2.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 1577/1577 [00:00<00:00,  2.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 1578/1578 [00:00<00:00,  2.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 1579/1579 [00:00<00:00,  2.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 1580/1580 [00:00<00:00,  3.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 1581/1581 [00:00<00:00,  3.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 1582/1582 [00:00<00:00,  3.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 1583/1583 [00:00<00:00,  3.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 1584/1584 [00:00<00:00,  3.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 1585/1585 [00:00<00:00,  3.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 1586/1586 [00:00<00:00,  3.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 1587/1587 [00:00<00:00,  3.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 1588/1588 [00:00<00:00,  3.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 1589/1589 [00:00<00:00,  3.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 1590/1590 [00:00<00:00,  3.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 1591/1591 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 1592/1592 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 1593/1593 [00:00<00:00,  3.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 1594/1594 [00:00<00:00,  3.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 1595/1595 [00:00<00:00,  3.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 1596/1596 [00:00<00:00,  3.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 1597/1597 [00:00<00:00,  3.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 1598/1598 [00:00<00:00,  3.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1599/1599 [00:00<00:00,  3.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 1600/1600 [00:05<00:00,  5.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 1601/1601 [00:05<00:00,  5.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 1602/1602 [00:04<00:00,  4.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 1603/1603 [00:04<00:00,  4.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 1604/1604 [00:04<00:00,  4.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 1605/1605 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 1606/1606 [00:05<00:00,  5.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 1607/1607 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 1608/1608 [00:05<00:00,  5.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 1609/1609 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 1610/1610 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 1611/1611 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 1612/1612 [00:00<00:00,  3.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 1613/1613 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 1614/1614 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 1615/1615 [00:00<00:00,  3.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 1616/1616 [00:00<00:00,  3.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1617/1617 [00:00<00:00,  3.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 1618/1618 [00:00<00:00,  3.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 1619/1619 [00:00<00:00,  3.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 1620/1620 [00:00<00:00,  3.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1621/1621 [00:00<00:00,  3.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 1622/1622 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 1623/1623 [00:00<00:00,  3.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 1624/1624 [00:00<00:00,  3.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 1625/1625 [00:00<00:00,  3.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 1626/1626 [00:00<00:00,  3.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 1627/1627 [00:00<00:00,  3.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 1628/1628 [00:00<00:00,  3.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 1629/1629 [00:00<00:00,  3.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1630/1630 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 1631/1631 [00:00<00:00,  3.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 1632/1632 [00:00<00:00,  3.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 1633/1633 [00:00<00:00,  3.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 1634/1634 [00:00<00:00,  3.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 1635/1635 [00:00<00:00,  3.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1636/1636 [00:00<00:00,  3.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 1637/1637 [00:00<00:00,  3.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 1638/1638 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 1639/1639 [00:00<00:00,  3.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 1640/1640 [00:00<00:00,  3.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 1641/1641 [00:00<00:00,  3.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1642/1642 [00:00<00:00,  3.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 1643/1643 [00:00<00:00,  3.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 1644/1644 [00:00<00:00,  3.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 1645/1645 [00:00<00:00,  3.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 1646/1646 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 1647/1647 [00:00<00:00,  3.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 1648/1648 [00:00<00:00,  3.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 1649/1649 [00:00<00:00,  3.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 1650/1650 [00:00<00:00,  3.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 1651/1651 [00:00<00:00,  3.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 1652/1652 [00:00<00:00,  3.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 1653/1653 [00:00<00:00,  3.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 1654/1654 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 1655/1655 [00:00<00:00,  3.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 1656/1656 [00:00<00:00,  3.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 1657/1657 [00:00<00:00,  3.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 1658/1658 [00:00<00:00,  3.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 1659/1659 [00:00<00:00,  3.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 1660/1660 [00:00<00:00,  3.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 1661/1661 [00:00<00:00,  3.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 1662/1662 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 1663/1663 [00:00<00:00,  3.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 1664/1664 [00:00<00:00,  3.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 1665/1665 [00:00<00:00,  3.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 1666/1666 [00:00<00:00,  3.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 1667/1667 [00:00<00:00,  3.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 1668/1668 [00:00<00:00,  3.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 1669/1669 [00:00<00:00,  3.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1670/1670 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 1671/1671 [00:00<00:00,  3.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 1672/1672 [00:00<00:00,  3.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1673/1673 [00:00<00:00,  3.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 1674/1674 [00:00<00:00,  3.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 1675/1675 [00:00<00:00,  3.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 1676/1676 [00:00<00:00,  3.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 1677/1677 [00:00<00:00,  3.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 1678/1678 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 1679/1679 [00:00<00:00,  3.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 1680/1680 [00:00<00:00,  3.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 1681/1681 [00:00<00:00,  3.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 1682/1682 [00:00<00:00,  3.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 1683/1683 [00:00<00:00,  3.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 1684/1684 [00:00<00:00,  3.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 1685/1685 [00:00<00:00,  3.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 1686/1686 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 1687/1687 [00:00<00:00,  3.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 1688/1688 [00:00<00:00,  3.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 1689/1689 [00:00<00:00,  3.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 1690/1690 [00:00<00:00,  3.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 1691/1691 [00:00<00:00,  1.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 1692/1692 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 1693/1693 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 1694/1694 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 1695/1695 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 1696/1696 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 1697/1697 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 1698/1698 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 1699/1699 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 1700/1700 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 1701/1701 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 1702/1702 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 1703/1703 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 1704/1704 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 1705/1705 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 1706/1706 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 1707/1707 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 1708/1708 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 1709/1709 [00:00<00:00,  3.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 1710/1710 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 1711/1711 [00:00<00:00,  3.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 1712/1712 [00:00<00:00,  2.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1713/1713 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 1714/1714 [00:00<00:00,  2.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 1715/1715 [00:00<00:00,  2.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 1716/1716 [00:00<00:00,  2.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 1717/1717 [00:00<00:00,  3.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 1718/1718 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 1719/1719 [00:00<00:00,  2.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 1720/1720 [00:00<00:00,  2.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1721/1721 [00:00<00:00,  2.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 1722/1722 [00:00<00:00,  2.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 1723/1723 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 1724/1724 [00:00<00:00,  2.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 1725/1725 [00:00<00:00,  2.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 1726/1726 [00:00<00:00,  2.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 1727/1727 [00:00<00:00,  2.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 1728/1728 [00:00<00:00,  3.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 1729/1729 [00:00<00:00,  2.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 1730/1730 [00:00<00:00,  2.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1731/1731 [00:00<00:00,  2.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 1732/1732 [00:00<00:00,  3.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 1733/1733 [00:00<00:00,  3.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 1734/1734 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 1735/1735 [00:00<00:00,  3.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 1736/1736 [00:00<00:00,  3.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 1737/1737 [00:00<00:00,  3.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 1738/1738 [00:00<00:00,  3.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 1739/1739 [00:00<00:00,  3.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 1740/1740 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 1741/1741 [00:00<00:00,  3.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 1742/1742 [00:00<00:00,  3.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 1743/1743 [00:00<00:00,  3.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 1744/1744 [00:00<00:00,  3.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 1745/1745 [00:00<00:00,  3.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 1746/1746 [00:00<00:00,  3.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 1747/1747 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 1748/1748 [00:00<00:00,  3.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 1749/1749 [00:00<00:00,  3.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 1750/1750 [00:00<00:00,  3.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 1751/1751 [00:00<00:00,  3.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 1752/1752 [00:00<00:00,  3.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 1753/1753 [00:00<00:00,  3.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 1754/1754 [00:00<00:00,  3.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 1755/1755 [00:00<00:00,  3.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 1756/1756 [00:00<00:00,  3.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 1757/1757 [00:00<00:00,  3.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 1758/1758 [00:00<00:00,  3.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 1759/1759 [00:00<00:00,  2.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 1760/1760 [00:00<00:00,  3.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 1761/1761 [00:00<00:00,  3.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 1762/1762 [00:00<00:00,  3.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 1763/1763 [00:00<00:00,  3.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 1764/1764 [00:00<00:00,  2.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1765/1765 [00:00<00:00,  3.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 1766/1766 [00:00<00:00,  3.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 1767/1767 [00:00<00:00,  3.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 1768/1768 [00:00<00:00,  2.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 1769/1769 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 1770/1770 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 1771/1771 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 1772/1772 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 1773/1773 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 1774/1774 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 1775/1775 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 1776/1776 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 1777/1777 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 1778/1778 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 1779/1779 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 1780/1780 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 1781/1781 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 1782/1782 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 1783/1783 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 1784/1784 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 1785/1785 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 1786/1786 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 1787/1787 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 1788/1788 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 1789/1789 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 1790/1790 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 1791/1791 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 1792/1792 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 1793/1793 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 1794/1794 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 1795/1795 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 1796/1796 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 1797/1797 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 1798/1798 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 1799/1799 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 1800/1800 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 1801/1801 [00:00<00:00,  3.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 1802/1802 [00:00<00:00,  3.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 1803/1803 [00:00<00:00,  3.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 1804/1804 [00:00<00:00,  3.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 1805/1805 [00:00<00:00,  3.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 1806/1806 [00:00<00:00,  3.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 1807/1807 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 1808/1808 [00:00<00:00,  3.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 1809/1809 [00:00<00:00,  3.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 1810/1810 [00:00<00:00,  3.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 1811/1811 [00:00<00:00,  3.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 1812/1812 [00:00<00:00,  3.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 1813/1813 [00:00<00:00,  3.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 1814/1814 [00:00<00:00,  3.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 1815/1815 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 1816/1816 [00:00<00:00,  3.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 1817/1817 [00:00<00:00,  3.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 1818/1818 [00:00<00:00,  3.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 1819/1819 [00:00<00:00,  3.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 1820/1820 [00:00<00:00,  3.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 1821/1821 [00:00<00:00,  3.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 1822/1822 [00:00<00:00,  3.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 1823/1823 [00:00<00:00,  3.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 1824/1824 [00:00<00:00,  3.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 1825/1825 [00:00<00:00,  3.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 1826/1826 [00:00<00:00,  3.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 1827/1827 [00:00<00:00,  3.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 1828/1828 [00:00<00:00,  3.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 1829/1829 [00:00<00:00,  3.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 1830/1830 [00:00<00:00,  3.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 1831/1831 [00:00<00:00,  3.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 1832/1832 [00:00<00:00,  3.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 1833/1833 [00:00<00:00,  3.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 1834/1834 [00:00<00:00,  3.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 1835/1835 [00:00<00:00,  3.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 1836/1836 [00:00<00:00,  3.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 1837/1837 [00:00<00:00,  3.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 1838/1838 [00:00<00:00,  3.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 1839/1839 [00:00<00:00,  2.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 1840/1840 [00:00<00:00,  2.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 1841/1841 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 1842/1842 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 1843/1843 [00:00<00:00,  2.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 1844/1844 [00:00<00:00,  2.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 1845/1845 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1846/1846 [00:00<00:00,  2.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 1847/1847 [00:00<00:00,  2.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 1848/1848 [00:00<00:00,  2.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 1849/1849 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1850/1850 [00:00<00:00,  2.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 1851/1851 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1852/1852 [00:00<00:00,  2.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 1853/1853 [00:00<00:00,  2.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 1854/1854 [00:00<00:00,  2.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 1855/1855 [00:00<00:00,  2.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 1856/1856 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 1857/1857 [00:00<00:00,  2.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 1858/1858 [00:00<00:00,  2.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1859/1859 [00:00<00:00,  2.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 1860/1860 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 1861/1861 [00:00<00:00,  2.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 1862/1862 [00:00<00:00,  2.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 1863/1863 [00:00<00:00,  2.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 1864/1864 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 1865/1865 [00:00<00:00,  2.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 1866/1866 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 1867/1867 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 1868/1868 [00:00<00:00,  2.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1869/1869 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 1870/1870 [00:00<00:00,  2.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1871/1871 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 1872/1872 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 1873/1873 [00:00<00:00,  2.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 1874/1874 [00:00<00:00,  2.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 1875/1875 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 1876/1876 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 1877/1877 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 1878/1878 [00:00<00:00,  2.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 1879/1879 [00:00<00:00,  2.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 1880/1880 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 1881/1881 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 1882/1882 [00:00<00:00,  2.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 1883/1883 [00:00<00:00,  2.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1884/1884 [00:00<00:00,  2.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 1885/1885 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 1886/1886 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 1887/1887 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 1888/1888 [00:00<00:00,  2.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 1889/1889 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 1890/1890 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 1891/1891 [00:00<00:00,  2.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 1892/1892 [00:00<00:00,  2.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 1893/1893 [00:00<00:00,  2.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1894/1894 [00:00<00:00,  2.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 1895/1895 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 1896/1896 [00:00<00:00,  2.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 1897/1897 [00:00<00:00,  2.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 1898/1898 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 1899/1899 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 1900/1900 [00:00<00:00,  2.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 1901/1901 [00:00<00:00,  2.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 1902/1902 [00:00<00:00,  2.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 1903/1903 [00:00<00:00,  2.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 1904/1904 [00:00<00:00,  2.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 1905/1905 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 1906/1906 [00:00<00:00,  2.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 1907/1907 [00:00<00:00,  2.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 1908/1908 [00:00<00:00,  2.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 1909/1909 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 1910/1910 [00:00<00:00,  2.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 1911/1911 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 1912/1912 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 1913/1913 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 1914/1914 [00:00<00:00,  1.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 1915/1915 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 1916/1916 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1916/1917 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1917/1917 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1917/1918 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1918/1918 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1918/1919 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1919/1919 [00:00<00:00,  2.38trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1919/1920 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1920/1920 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1920/1921 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1921/1921 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1921/1922 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1922/1922 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1922/1923 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1923/1923 [00:00<00:00,  2.46trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1923/1924 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1924/1924 [00:00<00:00,  2.40trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1924/1925 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1925/1925 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1925/1926 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1926/1926 [00:00<00:00,  2.25trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1926/1927 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1927/1927 [00:00<00:00,  2.26trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1927/1928 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1928/1928 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1928/1929 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1929/1929 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1929/1930 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1930/1930 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1930/1931 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1931/1931 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1931/1932 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1932/1932 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 1933/1933 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1933/1934 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1934/1934 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1934/1935 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1935/1935 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1935/1936 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1936/1936 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 1937/1937 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1937/1938 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1938/1938 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 1939/1939 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 1940/1940 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1940/1941 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1941/1941 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1941/1942 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1942/1942 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1942/1943 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1943/1943 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 1944/1944 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1944/1945 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1945/1945 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 1946/1946 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1946/1947 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1947/1947 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 1948/1948 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 1949/1949 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 1950/1950 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 1951/1951 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 1952/1952 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 1953/1953 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 1954/1954 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 1955/1955 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 1956/1956 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 1957/1957 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 1958/1958 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 1959/1959 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 1960/1960 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 1961/1961 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 1962/1962 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 1963/1963 [00:00<00:00,  2.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 1964/1964 [00:00<00:00,  2.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 1965/1965 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 1966/1966 [00:00<00:00,  2.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 1967/1967 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1967/1968 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1968/1968 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1968/1969 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1969/1969 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 1970/1970 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 1971/1971 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1971/1972 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1972/1972 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1972/1973 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1973/1973 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1973/1974 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1974/1974 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1974/1975 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1975/1975 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1975/1976 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1976/1976 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1976/1977 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1977/1977 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1977/1978 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1978/1978 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1978/1979 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1979/1979 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1979/1980 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1980/1980 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1980/1981 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1981/1981 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1981/1982 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1982/1982 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 1983/1983 [00:00<00:00,  2.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 1984/1984 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 1985/1985 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1985/1986 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1986/1986 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1986/1987 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1987/1987 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1987/1988 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1988/1988 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 1989/1989 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1989/1990 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1990/1990 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1990/1991 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1991/1991 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1991/1992 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1992/1992 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 1993/1993 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1993/1994 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1994/1994 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1994/1995 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1995/1995 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1995/1996 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1996/1996 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 1996/1997 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1997/1997 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 1998/1998 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 1999/1999 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 1999/2000 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2000/2001 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 2002/2002 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2002/2003 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2003/2003 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 2004/2004 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2004/2005 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2005/2005 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2005/2006 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2006/2006 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 2007/2007 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2008/2008 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 2009/2009 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 2010/2010 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 2011/2011 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 2012/2012 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 2013/2013 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 2014/2014 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 2015/2015 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 2016/2016 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 2017/2017 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 2018/2018 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 2019/2019 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 2020/2020 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 2021/2021 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 2022/2022 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 2023/2023 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 2024/2024 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 2025/2025 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 2026/2026 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 2027/2027 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 2028/2028 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 2029/2029 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 2030/2030 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 2031/2031 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 2032/2032 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 2033/2033 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 2034/2034 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 2035/2035 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 2036/2036 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 2037/2037 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 2038/2038 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 2039/2039 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 2040/2040 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 2041/2041 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 2042/2042 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 2043/2043 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 2044/2044 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 2045/2045 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 2046/2046 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 2047/2047 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 2048/2048 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 2049/2049 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 2050/2050 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 2051/2051 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 2052/2052 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 2053/2053 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 2054/2054 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 2055/2055 [00:00<00:00,  3.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 2056/2056 [00:00<00:00,  3.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2057/2057 [00:00<00:00,  3.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 2058/2058 [00:00<00:00,  3.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 2059/2059 [00:00<00:00,  3.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2060/2060 [00:00<00:00,  3.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 2061/2061 [00:00<00:00,  3.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 2062/2062 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 2063/2063 [00:00<00:00,  3.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 2064/2064 [00:00<00:00,  3.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 2065/2065 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 2066/2066 [00:00<00:00,  3.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2067/2067 [00:00<00:00,  3.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2068/2068 [00:00<00:00,  3.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2069/2069 [00:00<00:00,  3.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 2070/2070 [00:00<00:00,  3.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 2071/2071 [00:00<00:00,  3.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 2072/2072 [00:00<00:00,  3.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2073/2073 [00:00<00:00,  3.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 2074/2074 [00:00<00:00,  3.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 2075/2075 [00:00<00:00,  3.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2076/2076 [00:00<00:00,  3.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 2077/2077 [00:00<00:00,  3.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2078/2078 [00:00<00:00,  3.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 2079/2079 [00:00<00:00,  3.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2080/2080 [00:00<00:00,  3.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 2081/2081 [00:00<00:00,  3.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 2082/2082 [00:00<00:00,  3.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 2083/2083 [00:00<00:00,  3.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2084/2084 [00:00<00:00,  3.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 2085/2085 [00:00<00:00,  3.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 2086/2086 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 2087/2087 [00:00<00:00,  3.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 2088/2088 [00:00<00:00,  3.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 2089/2089 [00:00<00:00,  3.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 2090/2090 [00:00<00:00,  3.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 2091/2091 [00:00<00:00,  3.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 2092/2092 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 2093/2093 [00:00<00:00,  3.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 2094/2094 [00:00<00:00,  3.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2095/2095 [00:00<00:00,  3.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 2096/2096 [00:00<00:00,  3.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 2097/2097 [00:00<00:00,  3.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 2098/2098 [00:00<00:00,  3.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2099/2099 [00:00<00:00,  3.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 2100/2100 [00:00<00:00,  3.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 2101/2101 [00:00<00:00,  3.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 2102/2102 [00:00<00:00,  3.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2103/2103 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 2104/2104 [00:00<00:00,  3.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2105/2105 [00:00<00:00,  3.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 2106/2106 [00:00<00:00,  3.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 2107/2107 [00:00<00:00,  3.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 2108/2108 [00:00<00:00,  3.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 2109/2109 [00:00<00:00,  3.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 2110/2110 [00:00<00:00,  3.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2111/2111 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 2112/2112 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 2113/2113 [00:00<00:00,  3.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2114/2114 [00:00<00:00,  3.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 2115/2115 [00:00<00:00,  3.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 2116/2116 [00:00<00:00,  3.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 2117/2117 [00:00<00:00,  3.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 2118/2118 [00:00<00:00,  3.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2119/2119 [00:00<00:00,  3.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 2120/2120 [00:00<00:00,  3.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2121/2121 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 2122/2122 [00:00<00:00,  3.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 2123/2123 [00:00<00:00,  3.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2124/2124 [00:00<00:00,  3.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 2125/2125 [00:00<00:00,  3.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 2126/2126 [00:00<00:00,  3.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 2127/2127 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 2128/2128 [00:00<00:00,  3.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2129/2129 [00:00<00:00,  3.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 2130/2130 [00:00<00:00,  3.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 2131/2131 [00:00<00:00,  3.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 2132/2132 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 2133/2133 [00:00<00:00,  3.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 2134/2134 [00:00<00:00,  3.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 2135/2135 [00:00<00:00,  3.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 2136/2136 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 2137/2137 [00:00<00:00,  3.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 2138/2138 [00:00<00:00,  3.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 2139/2139 [00:00<00:00,  3.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 2140/2140 [00:00<00:00,  3.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 2141/2141 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 2142/2142 [00:00<00:00,  3.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 2143/2143 [00:00<00:00,  3.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 2144/2144 [00:00<00:00,  3.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 2145/2145 [00:00<00:00,  3.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 2146/2146 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 2147/2147 [00:00<00:00,  3.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 2148/2148 [00:00<00:00,  3.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 2149/2149 [00:00<00:00,  3.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 2150/2150 [00:00<00:00,  3.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2151/2151 [00:00<00:00,  3.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 2152/2152 [00:00<00:00,  3.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2153/2153 [00:00<00:00,  3.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2154/2154 [00:00<00:00,  3.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 2155/2155 [00:00<00:00,  3.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 2156/2156 [00:00<00:00,  3.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 2157/2157 [00:00<00:00,  3.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 2158/2158 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 2159/2159 [00:00<00:00,  3.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2160/2160 [00:00<00:00,  3.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 2161/2161 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 2162/2162 [00:00<00:00,  3.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 2163/2163 [00:00<00:00,  3.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 2164/2164 [00:00<00:00,  3.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 2165/2165 [00:00<00:00,  3.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 2166/2166 [00:00<00:00,  3.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 2167/2167 [00:00<00:00,  3.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 2168/2168 [00:00<00:00,  3.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 2169/2169 [00:00<00:00,  3.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2170/2170 [00:00<00:00,  3.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 2171/2171 [00:00<00:00,  2.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 2172/2172 [00:00<00:00,  3.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 2173/2173 [00:00<00:00,  3.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 2174/2174 [00:00<00:00,  3.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 2175/2175 [00:00<00:00,  3.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 2176/2176 [00:00<00:00,  3.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2177/2177 [00:00<00:00,  3.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 2178/2178 [00:00<00:00,  3.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 2179/2179 [00:00<00:00,  3.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 2180/2180 [00:00<00:00,  3.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 2181/2181 [00:00<00:00,  3.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 2182/2182 [00:00<00:00,  3.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 2183/2183 [00:00<00:00,  3.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2184/2184 [00:00<00:00,  3.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 2185/2185 [00:00<00:00,  2.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 2186/2186 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 2187/2187 [00:00<00:00,  2.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 2188/2188 [00:00<00:00,  2.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 2189/2189 [00:00<00:00,  2.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 2190/2190 [00:00<00:00,  2.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 2191/2191 [00:00<00:00,  2.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 2192/2192 [00:00<00:00,  2.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 2193/2193 [00:00<00:00,  2.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 2194/2194 [00:00<00:00,  2.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 2195/2195 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 2196/2196 [00:00<00:00,  2.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 2197/2197 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 2198/2198 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 2199/2199 [00:00<00:00,  2.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 2200/2200 [00:00<00:00,  2.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 2201/2201 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 2202/2202 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 2203/2203 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 2204/2204 [00:00<00:00,  2.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 2205/2205 [00:00<00:00,  2.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 2206/2206 [00:00<00:00,  2.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 2207/2207 [00:00<00:00,  2.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 2208/2208 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 2209/2209 [00:00<00:00,  2.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 2210/2210 [00:00<00:00,  2.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 2211/2211 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2212/2212 [00:00<00:00,  2.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 2213/2213 [00:00<00:00,  2.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 2214/2214 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 2215/2215 [00:00<00:00,  2.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 2216/2216 [00:00<00:00,  2.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2217/2217 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 2218/2218 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 2219/2219 [00:00<00:00,  2.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2220/2220 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 2221/2221 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2222/2222 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 2223/2223 [00:00<00:00,  2.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 2224/2224 [00:00<00:00,  2.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2225/2225 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 2226/2226 [00:00<00:00,  2.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 2227/2227 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 2228/2228 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 2229/2229 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 2230/2230 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 2231/2231 [00:00<00:00,  2.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 2232/2232 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 2233/2233 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 2234/2234 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 2235/2235 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 2236/2236 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 2237/2237 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 2238/2238 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 2239/2239 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 2240/2240 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 2241/2241 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 2242/2242 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 2243/2243 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 2244/2244 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 2245/2245 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 2246/2246 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 2247/2247 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 2248/2248 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 2249/2249 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 2250/2250 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 2251/2251 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 2252/2252 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 2253/2253 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 2254/2254 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 2255/2255 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 2256/2256 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 2257/2257 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 2258/2258 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 2259/2259 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 2260/2260 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2261/2261 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2262/2262 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2263/2263 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2264/2264 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 2265/2265 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 2266/2266 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2267/2267 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 2268/2268 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2269/2269 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 2270/2270 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2271/2271 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 2272/2272 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 2273/2273 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 2274/2274 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 2275/2275 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 2276/2276 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 2277/2277 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 2278/2278 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 2279/2279 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 2280/2280 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 2281/2281 [00:00<00:00,  3.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 2282/2282 [00:00<00:00,  3.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2283/2283 [00:00<00:00,  3.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 2284/2284 [00:00<00:00,  3.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 2285/2285 [00:00<00:00,  3.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 2286/2286 [00:00<00:00,  3.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 2287/2287 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 2288/2288 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 2289/2289 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 2290/2290 [00:00<00:00,  2.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 2291/2291 [00:00<00:00,  2.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 2292/2292 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 2293/2293 [00:00<00:00,  2.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 2294/2294 [00:00<00:00,  2.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 2295/2295 [00:00<00:00,  2.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 2296/2296 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 2297/2297 [00:00<00:00,  2.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 2298/2298 [00:00<00:00,  2.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 2299/2299 [00:00<00:00,  2.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 2300/2300 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 2301/2301 [00:00<00:00,  2.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 2302/2302 [00:00<00:00,  2.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 2303/2303 [00:00<00:00,  2.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 2304/2304 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 2305/2305 [00:00<00:00,  2.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 2306/2306 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2307/2307 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 2308/2308 [00:00<00:00,  2.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 2309/2309 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 2310/2310 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 2311/2311 [00:00<00:00,  2.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 2312/2312 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 2313/2313 [00:00<00:00,  2.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 2314/2314 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 2315/2315 [00:00<00:00,  2.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 2316/2316 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 2317/2317 [00:00<00:00,  2.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 2318/2318 [00:00<00:00,  2.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 2319/2319 [00:00<00:00,  2.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 2320/2320 [00:00<00:00,  2.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 2321/2321 [00:00<00:00,  2.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 2322/2322 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 2323/2323 [00:00<00:00,  2.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 2324/2324 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 2325/2325 [00:00<00:00,  2.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 2326/2326 [00:00<00:00,  2.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 2327/2327 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 2328/2328 [00:00<00:00,  2.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 2329/2329 [00:00<00:00,  2.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 2330/2330 [00:00<00:00,  2.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 2331/2331 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2332/2332 [00:00<00:00,  2.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 2333/2333 [00:00<00:00,  2.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 2334/2334 [00:00<00:00,  2.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 2335/2335 [00:00<00:00,  2.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 2336/2336 [00:00<00:00,  2.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 2337/2337 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 2338/2338 [00:00<00:00,  2.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 2339/2339 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 2340/2340 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 2341/2341 [00:00<00:00,  2.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 2342/2342 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 2343/2343 [00:00<00:00,  2.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 2344/2344 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 2345/2345 [00:00<00:00,  2.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 2346/2346 [00:00<00:00,  2.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 2347/2347 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 2348/2348 [00:00<00:00,  2.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 2349/2349 [00:00<00:00,  2.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 2350/2350 [00:00<00:00,  2.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 2351/2351 [00:00<00:00,  2.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 2352/2352 [00:00<00:00,  2.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 2353/2353 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 2354/2354 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 2355/2355 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 2356/2356 [00:00<00:00,  2.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 2357/2357 [00:00<00:00,  2.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 2358/2358 [00:00<00:00,  2.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 2359/2359 [00:00<00:00,  2.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 2360/2360 [00:00<00:00,  2.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 2361/2361 [00:00<00:00,  2.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 2362/2362 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 2363/2363 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 2364/2364 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 2365/2365 [00:00<00:00,  2.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 2366/2366 [00:00<00:00,  2.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 2367/2367 [00:00<00:00,  2.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 2368/2368 [00:00<00:00,  2.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 2369/2369 [00:00<00:00,  2.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 2370/2370 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 2371/2371 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 2372/2372 [00:00<00:00,  2.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 2373/2373 [00:00<00:00,  2.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 2374/2374 [00:00<00:00,  2.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 2375/2375 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 2376/2376 [00:00<00:00,  2.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 2377/2377 [00:00<00:00,  2.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 2378/2378 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 2379/2379 [00:00<00:00,  2.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 2380/2380 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 2381/2381 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2382/2382 [00:00<00:00,  2.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 2383/2383 [00:00<00:00,  2.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 2384/2384 [00:00<00:00,  2.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 2385/2385 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 2386/2386 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 2387/2387 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 2388/2388 [00:00<00:00,  2.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 2389/2389 [00:00<00:00,  2.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 2390/2390 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 2391/2391 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 2392/2392 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 2393/2393 [00:00<00:00,  2.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 2394/2394 [00:00<00:00,  2.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 2395/2395 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 2396/2396 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 2397/2397 [00:06<00:00,  6.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 2398/2398 [00:05<00:00,  5.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 2399/2399 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 2400/2400 [00:05<00:00,  5.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 2401/2401 [00:05<00:00,  5.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 2402/2402 [00:04<00:00,  4.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 2403/2403 [00:05<00:00,  5.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 2404/2404 [00:04<00:00,  4.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 2405/2405 [00:04<00:00,  4.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 2406/2406 [00:04<00:00,  4.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 2407/2407 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 2408/2408 [00:05<00:00,  5.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 2409/2409 [00:04<00:00,  4.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 2410/2410 [00:04<00:00,  4.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 2411/2411 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 2412/2412 [00:05<00:00,  5.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 2413/2413 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 2414/2414 [00:04<00:00,  4.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 2415/2415 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 2416/2416 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 2417/2417 [00:00<00:00,  2.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 2418/2418 [00:00<00:00,  2.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 2419/2419 [00:00<00:00,  2.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 2420/2420 [00:00<00:00,  2.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 2421/2421 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 2422/2422 [00:00<00:00,  2.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 2423/2423 [00:00<00:00,  2.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 2424/2424 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 2425/2425 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 2426/2426 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 2427/2427 [00:00<00:00,  2.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 2428/2428 [00:00<00:00,  2.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 2429/2429 [00:00<00:00,  2.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 2430/2430 [00:00<00:00,  2.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 2431/2431 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 2432/2432 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 2433/2433 [00:00<00:00,  2.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 2434/2434 [00:00<00:00,  2.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 2435/2435 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 2436/2436 [00:00<00:00,  2.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 2437/2437 [00:00<00:00,  2.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 2438/2438 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 2439/2439 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 2440/2440 [00:00<00:00,  2.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 2441/2441 [00:00<00:00,  2.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 2442/2442 [00:00<00:00,  2.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 2443/2443 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 2444/2444 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 2445/2445 [00:00<00:00,  2.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 2446/2446 [00:00<00:00,  2.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 2447/2447 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 2448/2448 [00:00<00:00,  2.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 2449/2449 [00:00<00:00,  2.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 2450/2450 [00:00<00:00,  2.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 2451/2451 [00:00<00:00,  2.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 2452/2452 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 2453/2453 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 2454/2454 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 2455/2455 [00:00<00:00,  2.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 2456/2456 [00:00<00:00,  2.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 2457/2457 [00:00<00:00,  2.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 2458/2458 [00:00<00:00,  2.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 2459/2459 [00:00<00:00,  2.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 2460/2460 [00:00<00:00,  2.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 2461/2461 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 2462/2462 [00:00<00:00,  2.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 2463/2463 [00:00<00:00,  2.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 2464/2464 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2465/2465 [00:00<00:00,  2.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 2466/2466 [00:00<00:00,  2.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 2467/2467 [00:00<00:00,  2.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 2468/2468 [00:00<00:00,  2.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 2469/2469 [00:00<00:00,  3.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 2470/2470 [00:00<00:00,  2.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 2471/2471 [00:00<00:00,  3.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 2472/2472 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 2473/2473 [00:00<00:00,  3.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 2474/2474 [00:00<00:00,  3.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 2475/2475 [00:00<00:00,  3.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 2476/2476 [00:00<00:00,  3.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 2477/2477 [00:00<00:00,  3.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 2478/2478 [00:00<00:00,  3.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 2479/2479 [00:00<00:00,  3.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 2480/2480 [00:00<00:00,  3.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 2481/2481 [00:00<00:00,  3.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 2482/2482 [00:00<00:00,  3.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 2483/2483 [00:00<00:00,  3.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 2484/2484 [00:00<00:00,  3.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 2485/2485 [00:00<00:00,  3.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 2486/2486 [00:00<00:00,  3.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 2487/2487 [00:00<00:00,  3.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 2488/2488 [00:00<00:00,  3.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 2489/2489 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2490/2490 [00:00<00:00,  3.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 2491/2491 [00:00<00:00,  2.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 2492/2492 [00:00<00:00,  2.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 2493/2493 [00:00<00:00,  2.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 2494/2494 [00:00<00:00,  3.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 2495/2495 [00:00<00:00,  3.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 2496/2496 [00:00<00:00,  3.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 2497/2497 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 2498/2498 [00:00<00:00,  3.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 2499/2499 [00:00<00:00,  2.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 2500/2500 [00:00<00:00,  3.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 2501/2501 [00:00<00:00,  3.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 2502/2502 [00:00<00:00,  3.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 2503/2503 [00:00<00:00,  2.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 2504/2504 [00:00<00:00,  2.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 2505/2505 [00:00<00:00,  2.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 2506/2506 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 2507/2507 [00:00<00:00,  2.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 2508/2508 [00:00<00:00,  2.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 2509/2509 [00:00<00:00,  2.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 2510/2510 [00:00<00:00,  2.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 2511/2511 [00:00<00:00,  2.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 2512/2512 [00:00<00:00,  2.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 2513/2513 [00:00<00:00,  2.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 2514/2514 [00:00<00:00,  2.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 2515/2515 [00:00<00:00,  2.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 2516/2516 [00:00<00:00,  2.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 2517/2517 [00:00<00:00,  2.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 2518/2518 [00:00<00:00,  2.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 2519/2519 [00:00<00:00,  2.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 2520/2520 [00:00<00:00,  2.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 2521/2521 [00:00<00:00,  2.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 2522/2522 [00:00<00:00,  2.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 2523/2523 [00:00<00:00,  2.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 2524/2524 [00:00<00:00,  2.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 2525/2525 [00:00<00:00,  2.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 2526/2526 [00:00<00:00,  2.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 2527/2527 [00:00<00:00,  2.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 2528/2528 [00:00<00:00,  2.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 2529/2529 [00:00<00:00,  2.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 2530/2530 [00:00<00:00,  2.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 2531/2531 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 2532/2532 [00:00<00:00,  2.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 2533/2533 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 2534/2534 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 2535/2535 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 2536/2536 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 2537/2537 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 2538/2538 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 2539/2539 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 2540/2540 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 2541/2541 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 2542/2542 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 2543/2543 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 2544/2544 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 2545/2545 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 2546/2546 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 2547/2547 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 2548/2548 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 2549/2549 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 2550/2550 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 2551/2551 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 2552/2552 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 2553/2553 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 2554/2554 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 2555/2555 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 2556/2556 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 2557/2557 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 2558/2558 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 2559/2559 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 2560/2560 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 2561/2561 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 2562/2562 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 2563/2563 [00:00<00:00,  2.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 2564/2564 [00:00<00:00,  2.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 2565/2565 [00:00<00:00,  2.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 2566/2566 [00:00<00:00,  2.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 2567/2567 [00:00<00:00,  2.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 2568/2568 [00:00<00:00,  2.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 2569/2569 [00:00<00:00,  2.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 2570/2570 [00:00<00:00,  2.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 2571/2571 [00:00<00:00,  2.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 2572/2572 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 2573/2573 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 2574/2574 [00:00<00:00,  2.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 2575/2575 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 2576/2576 [00:00<00:00,  2.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2577/2577 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 2578/2578 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 2579/2579 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2580/2580 [00:00<00:00,  2.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2581/2581 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 2582/2582 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2583/2583 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2584/2584 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 2585/2585 [00:00<00:00,  2.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 2586/2586 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 2587/2587 [00:00<00:00,  2.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 2588/2588 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 2589/2589 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 2590/2590 [00:00<00:00,  2.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 2591/2591 [00:00<00:00,  2.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 2592/2592 [00:00<00:00,  2.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 2593/2593 [00:00<00:00,  2.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 2594/2594 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 2595/2595 [00:00<00:00,  2.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 2596/2596 [00:00<00:00,  2.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 2597/2597 [00:00<00:00,  2.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 2598/2598 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2599/2599 [00:00<00:00,  2.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 2600/2600 [00:00<00:00,  2.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 2601/2601 [00:00<00:00,  2.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 2602/2602 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 2603/2603 [00:00<00:00,  2.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 2604/2604 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 2605/2605 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 2606/2606 [00:00<00:00,  2.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 2607/2607 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2608/2608 [00:00<00:00,  2.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 2609/2609 [00:00<00:00,  2.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 2610/2610 [00:00<00:00,  2.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 2611/2611 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 2612/2612 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 2613/2613 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 2614/2614 [00:00<00:00,  2.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 2615/2615 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 2616/2616 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 2617/2617 [00:00<00:00,  2.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 2618/2618 [00:00<00:00,  2.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 2619/2619 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 2620/2620 [00:00<00:00,  2.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 2621/2621 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 2622/2622 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 2623/2623 [00:00<00:00,  2.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 2624/2624 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 2625/2625 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 2626/2626 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 2627/2627 [00:00<00:00,  2.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 2628/2628 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 2629/2629 [00:00<00:00,  2.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 2630/2630 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 2631/2631 [00:00<00:00,  2.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 2632/2632 [00:00<00:00,  2.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2633/2633 [00:00<00:00,  2.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 2634/2634 [00:00<00:00,  2.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 2635/2635 [00:00<00:00,  2.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 2636/2636 [00:00<00:00,  2.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 2637/2637 [00:00<00:00,  2.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 2638/2638 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 2639/2639 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 2640/2640 [00:00<00:00,  2.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 2641/2641 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 2642/2642 [00:00<00:00,  2.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2643/2643 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 2644/2644 [00:00<00:00,  2.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 2645/2645 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2646/2646 [00:00<00:00,  2.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2647/2647 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 2648/2648 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2649/2649 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 2650/2650 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 2651/2651 [00:00<00:00,  2.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2652/2652 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 2653/2653 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 2654/2654 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 2655/2655 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 2656/2656 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 2657/2657 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 2658/2658 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 2659/2659 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 2660/2660 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 2661/2661 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 2662/2662 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 2663/2663 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 2664/2664 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 2665/2665 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 2666/2666 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 2667/2667 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 2668/2668 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 2669/2669 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 2670/2670 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 2671/2671 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 2672/2672 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 2673/2673 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 2674/2674 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 2675/2675 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 2676/2676 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 2677/2677 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 2678/2678 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 2679/2679 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 2680/2680 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 2681/2681 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 2682/2682 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 2683/2683 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 2684/2684 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 2685/2685 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 2686/2686 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 2687/2687 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 2688/2688 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 2689/2689 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 2690/2690 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 2691/2691 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 2692/2692 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 2693/2693 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 2694/2694 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 2695/2695 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 2696/2696 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 2697/2697 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 2698/2698 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 2699/2699 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 2700/2700 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 2701/2701 [00:00<00:00,  2.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 2702/2702 [00:00<00:00,  2.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 2703/2703 [00:00<00:00,  2.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 2704/2704 [00:00<00:00,  2.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 2705/2705 [00:00<00:00,  2.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 2706/2706 [00:00<00:00,  2.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 2707/2707 [00:00<00:00,  2.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 2708/2708 [00:00<00:00,  2.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 2709/2709 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 2710/2710 [00:00<00:00,  2.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 2711/2711 [00:00<00:00,  2.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 2712/2712 [00:00<00:00,  2.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 2713/2713 [00:00<00:00,  2.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 2714/2714 [00:00<00:00,  2.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 2715/2715 [00:00<00:00,  2.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 2716/2716 [00:00<00:00,  2.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 2717/2717 [00:00<00:00,  2.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 2718/2718 [00:00<00:00,  3.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 2719/2719 [00:00<00:00,  2.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 2720/2720 [00:00<00:00,  2.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 2721/2721 [00:00<00:00,  2.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 2722/2722 [00:00<00:00,  2.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 2723/2723 [00:00<00:00,  2.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 2724/2724 [00:00<00:00,  2.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 2725/2725 [00:00<00:00,  2.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 2726/2726 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 2727/2727 [00:00<00:00,  2.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 2728/2728 [00:00<00:00,  2.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 2729/2729 [00:00<00:00,  2.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 2730/2730 [00:00<00:00,  2.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 2731/2731 [00:00<00:00,  2.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 2732/2732 [00:00<00:00,  2.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 2733/2733 [00:00<00:00,  2.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 2734/2734 [00:00<00:00,  2.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 2735/2735 [00:00<00:00,  2.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 2736/2736 [00:00<00:00,  2.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 2737/2737 [00:00<00:00,  2.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 2738/2738 [00:00<00:00,  2.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 2739/2739 [00:00<00:00,  2.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 2740/2740 [00:00<00:00,  2.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 2741/2741 [00:00<00:00,  2.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 2742/2742 [00:00<00:00,  2.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 2743/2743 [00:00<00:00,  2.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 2744/2744 [00:00<00:00,  2.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 2745/2745 [00:00<00:00,  2.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 2746/2746 [00:00<00:00,  2.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 2747/2747 [00:00<00:00,  2.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 2748/2748 [00:00<00:00,  2.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 2749/2749 [00:00<00:00,  2.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 2750/2750 [00:00<00:00,  2.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 2751/2751 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 2752/2752 [00:00<00:00,  2.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 2753/2753 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 2754/2754 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 2755/2755 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 2756/2756 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 2757/2757 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 2758/2758 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 2759/2759 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 2760/2760 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 2761/2761 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 2762/2762 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 2763/2763 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 2764/2764 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 2765/2765 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 2766/2766 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 2767/2767 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 2768/2768 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 2769/2769 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 2770/2770 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 2771/2771 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 2772/2772 [00:00<00:00,  2.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 2773/2773 [00:00<00:00,  2.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 2774/2774 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 2775/2775 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 2776/2776 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 2777/2777 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 2778/2778 [00:00<00:00,  2.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 2779/2779 [00:00<00:00,  2.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 2780/2780 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 2781/2781 [00:00<00:00,  2.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2782/2782 [00:00<00:00,  2.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2783/2783 [00:00<00:00,  2.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2784/2784 [00:00<00:00,  2.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 2785/2785 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 2786/2786 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 2787/2787 [00:00<00:00,  2.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2788/2788 [00:00<00:00,  2.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 2789/2789 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 2790/2790 [00:00<00:00,  2.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 2791/2791 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2792/2792 [00:00<00:00,  2.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 2793/2793 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 2794/2794 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2795/2795 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2796/2796 [00:00<00:00,  2.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2797/2797 [00:00<00:00,  2.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 2798/2798 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 2799/2799 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 2800/2800 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 2801/2801 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2802/2802 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 2803/2803 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2804/2804 [00:00<00:00,  2.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2805/2805 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2806/2806 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 2807/2807 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 2808/2808 [00:00<00:00,  2.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2809/2809 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2810/2810 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 2811/2811 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 2812/2812 [00:00<00:00,  2.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 2813/2813 [00:00<00:00,  2.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2814/2814 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2815/2815 [00:00<00:00,  2.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2816/2816 [00:00<00:00,  2.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 2817/2817 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 2818/2818 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 2819/2819 [00:00<00:00,  2.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 2820/2820 [00:00<00:00,  2.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 2821/2821 [00:00<00:00,  2.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2822/2822 [00:00<00:00,  2.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 2823/2823 [00:00<00:00,  2.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2824/2824 [00:00<00:00,  2.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 2825/2825 [00:00<00:00,  2.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2826/2826 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 2827/2827 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 2828/2828 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 2829/2829 [00:00<00:00,  2.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 2830/2830 [00:00<00:00,  2.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2831/2831 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 2832/2832 [00:00<00:00,  2.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 2833/2833 [00:00<00:00,  2.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2834/2834 [00:00<00:00,  2.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2835/2835 [00:00<00:00,  2.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 2836/2836 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 2837/2837 [00:00<00:00,  2.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2838/2838 [00:00<00:00,  2.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 2839/2839 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 2840/2840 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 2841/2841 [00:00<00:00,  2.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 2842/2842 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 2843/2843 [00:00<00:00,  2.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 2844/2844 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2845/2845 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 2846/2846 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 2847/2847 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2848/2848 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 2849/2849 [00:00<00:00,  2.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2850/2850 [00:00<00:00,  2.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 2851/2851 [00:00<00:00,  2.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2852/2852 [00:00<00:00,  2.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2853/2853 [00:00<00:00,  2.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 2854/2854 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 2855/2855 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2856/2856 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 2857/2857 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 2858/2858 [00:00<00:00,  2.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 2859/2859 [00:00<00:00,  2.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 2860/2860 [00:00<00:00,  2.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 2861/2861 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 2862/2862 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 2863/2863 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 2864/2864 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 2865/2865 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 2866/2866 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 2867/2867 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 2868/2868 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 2869/2869 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 2870/2870 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 2871/2871 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 2872/2872 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 2873/2873 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 2874/2874 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 2875/2875 [00:00<00:00,  2.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 2876/2876 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 2877/2877 [00:00<00:00,  2.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 2878/2878 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2878/2879 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2879/2879 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2879/2880 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2880/2880 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2880/2881 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2881/2881 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2881/2882 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2882/2882 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2882/2883 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2883/2883 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2883/2884 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2884/2884 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2884/2885 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2885/2885 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2885/2886 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2886/2886 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 2887/2887 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2887/2888 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2888/2888 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2888/2889 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2889/2889 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2889/2890 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2890/2890 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2890/2891 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2891/2891 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2891/2892 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2892/2892 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2892/2893 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2893/2893 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2893/2894 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2894/2894 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 2895/2895 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2895/2896 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2896/2896 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2896/2897 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2897/2897 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2897/2898 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2898/2898 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2898/2899 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2899/2899 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2899/2900 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2900/2900 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2900/2901 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2901/2901 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2901/2902 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2902/2902 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2902/2903 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2903/2903 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2903/2904 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2904/2904 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2904/2905 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2905/2905 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2905/2906 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2906/2906 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2906/2907 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2907/2907 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 2908/2908 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2908/2909 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2909/2909 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2909/2910 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2910/2910 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 2911/2911 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 2912/2912 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 2913/2913 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 2914/2914 [00:00<00:00,  1.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 2915/2915 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 2916/2916 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 2917/2917 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2917/2918 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2918/2918 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 2919/2919 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2919/2920 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2920/2920 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2920/2921 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2921/2921 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2921/2922 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2922/2922 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 2923/2923 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 2924/2924 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 2925/2925 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 2926/2926 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 2927/2927 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 2928/2928 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 2929/2929 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 2930/2930 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 2931/2931 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 2932/2932 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 2933/2933 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 2934/2934 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 2935/2935 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 2936/2936 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 2937/2937 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 2938/2938 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 2939/2939 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 2940/2940 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 2941/2941 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 2942/2942 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 2943/2943 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 2944/2944 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 2945/2945 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 2946/2946 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 2947/2947 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 2948/2948 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 2949/2949 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 2950/2950 [00:00<00:00,  2.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 2951/2951 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 2952/2952 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 2953/2953 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 2954/2954 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 2955/2955 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 2956/2956 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2956/2957 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2957/2957 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2957/2958 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2958/2958 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2958/2959 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2959/2959 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2959/2960 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2960/2960 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2960/2961 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2961/2961 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 2962/2962 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2962/2963 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2963/2963 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 2964/2964 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2964/2965 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2965/2965 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2965/2966 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2966/2967 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2967/2967 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2967/2968 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2968/2968 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2968/2969 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2969/2969 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 2970/2970 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2970/2971 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2971/2971 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 2972/2972 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2972/2973 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2973/2973 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2973/2974 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2974/2974 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2974/2975 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2975/2975 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 2976/2976 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2976/2977 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2977/2977 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 2978/2978 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 2979/2979 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2979/2980 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2980/2980 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2980/2981 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2981/2981 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2981/2982 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2982/2982 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 2983/2983 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2983/2984 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2984/2984 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2984/2985 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2985/2985 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 2986/2986 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2986/2987 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2987/2987 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2987/2988 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2988/2988 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 2989/2989 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2989/2990 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2990/2990 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2990/2991 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2991/2991 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2991/2992 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2992/2992 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 2993/2993 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2993/2994 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2994/2995 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2995/2995 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 2996/2996 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 2996/2997 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2997/2997 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2997/2998 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2998/2998 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2998/2999 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2999/2999 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 2999/3000 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 3001/3001 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 3001/3002 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3002/3002 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 3002/3003 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3003/3003 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 3004/3004 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 3004/3005 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3005/3005 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 3006/3006 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 3006/3007 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3007/3007 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 3007/3008 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3008/3008 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 3008/3009 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3009/3009 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 3009/3010 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3010/3010 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 3011/3011 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 3012/3012 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 3013/3013 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 3014/3014 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 3015/3015 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 3016/3016 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 3017/3017 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 3018/3018 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 3019/3019 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 3020/3020 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 3021/3021 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 3022/3022 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 3023/3023 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 3024/3024 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 3025/3025 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 3026/3026 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 3027/3027 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 3028/3028 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 3029/3029 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 3030/3030 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 3031/3031 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 3032/3032 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 3033/3033 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 3034/3034 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 3035/3035 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 3036/3036 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 3037/3037 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 3038/3038 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 3039/3039 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 3040/3040 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 3041/3041 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 3042/3042 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 3043/3043 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 3044/3044 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 3045/3045 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 3046/3046 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 3047/3047 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 3048/3048 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 3049/3049 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 3050/3050 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 3051/3051 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 3052/3052 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 3053/3053 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 3054/3054 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 3055/3055 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 3056/3056 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 3057/3057 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 3058/3058 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 3059/3059 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 3060/3060 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 3061/3061 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 3062/3062 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 3063/3063 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 3064/3064 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 3065/3065 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 3066/3066 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 3067/3067 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 3068/3068 [00:03<00:00,  3.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 3069/3069 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 3070/3070 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 3071/3071 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 3072/3072 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 3073/3073 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 3074/3074 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 3075/3075 [00:04<00:00,  4.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 3076/3076 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 3077/3077 [00:04<00:00,  4.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 3078/3078 [00:05<00:00,  5.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 3079/3079 [00:05<00:00,  5.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 3080/3080 [00:04<00:00,  4.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 3081/3081 [00:06<00:00,  6.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 3082/3082 [00:05<00:00,  5.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 3083/3083 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 3084/3084 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 3085/3085 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 3086/3086 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 3087/3087 [00:00<00:00,  2.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 3088/3088 [00:00<00:00,  2.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 3089/3089 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 3090/3090 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 3091/3091 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 3092/3092 [00:00<00:00,  2.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 3093/3093 [00:00<00:00,  2.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 3094/3094 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 3095/3095 [00:00<00:00,  2.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 3096/3096 [00:00<00:00,  2.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 3097/3097 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 3098/3098 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 3099/3099 [00:00<00:00,  2.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 3100/3100 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 3101/3101 [00:00<00:00,  2.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 3102/3102 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 3103/3103 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 3104/3104 [00:00<00:00,  2.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 3105/3105 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 3106/3106 [00:00<00:00,  2.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 3107/3107 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 3108/3108 [00:00<00:00,  2.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 3109/3109 [00:00<00:00,  2.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 3110/3110 [00:00<00:00,  2.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 3111/3111 [00:00<00:00,  2.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 3112/3112 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3113/3113 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3114/3114 [00:00<00:00,  2.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 3115/3115 [00:00<00:00,  2.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 3116/3116 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 3117/3117 [00:00<00:00,  2.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 3118/3118 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3119/3119 [00:00<00:00,  2.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 3120/3120 [00:00<00:00,  2.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 3121/3121 [00:00<00:00,  2.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 3122/3122 [00:00<00:00,  2.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 3123/3123 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 3124/3124 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 3125/3125 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3126/3126 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3127/3127 [00:00<00:00,  2.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 3128/3128 [00:00<00:00,  2.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 3129/3129 [00:00<00:00,  2.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 3130/3130 [00:00<00:00,  2.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 3131/3131 [00:00<00:00,  2.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 3132/3132 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3133/3133 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 3134/3134 [00:00<00:00,  2.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 3135/3135 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3136/3136 [00:00<00:00,  2.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 3137/3137 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3138/3138 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 3139/3139 [00:00<00:00,  2.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 3140/3140 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3141/3141 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3142/3142 [00:00<00:00,  2.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 3143/3143 [00:00<00:00,  2.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 3144/3144 [00:00<00:00,  2.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 3145/3145 [00:00<00:00,  2.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 3146/3146 [00:00<00:00,  2.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 3147/3147 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3148/3148 [00:00<00:00,  2.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 3149/3149 [00:00<00:00,  2.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 3150/3150 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 3151/3151 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3152/3152 [00:00<00:00,  2.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 3153/3153 [00:00<00:00,  2.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 3154/3154 [00:00<00:00,  2.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 3155/3155 [00:00<00:00,  2.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 3156/3156 [00:00<00:00,  2.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 3157/3157 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 3158/3158 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3159/3159 [00:00<00:00,  2.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 3160/3160 [00:00<00:00,  2.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 3161/3161 [00:00<00:00,  2.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 3162/3162 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 3163/3163 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 3164/3164 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 3165/3165 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 3166/3166 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 3167/3167 [00:00<00:00,  2.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 3168/3168 [00:00<00:00,  2.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 3169/3169 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 3170/3170 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 3171/3171 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 3172/3172 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 3173/3173 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 3174/3174 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 3175/3175 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 3176/3176 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 3177/3177 [00:00<00:00,  2.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 3178/3178 [00:00<00:00,  2.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 3179/3179 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 3180/3180 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 3181/3181 [00:00<00:00,  2.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 3182/3182 [00:00<00:00,  2.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 3183/3183 [00:00<00:00,  2.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 3184/3184 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 3185/3185 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 3186/3186 [00:00<00:00,  2.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 3187/3187 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3188/3188 [00:00<00:00,  2.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 3189/3189 [00:00<00:00,  2.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 3190/3190 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 3191/3191 [00:00<00:00,  2.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 3192/3192 [00:00<00:00,  2.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 3193/3193 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 3194/3194 [00:00<00:00,  2.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 3195/3195 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 3196/3196 [00:00<00:00,  2.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 3197/3197 [00:00<00:00,  2.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 3198/3198 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 3199/3199 [00:00<00:00,  2.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 3200/3200 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3201/3201 [00:00<00:00,  2.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 3202/3202 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 3203/3203 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3204/3204 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3205/3205 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3206/3206 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3207/3207 [00:00<00:00,  2.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 3208/3208 [00:00<00:00,  2.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 3209/3209 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 3210/3210 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 3211/3211 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 3212/3212 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 3213/3213 [00:00<00:00,  2.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 3214/3214 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3215/3215 [00:00<00:00,  2.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 3216/3216 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3217/3217 [00:00<00:00,  2.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 3218/3218 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3219/3219 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 3220/3220 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 3221/3221 [00:00<00:00,  2.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 3222/3222 [00:00<00:00,  2.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 3223/3223 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 3224/3224 [00:00<00:00,  2.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 3225/3225 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3226/3226 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3227/3227 [00:00<00:00,  2.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 3228/3228 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 3229/3229 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 3230/3230 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 3231/3231 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 3232/3232 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3233/3233 [00:00<00:00,  2.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 3234/3234 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3235/3235 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3236/3236 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3237/3237 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3238/3238 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3239/3239 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3240/3240 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 3241/3241 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 3242/3242 [00:00<00:00,  2.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 3243/3243 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 3244/3244 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3245/3245 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 3246/3246 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3247/3247 [00:00<00:00,  2.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 3248/3248 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 3249/3249 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 3250/3250 [00:00<00:00,  2.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 3251/3251 [00:00<00:00,  2.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 3252/3252 [00:00<00:00,  2.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 3253/3253 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3254/3254 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 3255/3255 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 3256/3256 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3257/3257 [00:00<00:00,  2.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 3258/3258 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3259/3259 [00:00<00:00,  2.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 3260/3260 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 3261/3261 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3262/3262 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3263/3263 [00:00<00:00,  2.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3264/3264 [00:00<00:00,  2.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3265/3265 [00:00<00:00,  2.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 3266/3266 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3267/3267 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3268/3268 [00:00<00:00,  2.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 3269/3269 [00:00<00:00,  2.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 3270/3270 [00:00<00:00,  2.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 3271/3271 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 3272/3272 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 3273/3273 [00:00<00:00,  2.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 3274/3274 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3275/3275 [00:00<00:00,  2.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3276/3276 [00:00<00:00,  2.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 3277/3277 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 3278/3278 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 3279/3279 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 3280/3280 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 3281/3281 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 3282/3282 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3283/3283 [00:00<00:00,  2.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 3284/3284 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3285/3285 [00:00<00:00,  2.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 3286/3286 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 3287/3287 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 3288/3288 [00:00<00:00,  2.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 3289/3289 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 3290/3290 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 3291/3291 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 3292/3292 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 3293/3293 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 3294/3294 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 3295/3295 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 3296/3296 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 3297/3297 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 3298/3298 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 3299/3299 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3300/3300 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 3301/3301 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 3302/3302 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 3303/3303 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 3304/3304 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 3305/3305 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 3306/3306 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 3307/3307 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 3308/3308 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 3309/3309 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 3310/3310 [00:00<00:00,  1.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 3311/3311 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 3312/3312 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 3313/3313 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 3314/3314 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 3315/3315 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 3316/3316 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 3317/3317 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 3318/3318 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 3319/3319 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 3320/3320 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 3321/3321 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 3322/3322 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 3323/3323 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 3324/3324 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 3325/3325 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 3326/3326 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 3327/3327 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 3328/3328 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 3329/3329 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 3330/3330 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 3331/3331 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 3332/3332 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 3333/3333 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 3334/3334 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 3335/3335 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 3336/3336 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 3337/3337 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 3338/3338 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 3339/3339 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 3340/3340 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3341/3341 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 3342/3342 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 3343/3343 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 3344/3344 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 3345/3345 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 3346/3346 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 3347/3347 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 3348/3348 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 3349/3349 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 3350/3350 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 3351/3351 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 3352/3352 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 3353/3353 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 3354/3354 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 3355/3355 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 3356/3356 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 3357/3357 [00:00<00:00,  1.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 3358/3358 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 3359/3359 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 3360/3360 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 3361/3361 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 3362/3362 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 3363/3363 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 3364/3364 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 3365/3365 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 3366/3366 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 3367/3367 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 3368/3368 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3369/3369 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3370/3370 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3371/3371 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 3372/3372 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 3373/3373 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 3374/3374 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 3375/3375 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 3376/3376 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 3377/3377 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 3378/3378 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 3379/3379 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3380/3380 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3381/3381 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 3382/3382 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3383/3383 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 3384/3384 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 3385/3385 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 3386/3386 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3387/3387 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3388/3388 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3389/3389 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3390/3390 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3391/3391 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3392/3392 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 3393/3393 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 3394/3394 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3395/3395 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3396/3396 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 3397/3397 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3398/3398 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 3399/3399 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3400/3400 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3401/3401 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 3402/3402 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3403/3403 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 3404/3404 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 3405/3405 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 3406/3406 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 3407/3407 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 3408/3408 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 3409/3409 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 3410/3410 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 3411/3411 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 3412/3412 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 3413/3413 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 3414/3414 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 3415/3415 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3416/3416 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 3417/3417 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 3418/3418 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 3419/3419 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 3420/3420 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 3421/3421 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 3422/3422 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 3423/3423 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3424/3424 [00:00<00:00,  2.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 3425/3425 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3426/3426 [00:00<00:00,  2.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 3427/3427 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 3428/3428 [00:00<00:00,  2.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 3429/3429 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3430/3430 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3431/3431 [00:00<00:00,  2.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3432/3432 [00:00<00:00,  2.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 3433/3433 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3434/3434 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 3435/3435 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 3436/3436 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3437/3437 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3438/3438 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 3439/3439 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 3440/3440 [00:00<00:00,  2.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 3441/3441 [00:00<00:00,  2.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3442/3442 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3443/3443 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 3444/3444 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3445/3445 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3446/3446 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 3447/3447 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 3448/3448 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3449/3449 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3450/3450 [00:00<00:00,  2.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 3451/3451 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3452/3452 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 3453/3453 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3454/3454 [00:00<00:00,  2.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3455/3455 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3456/3456 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3457/3457 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 3458/3458 [00:00<00:00,  2.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3459/3459 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 3460/3460 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 3461/3461 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3462/3462 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3463/3463 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 3464/3464 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3465/3465 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 3466/3466 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 3467/3467 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 3468/3468 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 3469/3469 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 3470/3470 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 3471/3471 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 3472/3472 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 3473/3473 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 3474/3474 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3475/3475 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 3476/3476 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 3477/3477 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 3478/3478 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 3479/3479 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 3480/3480 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 3481/3481 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 3482/3482 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 3483/3483 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 3484/3484 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 3485/3485 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 3486/3486 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 3487/3487 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3488/3488 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 3489/3489 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 3490/3490 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 3491/3491 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 3492/3492 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 3493/3493 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 3494/3494 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3495/3495 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3496/3496 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3497/3497 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 3498/3498 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 3499/3499 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 3500/3500 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 3501/3501 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 3502/3502 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 3503/3503 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3504/3504 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3505/3505 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 3506/3506 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 3507/3507 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 3508/3508 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 3509/3509 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3510/3510 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 3511/3511 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 3512/3512 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3513/3513 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 3514/3514 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 3515/3515 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 3516/3516 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3517/3517 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 3518/3518 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 3519/3519 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 3520/3520 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 3521/3521 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 3522/3522 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3523/3523 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 3524/3524 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 3525/3525 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 3526/3526 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3527/3527 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3528/3528 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3529/3529 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 3530/3530 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 3531/3531 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3532/3532 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 3533/3533 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 3534/3534 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3535/3535 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 3536/3536 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3537/3537 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 3538/3538 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 3539/3539 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3540/3540 [00:00<00:00,  2.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 3541/3541 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3542/3542 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3543/3543 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 3544/3544 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3545/3545 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3546/3546 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 3547/3547 [00:00<00:00,  2.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 3548/3548 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3549/3549 [00:00<00:00,  2.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 3550/3550 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 3551/3551 [00:00<00:00,  2.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3552/3552 [00:00<00:00,  2.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3553/3553 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3554/3554 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3555/3555 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3556/3556 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 3557/3557 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3558/3558 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3559/3559 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3560/3560 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3561/3561 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 3562/3562 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 3563/3563 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3564/3564 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3565/3565 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3566/3566 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 3567/3567 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3568/3568 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3569/3569 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 3570/3570 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 3571/3571 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3572/3572 [00:00<00:00,  2.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 3573/3573 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3574/3574 [00:00<00:00,  2.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3575/3575 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3576/3576 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 3577/3577 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3578/3578 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 3579/3579 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 3580/3580 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3581/3581 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3582/3582 [00:00<00:00,  2.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3583/3583 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3584/3584 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3585/3585 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 3586/3586 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 3587/3587 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3588/3588 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 3589/3589 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3590/3590 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3591/3591 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3592/3592 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3593/3593 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 3594/3594 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 3595/3595 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 3596/3596 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 3597/3597 [00:05<00:00,  5.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 3598/3598 [00:05<00:00,  5.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 3599/3599 [00:05<00:00,  5.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 3600/3600 [00:05<00:00,  5.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 3601/3601 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 3602/3602 [00:05<00:00,  5.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 3603/3603 [00:05<00:00,  5.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 3604/3604 [00:04<00:00,  4.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 3605/3605 [00:05<00:00,  5.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 3606/3606 [00:05<00:00,  5.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 3607/3607 [00:05<00:00,  5.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 3608/3608 [00:05<00:00,  5.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 3609/3609 [00:04<00:00,  4.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 3610/3610 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 3611/3611 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 3612/3612 [00:05<00:00,  5.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 3613/3613 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 3614/3614 [00:04<00:00,  4.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 3615/3615 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 3616/3616 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 3617/3617 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 3618/3618 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 3619/3619 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 3620/3620 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 3621/3621 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 3622/3622 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 3623/3623 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 3624/3624 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 3625/3625 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3626/3626 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3627/3627 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 3628/3628 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 3629/3629 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 3630/3630 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3631/3631 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 3632/3632 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 3633/3633 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 3634/3634 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 3635/3635 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 3636/3636 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3637/3637 [00:00<00:00,  2.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 3638/3638 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 3639/3639 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 3640/3640 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 3641/3641 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 3642/3642 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 3643/3643 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 3644/3644 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 3645/3645 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3646/3646 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 3647/3647 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3648/3648 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 3649/3649 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 3650/3650 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3651/3651 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3652/3652 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 3653/3653 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 3654/3654 [00:00<00:00,  2.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 3655/3655 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3656/3656 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3657/3657 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 3658/3658 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 3659/3659 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 3660/3660 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 3661/3661 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 3662/3662 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3663/3663 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3664/3664 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 3665/3665 [00:00<00:00,  2.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 3666/3666 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3667/3667 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3668/3668 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3669/3669 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 3670/3670 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3671/3671 [00:00<00:00,  2.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 3672/3672 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3673/3673 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3674/3674 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3675/3675 [00:00<00:00,  2.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 3676/3676 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 3677/3677 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3678/3678 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 3679/3679 [00:00<00:00,  2.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 3680/3680 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 3681/3681 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3682/3682 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3683/3683 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3684/3684 [00:00<00:00,  2.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 3685/3685 [00:00<00:00,  2.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 3686/3686 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 3687/3687 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3688/3688 [00:00<00:00,  2.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 3689/3689 [00:00<00:00,  2.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 3690/3690 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 3691/3691 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 3692/3692 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3693/3693 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 3694/3694 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 3695/3695 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 3696/3696 [00:00<00:00,  2.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 3697/3697 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 3698/3698 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3699/3699 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 3700/3700 [00:00<00:00,  2.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 3701/3701 [00:00<00:00,  2.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 3702/3702 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3703/3703 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 3704/3704 [00:00<00:00,  2.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 3705/3705 [00:00<00:00,  2.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 3706/3706 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 3707/3707 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3708/3708 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3709/3709 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3710/3710 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 3711/3711 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 3712/3712 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3713/3713 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 3714/3714 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 3715/3715 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3716/3716 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3717/3717 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 3718/3718 [00:00<00:00,  2.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 3719/3719 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3720/3720 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 3721/3721 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3722/3722 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3723/3723 [00:00<00:00,  2.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 3724/3724 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3725/3725 [00:00<00:00,  2.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 3726/3726 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3727/3727 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3728/3728 [00:00<00:00,  2.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 3729/3729 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3730/3730 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3731/3731 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 3732/3732 [00:00<00:00,  2.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 3733/3733 [00:00<00:00,  2.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 3734/3734 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3735/3735 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 3736/3736 [00:00<00:00,  2.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 3737/3737 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3738/3738 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3739/3739 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3740/3740 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 3741/3741 [00:00<00:00,  2.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 3742/3742 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 3743/3743 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 3744/3744 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3745/3745 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3746/3746 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3747/3747 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3748/3748 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3749/3749 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3750/3750 [00:00<00:00,  2.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 3751/3751 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 3752/3752 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 3753/3753 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3754/3754 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 3755/3755 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3756/3756 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 3757/3757 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3758/3758 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3759/3759 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 3760/3760 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3761/3761 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 3762/3762 [00:00<00:00,  2.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 3763/3763 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3764/3764 [00:00<00:00,  2.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 3765/3765 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 3766/3766 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3767/3767 [00:00<00:00,  2.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 3768/3768 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 3769/3769 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3770/3770 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3771/3771 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3772/3772 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3773/3773 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3774/3774 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 3775/3775 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3776/3776 [00:00<00:00,  2.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 3777/3777 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 3778/3778 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 3779/3779 [00:00<00:00,  2.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 3780/3780 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3781/3781 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3782/3782 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 3783/3783 [00:00<00:00,  2.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 3784/3784 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3785/3785 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 3786/3786 [00:00<00:00,  2.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 3787/3787 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 3788/3788 [00:00<00:00,  2.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 3789/3789 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3790/3790 [00:00<00:00,  2.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 3791/3791 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3792/3792 [00:00<00:00,  2.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 3793/3793 [00:00<00:00,  2.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 3794/3794 [00:00<00:00,  2.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 3795/3795 [00:00<00:00,  2.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 3796/3796 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 3797/3797 [00:00<00:00,  2.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 3798/3798 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 3799/3799 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 3800/3800 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 3801/3801 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 3802/3802 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 3803/3803 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 3804/3804 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 3805/3805 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 3806/3806 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 3807/3807 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 3808/3808 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 3809/3809 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 3810/3810 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 3811/3811 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 3812/3812 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 3813/3813 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 3814/3814 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 3815/3815 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 3816/3816 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 3817/3817 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 3818/3818 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 3819/3819 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 3820/3820 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 3821/3821 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 3822/3822 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 3823/3823 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 3824/3824 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 3825/3825 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 3826/3826 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 3827/3827 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 3828/3828 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 3829/3829 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 3830/3830 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 3831/3831 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 3832/3832 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 3833/3833 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 3834/3834 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 3835/3835 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 3836/3836 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 3837/3837 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 3838/3838 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 3839/3839 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 3840/3840 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 3841/3841 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 3842/3842 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 3843/3843 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 3844/3844 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 3845/3845 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 3846/3846 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 3847/3847 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 3848/3848 [00:00<00:00,  2.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 3849/3849 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 3850/3850 [00:00<00:00,  2.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 3851/3851 [00:00<00:00,  2.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 3852/3852 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 3853/3853 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 3854/3854 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3855/3855 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 3856/3856 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 3857/3857 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 3858/3858 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 3859/3859 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 3860/3860 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3861/3861 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 3862/3862 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 3863/3863 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 3864/3864 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 3865/3865 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 3866/3866 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 3867/3867 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 3868/3868 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 3869/3869 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 3870/3870 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 3871/3871 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 3872/3872 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 3873/3873 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 3874/3874 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 3875/3875 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3876/3876 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 3877/3877 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 3878/3878 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 3879/3879 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 3880/3880 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 3881/3881 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 3882/3882 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 3883/3883 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 3884/3884 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 3885/3885 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 3886/3886 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 3887/3887 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 3888/3888 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 3889/3889 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 3890/3890 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 3891/3891 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 3892/3892 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 3893/3893 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 3894/3894 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 3895/3895 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3896/3896 [00:00<00:00,  2.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 3897/3897 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 3898/3898 [00:00<00:00,  1.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 3899/3899 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 3900/3900 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3901/3901 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3902/3902 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3903/3903 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3904/3904 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 3905/3905 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3906/3906 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 3907/3907 [00:00<00:00,  2.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 3908/3908 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 3909/3909 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 3910/3910 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3911/3911 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3912/3912 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 3913/3913 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 3914/3914 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3915/3915 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3916/3916 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3917/3917 [00:00<00:00,  2.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 3918/3918 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 3919/3919 [00:00<00:00,  2.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 3920/3920 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3921/3921 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3922/3922 [00:00<00:00,  2.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 3923/3923 [00:00<00:00,  2.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 3924/3924 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 3925/3925 [00:00<00:00,  2.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 3926/3926 [00:00<00:00,  2.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 3927/3927 [00:00<00:00,  2.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 3928/3928 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3929/3929 [00:00<00:00,  2.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 3930/3930 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3931/3931 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 3932/3932 [00:00<00:00,  2.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 3933/3933 [00:00<00:00,  2.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 3934/3934 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 3935/3935 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3936/3936 [00:00<00:00,  2.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 3937/3937 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 3938/3938 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3939/3939 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3940/3940 [00:00<00:00,  2.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 3941/3941 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3942/3942 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3943/3943 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 3944/3944 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3945/3945 [00:00<00:00,  2.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3946/3946 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 3947/3947 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 3948/3948 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 3949/3949 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 3950/3950 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 3951/3951 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 3952/3952 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 3953/3953 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 3954/3954 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 3955/3955 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 3956/3956 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 3957/3957 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3958/3958 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 3959/3959 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 3960/3960 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 3961/3961 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3962/3962 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 3963/3963 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3964/3964 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 3965/3965 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3966/3966 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 3967/3967 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 3968/3968 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 3969/3969 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3970/3970 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 3971/3971 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 3972/3972 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 3973/3973 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 3974/3974 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 3975/3975 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 3976/3976 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 3977/3977 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 3978/3978 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 3979/3979 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 3980/3980 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 3981/3981 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 3982/3982 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 3983/3983 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 3984/3984 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 3985/3985 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 3986/3986 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 3987/3987 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 3988/3988 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 3989/3989 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 3990/3990 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 3991/3991 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 3992/3992 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 3993/3993 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 3994/3994 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 3995/3995 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 3996/3996 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 3997/3997 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 3998/3998 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 3999/3999 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 4000/4000 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 4001/4001 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 4002/4002 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 4003/4003 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 4004/4004 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 4005/4005 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 4006/4006 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 4007/4007 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 4008/4008 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 4009/4009 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 4010/4010 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 4011/4011 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 4012/4012 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 4013/4013 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 4014/4014 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 4015/4015 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 4016/4016 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 4017/4017 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 4018/4018 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 4019/4019 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 4020/4020 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 4021/4021 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 4022/4022 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 4023/4023 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 4024/4024 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 4025/4025 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 4026/4026 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 4027/4027 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 4028/4028 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 4029/4029 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 4030/4030 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 4031/4031 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 4032/4032 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 4033/4033 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 4034/4034 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 4035/4035 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 4036/4036 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 4037/4037 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 4038/4038 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 4039/4039 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 4040/4040 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 4041/4041 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 4042/4042 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 4043/4043 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 4044/4044 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 4045/4045 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 4046/4046 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 4047/4047 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 4048/4048 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 4049/4049 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 4050/4050 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 4051/4051 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 4052/4052 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 4053/4053 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 4054/4054 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 4055/4055 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 4056/4056 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 4057/4057 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 4058/4058 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 4059/4059 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 4060/4060 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 4061/4061 [00:00<00:00,  2.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 4062/4062 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 4063/4063 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 4064/4064 [00:00<00:00,  2.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 4065/4065 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 4066/4066 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 4067/4067 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 4068/4068 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 4069/4069 [00:00<00:00,  2.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 4070/4070 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 4071/4071 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 4072/4072 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 4073/4073 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 4074/4074 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 4075/4075 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 4076/4076 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 4077/4077 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 4078/4078 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 4079/4079 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 4080/4080 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 4081/4081 [00:00<00:00,  2.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 4082/4082 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 4083/4083 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 4084/4084 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 4085/4085 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 4086/4086 [00:00<00:00,  2.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 4087/4087 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 4088/4088 [00:00<00:00,  2.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 4089/4089 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 4090/4090 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 4091/4091 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 4092/4092 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 4093/4093 [00:00<00:00,  2.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 4094/4094 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 4095/4095 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 4096/4096 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 4097/4097 [00:00<00:00,  2.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 4098/4098 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 4099/4099 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 4100/4100 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 4101/4101 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 4102/4102 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 4103/4103 [00:00<00:00,  2.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 4104/4104 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 4105/4105 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 4106/4106 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 4107/4107 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 4108/4108 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 4109/4109 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 4110/4110 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 4111/4111 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 4112/4112 [00:00<00:00,  2.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 4113/4113 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 4114/4114 [00:00<00:00,  2.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 4115/4115 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 4116/4116 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 4117/4117 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 4118/4118 [00:00<00:00,  2.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 4119/4119 [00:00<00:00,  2.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 4120/4120 [00:00<00:00,  2.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 4121/4121 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 4122/4122 [00:00<00:00,  2.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 4123/4123 [00:00<00:00,  2.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 4124/4124 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 4125/4125 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4126/4126 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 4127/4127 [00:00<00:00,  2.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 4128/4128 [00:00<00:00,  2.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 4129/4129 [00:00<00:00,  2.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 4130/4130 [00:00<00:00,  2.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 4131/4131 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4132/4132 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 4133/4133 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 4134/4134 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 4135/4135 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 4136/4136 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 4137/4137 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 4138/4138 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4139/4139 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4140/4140 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4141/4141 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 4142/4142 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 4143/4143 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 4144/4144 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 4145/4145 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 4146/4146 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 4147/4147 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 4148/4148 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 4149/4149 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 4150/4150 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 4151/4151 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 4152/4152 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 4153/4153 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 4154/4154 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 4155/4155 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 4156/4156 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 4157/4157 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 4158/4158 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4159/4159 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4160/4160 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 4161/4161 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 4162/4162 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4163/4163 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 4164/4164 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 4165/4165 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 4166/4166 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 4167/4167 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4168/4168 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 4169/4169 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4170/4170 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 4171/4171 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 4172/4172 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 4173/4173 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 4174/4174 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 4175/4175 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4176/4176 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 4177/4177 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 4178/4178 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4179/4179 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 4180/4180 [00:00<00:00,  1.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 4181/4181 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 4182/4182 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 4183/4183 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 4184/4184 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 4185/4185 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 4186/4186 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4187/4187 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4188/4188 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 4189/4189 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 4190/4190 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 4191/4191 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 4192/4192 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 4193/4193 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 4194/4194 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 4195/4195 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 4196/4196 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4197/4197 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4198/4198 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4199/4199 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 4200/4200 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 4201/4201 [00:00<00:00,  1.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 4202/4202 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 4203/4203 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 4204/4204 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 4205/4205 [00:00<00:00,  1.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 4206/4206 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 4207/4207 [00:00<00:00,  1.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 4208/4208 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 4209/4209 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 4210/4210 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 4211/4211 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4212/4212 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 4213/4213 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 4214/4214 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 4215/4215 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 4216/4216 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 4217/4217 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 4218/4218 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 4219/4219 [00:00<00:00,  1.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 4220/4220 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 4221/4221 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4222/4222 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4223/4223 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 4224/4224 [00:00<00:00,  1.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 4225/4225 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4226/4226 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4227/4227 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4228/4228 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 4229/4229 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4230/4230 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 4231/4231 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 4232/4232 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 4233/4233 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 4234/4234 [00:00<00:00,  1.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 4235/4235 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 4236/4236 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 4237/4237 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 4238/4238 [00:00<00:00,  1.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 4239/4239 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 4240/4240 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 4241/4241 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4242/4242 [00:00<00:00,  1.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 4243/4243 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 4244/4244 [00:00<00:00,  1.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 4245/4245 [00:00<00:00,  1.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 4246/4246 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 4247/4247 [00:00<00:00,  1.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 4248/4248 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 4249/4249 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 4250/4250 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4251/4251 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4252/4252 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 4253/4253 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 4254/4254 [00:00<00:00,  1.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 4255/4255 [00:00<00:00,  1.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 4256/4256 [00:00<00:00,  1.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 4257/4257 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 4258/4258 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 4259/4259 [00:00<00:00,  1.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 4260/4260 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 4261/4261 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 4262/4262 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 4263/4263 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4264/4264 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 4265/4265 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 4266/4266 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 4267/4267 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4268/4268 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 4269/4269 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 4270/4270 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4271/4271 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4272/4272 [00:00<00:00,  1.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 4273/4273 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 4274/4274 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4275/4275 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4276/4276 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4277/4277 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4278/4278 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4279/4279 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 4280/4280 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 4281/4281 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 4282/4282 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 4283/4283 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 4284/4284 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4285/4285 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4286/4286 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4287/4287 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 4288/4288 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4289/4289 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4290/4290 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 4291/4291 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 4292/4292 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 4293/4293 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 4294/4294 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 4295/4295 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 4296/4296 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 4297/4297 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 4298/4298 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4299/4299 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 4300/4300 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4301/4301 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 4302/4302 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 4303/4303 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 4304/4304 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 4305/4305 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 4306/4306 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 4307/4307 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 4308/4308 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 4309/4309 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4310/4310 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 4311/4311 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 4312/4312 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 4313/4313 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 4314/4314 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4315/4315 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 4316/4316 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4317/4317 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 4318/4318 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 4319/4319 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 4320/4320 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 4321/4321 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 4322/4322 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 4323/4323 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4323/4324 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4324/4324 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4324/4325 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4325/4325 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4325/4326 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4326/4326 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4326/4327 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4327/4327 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4327/4328 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4328/4328 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4328/4329 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4329/4329 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4329/4330 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4330/4330 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4330/4331 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4331/4331 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4331/4332 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4332/4332 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4332/4333 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4333/4333 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4333/4334 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4334/4334 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4334/4335 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4335/4335 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4335/4336 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4336/4336 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4336/4337 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4337/4337 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4337/4338 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4338/4338 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4338/4339 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4339/4339 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4339/4340 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4340/4340 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4340/4341 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4341/4341 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4341/4342 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4342/4342 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4342/4343 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4343/4343 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4343/4344 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4344/4344 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4344/4345 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4345/4345 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4345/4346 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4346/4346 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4346/4347 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4347/4347 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4347/4348 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4348/4348 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4348/4349 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4349/4349 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4349/4350 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4350/4350 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4350/4351 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4351/4351 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4351/4352 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4352/4352 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4352/4353 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4353/4353 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4353/4354 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4354/4354 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4354/4355 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4355/4355 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4355/4356 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4356/4356 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4356/4357 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4357/4357 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4357/4358 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4358/4358 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 4359/4359 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4359/4360 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4360/4360 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4360/4361 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4361/4361 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 4362/4362 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4362/4363 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4363/4363 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4363/4364 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4364/4364 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 4365/4365 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 4366/4366 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4366/4367 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4367/4367 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4367/4368 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4368/4368 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4368/4369 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4369/4369 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4369/4370 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4370/4370 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 4371/4371 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 4372/4372 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4372/4373 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4373/4373 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4373/4374 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4374/4374 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4374/4375 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4375/4375 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4375/4376 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4376/4376 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 4377/4377 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4377/4378 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4378/4378 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4378/4379 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4379/4379 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 4380/4380 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 4381/4381 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 4382/4382 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 4383/4383 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 4384/4384 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 4385/4385 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 4386/4386 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 4387/4387 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 4388/4388 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 4389/4389 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 4390/4390 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 4391/4391 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 4392/4392 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 4393/4393 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 4394/4394 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 4395/4395 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 4396/4396 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 4397/4397 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 4398/4398 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4399/4399 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 4400/4400 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4401/4401 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 4402/4402 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 4403/4403 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 4404/4404 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 4405/4405 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4406/4406 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4407/4407 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4408/4408 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 4409/4409 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 4410/4410 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 4411/4411 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 4412/4412 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4413/4413 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4414/4414 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 4415/4415 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 4416/4416 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4417/4417 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 4418/4418 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 4419/4419 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 4420/4420 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 4421/4421 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 4422/4422 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 4423/4423 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 4424/4424 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 4425/4425 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 4426/4426 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 4427/4427 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 4428/4428 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 4429/4429 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 4430/4430 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 4431/4431 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 4432/4432 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4433/4433 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 4434/4434 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 4435/4435 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 4436/4436 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 4437/4437 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 4438/4438 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4438/4439 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4439/4439 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 4440/4440 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4440/4441 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4441/4441 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4441/4442 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4442/4442 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4442/4443 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4443/4443 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4443/4444 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4444/4444 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4444/4445 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4445/4445 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4445/4446 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4446/4446 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4446/4447 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4447/4447 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4447/4448 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4448/4448 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4448/4449 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4449/4449 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4449/4450 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4450/4450 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4450/4451 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4451/4451 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4451/4452 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4452/4452 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4452/4453 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4453/4453 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4453/4454 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4454/4454 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4454/4455 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4455/4455 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4455/4456 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4456/4456 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4456/4457 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4457/4457 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 4458/4458 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4458/4459 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4459/4459 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4459/4460 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4460/4460 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4460/4461 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4461/4461 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4461/4462 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4462/4462 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4462/4463 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4463/4463 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4463/4464 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4464/4464 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4464/4465 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4465/4465 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4465/4466 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4466/4466 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4466/4467 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4467/4467 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4467/4468 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4468/4468 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 4469/4469 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4469/4470 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4470/4470 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4470/4471 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4471/4471 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4471/4472 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4472/4472 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 4473/4473 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4473/4474 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4474/4474 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4474/4475 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4475/4475 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4475/4476 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4476/4476 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4476/4477 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4477/4477 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 4478/4478 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4478/4479 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4479/4479 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 4480/4480 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4480/4481 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4481/4481 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4481/4482 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4482/4482 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4482/4483 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4483/4483 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4483/4484 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4484/4484 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4484/4485 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4485/4485 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4485/4486 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 4487/4487 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4487/4488 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4488/4488 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4488/4489 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4489/4489 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4489/4490 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4490/4490 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 4491/4491 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4491/4492 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4492/4492 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4492/4493 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4493/4493 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4493/4494 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4494/4494 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 4495/4495 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4495/4496 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4496/4496 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4496/4497 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4497/4497 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 4498/4498 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 4499/4499 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 4500/4500 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4500/4501 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4501/4501 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4501/4502 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4502/4502 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 4503/4503 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 4504/4504 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 4504/4505 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4505/4505 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4505/4506 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4506/4506 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4506/4507 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4507/4507 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4507/4508 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4508/4508 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4508/4509 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4509/4509 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4509/4510 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4510/4510 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4510/4511 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4511/4511 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4511/4512 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4512/4512 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4512/4513 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4513/4513 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4513/4514 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4514/4514 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4514/4515 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4515/4515 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 4515/4516 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4516/4516 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 4517/4517 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 4518/4518 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 4519/4519 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 4520/4520 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 4521/4521 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 4522/4522 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 4523/4523 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 4524/4524 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 4525/4525 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 4526/4526 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 4527/4527 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 4528/4528 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 4529/4529 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 4530/4530 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 4531/4531 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 4532/4532 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 4533/4533 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 4534/4534 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 4535/4535 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 4536/4536 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 4537/4537 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 4538/4538 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 4539/4539 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 4540/4540 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 4541/4541 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 4542/4542 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 4543/4543 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 4544/4544 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 4545/4545 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 4546/4546 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 4547/4547 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 4548/4548 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 4549/4549 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 4550/4550 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 4551/4551 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 4552/4552 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 4553/4553 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 4554/4554 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 4555/4555 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 4556/4556 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 4557/4557 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 4558/4558 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 4559/4559 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 4560/4560 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 4561/4561 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 4562/4562 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 4563/4563 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 4564/4564 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 4565/4565 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 4566/4566 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 4567/4567 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 4568/4568 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 4569/4569 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 4570/4570 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 4571/4571 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 4572/4572 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 4573/4573 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 4574/4574 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 4575/4575 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 4576/4576 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 4577/4577 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 4578/4578 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 4579/4579 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 4580/4580 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 4581/4581 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 4582/4582 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 4583/4583 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 4584/4584 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 4585/4585 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 4586/4586 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 4587/4587 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 4588/4588 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 4589/4589 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 4590/4590 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 4591/4591 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 4592/4592 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 4593/4593 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 4594/4594 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 4595/4595 [00:04<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 4596/4596 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 4597/4597 [00:04<00:00,  4.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 4598/4598 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 4599/4599 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 4600/4600 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 4601/4601 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 4602/4602 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 4603/4603 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 4604/4604 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 4605/4605 [00:04<00:00,  4.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 4606/4606 [00:04<00:00,  4.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 4607/4607 [00:04<00:00,  4.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 4608/4608 [00:03<00:00,  3.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 4609/4609 [00:05<00:00,  5.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 4610/4610 [00:04<00:00,  4.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 4611/4611 [00:05<00:00,  5.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 4612/4612 [00:05<00:00,  5.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 4613/4613 [00:04<00:00,  4.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 4614/4614 [00:06<00:00,  6.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 4615/4615 [00:06<00:00,  6.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 4616/4616 [00:05<00:00,  5.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 4617/4617 [00:05<00:00,  5.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 4618/4618 [00:04<00:00,  4.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 4619/4619 [00:06<00:00,  6.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 4620/4620 [00:05<00:00,  5.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 4621/4621 [00:06<00:00,  6.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4622/4622 [00:05<00:00,  5.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 4623/4623 [00:05<00:00,  5.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4624/4624 [00:06<00:00,  6.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 4625/4625 [00:05<00:00,  5.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 4626/4626 [00:05<00:00,  5.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 4627/4627 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 4628/4628 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 4629/4629 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 4630/4630 [00:00<00:00,  2.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 4631/4631 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4632/4632 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4633/4633 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4634/4634 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 4635/4635 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 4636/4636 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 4637/4637 [00:00<00:00,  2.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 4638/4638 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 4639/4639 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4640/4640 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4641/4641 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 4642/4642 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4643/4643 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4644/4644 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4645/4645 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4646/4646 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4647/4647 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4648/4648 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4649/4649 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4650/4650 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 4651/4651 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 4652/4652 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 4653/4653 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4654/4654 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4655/4655 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 4656/4656 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 4657/4657 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4658/4658 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 4659/4659 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4660/4660 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4661/4661 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4662/4662 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4663/4663 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 4664/4664 [00:00<00:00,  2.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 4665/4665 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4666/4666 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 4667/4667 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4668/4668 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 4669/4669 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 4670/4670 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 4671/4671 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4672/4672 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 4673/4673 [00:00<00:00,  2.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 4674/4674 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4675/4675 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4676/4676 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 4677/4677 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4678/4678 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 4679/4679 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4680/4680 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4681/4681 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4682/4682 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 4683/4683 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 4684/4684 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 4685/4685 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4686/4686 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4687/4687 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4688/4688 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4689/4689 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4690/4690 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4691/4691 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 4692/4692 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4693/4693 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4694/4694 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4695/4695 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4696/4696 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4697/4697 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 4698/4698 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 4699/4699 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 4700/4700 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 4701/4701 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4702/4702 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4703/4703 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4704/4704 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4705/4705 [00:00<00:00,  1.98trial/s, best loss: 1.0]\n",
      "100%|██████████| 4706/4706 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 4707/4707 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 4708/4708 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4709/4709 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 4710/4710 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 4711/4711 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4712/4712 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4713/4713 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4714/4714 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4715/4715 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4716/4716 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4717/4717 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4718/4718 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4719/4719 [00:00<00:00,  2.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 4720/4720 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4721/4721 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4722/4722 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4723/4723 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4724/4724 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4725/4725 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4726/4726 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4727/4727 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4728/4728 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 4729/4729 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 4730/4730 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4731/4731 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4732/4732 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4733/4733 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4734/4734 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4735/4735 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4736/4736 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4737/4737 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 4738/4738 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4739/4739 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4740/4740 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4741/4741 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 4742/4742 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4743/4743 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4744/4744 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4745/4745 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 4746/4746 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4747/4747 [00:00<00:00,  1.99trial/s, best loss: 1.0]\n",
      "100%|██████████| 4748/4748 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4749/4749 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4750/4750 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4751/4751 [00:00<00:00,  1.97trial/s, best loss: 1.0]\n",
      "100%|██████████| 4752/4752 [00:00<00:00,  1.96trial/s, best loss: 1.0]\n",
      "100%|██████████| 4753/4753 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4754/4754 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 4755/4755 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4756/4756 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4757/4757 [00:00<00:00,  1.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 4758/4758 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 4759/4759 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 4760/4760 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4761/4761 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4762/4762 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 4763/4763 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4764/4764 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 4765/4765 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 4766/4766 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4767/4767 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 4768/4768 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 4769/4769 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4770/4770 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 4771/4771 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4772/4772 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 4773/4773 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 4774/4774 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 4775/4775 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4776/4776 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 4777/4777 [00:00<00:00,  1.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 4778/4778 [00:00<00:00,  1.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 4779/4779 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4780/4780 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 4781/4781 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4782/4782 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4783/4783 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 4784/4784 [00:00<00:00,  1.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 4785/4785 [00:00<00:00,  1.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 4786/4786 [00:00<00:00,  1.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 4787/4787 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 4788/4788 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4789/4789 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 4790/4790 [00:00<00:00,  1.78trial/s, best loss: 1.0]\n",
      "100%|██████████| 4791/4791 [00:00<00:00,  1.83trial/s, best loss: 1.0]\n",
      "100%|██████████| 4792/4792 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 4793/4793 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4794/4794 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4795/4795 [00:00<00:00,  1.95trial/s, best loss: 1.0]\n",
      "100%|██████████| 4796/4796 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 4797/4797 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 4798/4798 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4799/4799 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4800/4800 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 4801/4801 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4802/4802 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4803/4803 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 4804/4804 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4805/4805 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4806/4806 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4807/4807 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4808/4808 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4809/4809 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4810/4810 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 4811/4811 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4812/4812 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4813/4813 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4814/4814 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4815/4815 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4816/4816 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 4817/4817 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 4818/4818 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 4819/4819 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 4820/4820 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4821/4821 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4822/4822 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4823/4823 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4824/4824 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4825/4825 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4826/4826 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4827/4827 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 4828/4828 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 4829/4829 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 4830/4830 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4831/4831 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4832/4832 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4833/4833 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4834/4834 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4835/4835 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4836/4836 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4837/4837 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4838/4838 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 4839/4839 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4840/4840 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4841/4841 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4842/4842 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 4843/4843 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 4844/4844 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4845/4845 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4846/4846 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4847/4847 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 4848/4848 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4849/4849 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4850/4850 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 4851/4851 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4852/4852 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4853/4853 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4854/4854 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4855/4855 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4856/4856 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 4857/4857 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4858/4858 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4859/4859 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 4860/4860 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4861/4861 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4862/4862 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4863/4863 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4864/4864 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4865/4865 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 4866/4866 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4867/4867 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 4868/4868 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 4869/4869 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4870/4870 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4871/4871 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4872/4872 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 4873/4873 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4874/4874 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 4875/4875 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 4876/4876 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4877/4877 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4878/4878 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 4879/4879 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4880/4880 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4881/4881 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 4882/4882 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 4883/4883 [00:00<00:00,  1.77trial/s, best loss: 1.0]\n",
      "100%|██████████| 4884/4884 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4885/4885 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4886/4886 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4887/4887 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4888/4888 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 4889/4889 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4890/4890 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4891/4891 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 4892/4892 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 4893/4893 [00:00<00:00,  1.84trial/s, best loss: 1.0]\n",
      "100%|██████████| 4894/4894 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4895/4895 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4896/4896 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4897/4897 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 4898/4898 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4899/4899 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 4900/4900 [00:00<00:00,  1.86trial/s, best loss: 1.0]\n",
      "100%|██████████| 4901/4901 [00:00<00:00,  1.92trial/s, best loss: 1.0]\n",
      "100%|██████████| 4902/4902 [00:00<00:00,  1.90trial/s, best loss: 1.0]\n",
      "100%|██████████| 4903/4903 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 4904/4904 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4905/4905 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4906/4906 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 4907/4907 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4908/4908 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4909/4909 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4910/4910 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4911/4911 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 4912/4912 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4913/4913 [00:00<00:00,  1.93trial/s, best loss: 1.0]\n",
      "100%|██████████| 4914/4914 [00:00<00:00,  1.89trial/s, best loss: 1.0]\n",
      "100%|██████████| 4915/4915 [00:00<00:00,  1.88trial/s, best loss: 1.0]\n",
      "100%|██████████| 4916/4916 [00:00<00:00,  1.87trial/s, best loss: 1.0]\n",
      "100%|██████████| 4917/4917 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4918/4918 [00:00<00:00,  1.91trial/s, best loss: 1.0]\n",
      "100%|██████████| 4919/4919 [00:00<00:00,  1.85trial/s, best loss: 1.0]\n",
      "100%|██████████| 4920/4920 [00:00<00:00,  1.94trial/s, best loss: 1.0]\n",
      "100%|██████████| 4921/4921 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 4922/4922 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 4923/4923 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 4924/4924 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 4925/4925 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 4926/4926 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 4927/4927 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 4928/4928 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 4929/4929 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 4930/4930 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 4931/4931 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 4932/4932 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 4933/4933 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 4934/4934 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 4935/4935 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 4936/4936 [00:00<00:00,  1.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 4937/4937 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 4938/4938 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 4939/4939 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 4940/4940 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 4941/4941 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 4942/4942 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 4943/4943 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 4944/4944 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 4945/4945 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 4946/4946 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 4947/4947 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 4948/4948 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 4949/4949 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 4950/4950 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 4951/4951 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 4952/4952 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 4953/4953 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 4954/4954 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 4955/4955 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 4956/4956 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 4957/4957 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 4958/4958 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 4959/4959 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 4960/4960 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 4961/4961 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 4962/4962 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 4963/4963 [00:00<00:00,  1.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 4964/4964 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 4965/4965 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 4966/4966 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 4967/4967 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 4968/4968 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 4969/4969 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 4970/4970 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 4971/4971 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 4972/4972 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 4973/4973 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 4974/4974 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 4975/4975 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 4976/4976 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 4977/4977 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 4978/4978 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 4979/4979 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 4980/4980 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 4981/4981 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 4982/4982 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 4983/4983 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 4984/4984 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 4985/4985 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 4986/4986 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 4987/4987 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 4988/4988 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 4989/4989 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 4990/4990 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 4991/4991 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 4992/4992 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 4993/4993 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 4994/4994 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 4995/4995 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 4996/4996 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 4997/4997 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 4998/4998 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 4999/4999 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 5000/5000 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 5001/5001 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 5002/5002 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 5003/5003 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 5004/5004 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 5005/5005 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 5006/5006 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 5007/5007 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 5008/5008 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 5009/5009 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 5010/5010 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 5011/5011 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 5012/5012 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 5013/5013 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 5014/5014 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 5015/5015 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 5016/5016 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 5017/5017 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 5018/5018 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 5019/5019 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 5020/5020 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 5021/5021 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 5022/5022 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 5023/5023 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 5024/5024 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 5025/5025 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 5026/5026 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 5027/5027 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 5028/5028 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 5029/5029 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 5030/5030 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 5031/5031 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 5032/5032 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 5033/5033 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 5034/5034 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 5035/5035 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 5036/5036 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 5037/5037 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 5038/5038 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 5039/5039 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 5040/5040 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 5041/5041 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 5042/5042 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 5043/5043 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 5044/5044 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 5045/5045 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 5046/5046 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 5047/5047 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 5048/5048 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 5049/5049 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 5050/5050 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 5051/5051 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 5052/5052 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 5053/5053 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 5054/5054 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 5055/5055 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 5056/5056 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 5057/5057 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 5058/5058 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 5059/5059 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 5060/5060 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 5061/5061 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 5062/5062 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 5063/5063 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 5064/5064 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 5065/5065 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 5066/5066 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 5067/5067 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 5068/5068 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 5069/5069 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 5070/5070 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 5071/5071 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 5072/5072 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 5073/5073 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 5074/5074 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 5075/5075 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 5076/5076 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 5077/5077 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 5078/5078 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 5079/5079 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 5080/5080 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 5081/5081 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 5082/5082 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 5083/5083 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 5084/5084 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 5085/5085 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 5086/5086 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 5087/5087 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 5088/5088 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 5089/5089 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 5090/5090 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 5091/5091 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 5092/5092 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 5093/5093 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 5094/5094 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 5095/5095 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 5096/5096 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 5097/5097 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 5098/5098 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 5099/5099 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 5100/5100 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 5101/5101 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 5102/5102 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 5103/5103 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 5104/5104 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 5105/5105 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 5106/5106 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 5107/5107 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 5108/5108 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 5109/5109 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 5110/5110 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 5111/5111 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 5112/5112 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 5113/5113 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 5114/5114 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 5115/5115 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 5116/5116 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 5117/5117 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 5118/5118 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 5119/5119 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 5120/5120 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 5121/5121 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 5122/5122 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 5123/5123 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 5124/5124 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 5125/5125 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 5126/5126 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 5127/5127 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 5128/5128 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 5129/5129 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 5130/5130 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 5131/5131 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 5132/5132 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 5133/5133 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 5134/5134 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 5135/5135 [00:00<00:00,  1.80trial/s, best loss: 1.0]\n",
      "100%|██████████| 5136/5136 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 5137/5137 [00:00<00:00,  1.79trial/s, best loss: 1.0]\n",
      "100%|██████████| 5138/5138 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 5139/5139 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 5140/5140 [00:00<00:00,  1.82trial/s, best loss: 1.0]\n",
      "100%|██████████| 5141/5141 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 5142/5142 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 5143/5143 [00:00<00:00,  1.76trial/s, best loss: 1.0]\n",
      "100%|██████████| 5144/5144 [00:00<00:00,  1.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 5145/5145 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 5146/5146 [00:00<00:00,  1.81trial/s, best loss: 1.0]\n",
      "100%|██████████| 5147/5147 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5148/5148 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5149/5149 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5150/5150 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5151/5151 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5152/5152 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 5153/5153 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5154/5154 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5155/5155 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5156/5156 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5157/5157 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5158/5158 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5159/5159 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5160/5160 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5161/5161 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 5162/5162 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5163/5163 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5164/5164 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5165/5165 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5166/5166 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5167/5167 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5168/5168 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5169/5169 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5170/5170 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 5171/5171 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5172/5172 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5173/5173 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5174/5174 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5175/5175 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5176/5176 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5177/5177 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5178/5178 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5179/5179 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5180/5180 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5181/5181 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5182/5182 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5183/5183 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5184/5184 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5185/5185 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5186/5186 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5187/5187 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5188/5188 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5189/5189 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5190/5190 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5191/5191 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5192/5192 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5193/5193 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5194/5194 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5195/5195 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5196/5196 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5197/5197 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5198/5198 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 5199/5199 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5200/5200 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5201/5201 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5202/5202 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5203/5203 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5204/5204 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5205/5205 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5206/5206 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5207/5207 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 5208/5208 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5209/5209 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5210/5210 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5211/5211 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5212/5212 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5213/5213 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5214/5214 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5215/5215 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5216/5216 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 5217/5217 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5218/5218 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5219/5219 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5220/5220 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5221/5221 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5222/5222 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5223/5223 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 5224/5224 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5225/5225 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5226/5226 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5227/5227 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5228/5228 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5229/5229 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5230/5230 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5231/5231 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5232/5232 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5233/5233 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5234/5234 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 5235/5235 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5236/5236 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5237/5237 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5238/5238 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5239/5239 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5240/5240 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5241/5241 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5242/5242 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5243/5243 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5244/5244 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5245/5245 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5246/5246 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5247/5247 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5248/5248 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5249/5249 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5250/5250 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5251/5251 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5252/5252 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5253/5253 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5254/5254 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5255/5255 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5256/5256 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5257/5257 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5258/5258 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5259/5259 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5260/5260 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5261/5261 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5262/5262 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5263/5263 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5264/5264 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5265/5265 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5266/5266 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5267/5267 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5268/5268 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5269/5269 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5270/5270 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5271/5271 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 5272/5272 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5273/5273 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5274/5274 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5275/5275 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5276/5276 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5277/5277 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5278/5278 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5279/5279 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5280/5280 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 5281/5281 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5282/5282 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5283/5283 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5284/5284 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5285/5285 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5286/5286 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5287/5287 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5288/5288 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5289/5289 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 5290/5290 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5291/5291 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5292/5292 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5293/5293 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5294/5294 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5295/5295 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5296/5296 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5297/5297 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5298/5298 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5299/5299 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 5300/5300 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5301/5301 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5302/5302 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5303/5303 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5304/5304 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5305/5305 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5306/5306 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5307/5307 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5308/5308 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 5309/5309 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5310/5310 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5311/5311 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5312/5312 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5313/5313 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5314/5314 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5315/5315 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5316/5316 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5317/5317 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 5318/5318 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5319/5319 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5320/5320 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5321/5321 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5322/5322 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5323/5323 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5324/5324 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 5325/5325 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 5326/5326 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 5327/5327 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5328/5328 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5329/5329 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5330/5330 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5331/5331 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5332/5332 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5333/5333 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5334/5334 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5335/5335 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5336/5336 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5337/5337 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5338/5338 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5339/5339 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5340/5340 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5341/5341 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 5342/5342 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5343/5343 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5344/5344 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5345/5345 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 5346/5346 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5347/5347 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5348/5348 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5349/5349 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5350/5350 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 5351/5351 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5352/5352 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5353/5353 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5354/5354 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 5355/5355 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5356/5356 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5357/5357 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5358/5358 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5359/5359 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5360/5360 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5361/5361 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 5362/5362 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 5363/5363 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 5364/5364 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5365/5365 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5366/5366 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5367/5367 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5368/5368 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5369/5369 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 5370/5370 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5371/5371 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5372/5372 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5373/5373 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5374/5374 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5375/5375 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 5376/5376 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5377/5377 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5378/5378 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5379/5379 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5380/5380 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5381/5381 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5382/5382 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 5383/5383 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5384/5384 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5385/5385 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5386/5386 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5387/5387 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5388/5388 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 5389/5389 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5390/5390 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 5391/5391 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5392/5392 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5393/5393 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5394/5394 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 5395/5395 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5396/5396 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 5397/5397 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5398/5398 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5399/5399 [00:05<00:00,  5.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 5400/5400 [00:06<00:00,  6.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 5401/5401 [00:05<00:00,  5.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 5402/5402 [00:05<00:00,  5.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 5403/5403 [00:05<00:00,  5.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 5404/5404 [00:06<00:00,  6.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 5405/5405 [00:06<00:00,  6.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 5406/5406 [00:05<00:00,  5.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 5407/5407 [00:05<00:00,  5.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 5408/5408 [00:05<00:00,  5.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 5409/5409 [00:07<00:00,  7.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 5410/5410 [00:05<00:00,  5.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 5411/5411 [00:05<00:00,  5.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 5412/5412 [00:05<00:00,  5.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 5413/5413 [00:05<00:00,  5.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 5414/5414 [00:05<00:00,  5.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 5415/5415 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 5416/5416 [00:04<00:00,  4.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 5417/5417 [00:04<00:00,  4.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 5418/5418 [00:05<00:00,  5.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 5419/5419 [00:05<00:00,  5.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 5420/5420 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 5421/5421 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 5422/5422 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 5423/5423 [00:04<00:00,  4.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 5424/5424 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 5425/5425 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 5426/5426 [00:04<00:00,  4.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 5427/5427 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 5428/5428 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 5429/5429 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 5430/5430 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 5431/5431 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 5432/5432 [00:03<00:00,  3.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 5433/5433 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 5434/5434 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 5435/5435 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 5436/5436 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 5437/5437 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 5438/5438 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 5439/5439 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 5440/5440 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5441/5441 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5442/5442 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5443/5443 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5444/5444 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5445/5445 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5446/5446 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5447/5447 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5448/5448 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5449/5449 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5450/5450 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5451/5451 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5452/5452 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5453/5453 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5454/5454 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5455/5455 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5456/5456 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5457/5457 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5458/5458 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5459/5459 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5460/5460 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5461/5461 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5462/5462 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5463/5463 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5464/5464 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5465/5465 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5466/5466 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5467/5467 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5468/5468 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5469/5469 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5470/5470 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5471/5471 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5472/5472 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5473/5473 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5474/5474 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5475/5475 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 5476/5476 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5477/5477 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5478/5478 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5479/5479 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5480/5480 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5481/5481 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5482/5482 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5483/5483 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5484/5484 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5485/5485 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 5486/5486 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5487/5487 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5488/5488 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5489/5489 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5490/5490 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5491/5491 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5492/5492 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5493/5493 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5494/5494 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5495/5495 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5496/5496 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5497/5497 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5498/5498 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5499/5499 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5500/5500 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5501/5501 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5502/5502 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5503/5503 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5504/5504 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5505/5505 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5506/5506 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5507/5507 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5508/5508 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5509/5509 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5510/5510 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5511/5511 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5512/5512 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5513/5513 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 5514/5514 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5515/5515 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5516/5516 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5517/5517 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5518/5518 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5519/5519 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5520/5520 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5521/5521 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5522/5522 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5523/5523 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 5524/5524 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5525/5525 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5526/5526 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5527/5527 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5528/5528 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5529/5529 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5530/5530 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5531/5531 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5532/5532 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5533/5533 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5534/5534 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5535/5535 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5536/5536 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5537/5537 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5538/5538 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5539/5539 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5540/5540 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5541/5541 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5542/5542 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 5543/5543 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5544/5544 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5545/5545 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5546/5546 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5547/5547 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5548/5548 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5549/5549 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5550/5550 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5551/5551 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 5552/5552 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5553/5553 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5554/5554 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 5555/5555 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 5556/5556 [00:00<00:00,  1.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 5557/5557 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 5558/5558 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 5559/5559 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 5560/5560 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 5561/5561 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 5562/5562 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5563/5563 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 5564/5564 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 5565/5565 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5566/5566 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5567/5567 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5568/5568 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5569/5569 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5570/5570 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 5571/5571 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5572/5572 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5573/5573 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 5574/5574 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5575/5575 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5576/5576 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 5577/5577 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5578/5578 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 5579/5579 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 5580/5580 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5581/5581 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5582/5582 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5583/5583 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5584/5584 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5585/5585 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5586/5586 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5587/5587 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5588/5588 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 5589/5589 [00:00<00:00,  1.75trial/s, best loss: 1.0]\n",
      "100%|██████████| 5590/5590 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 5591/5591 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5592/5592 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 5593/5593 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5594/5594 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 5595/5595 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5596/5596 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5597/5597 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5598/5598 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 5599/5599 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 5600/5600 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5601/5601 [00:00<00:00,  1.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 5602/5602 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5603/5603 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5604/5604 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5605/5605 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5606/5606 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5607/5607 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 5608/5608 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 5609/5609 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5610/5610 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5611/5611 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5612/5612 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5613/5613 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5614/5614 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5615/5615 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 5616/5616 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5617/5617 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 5618/5618 [00:00<00:00,  1.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 5619/5619 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 5620/5620 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 5621/5621 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5622/5622 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5623/5623 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5624/5624 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5625/5625 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5626/5626 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 5627/5627 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 5628/5628 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5629/5629 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 5630/5630 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5631/5631 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 5632/5632 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5633/5633 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5634/5634 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5635/5635 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 5636/5636 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5637/5637 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5638/5638 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5639/5639 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5640/5640 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5641/5641 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 5642/5642 [00:00<00:00,  1.74trial/s, best loss: 1.0]\n",
      "100%|██████████| 5643/5643 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 5644/5644 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5645/5645 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 5646/5646 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5647/5647 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5648/5648 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5649/5649 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5650/5650 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5651/5651 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5652/5652 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5653/5653 [00:00<00:00,  1.72trial/s, best loss: 1.0]\n",
      "100%|██████████| 5654/5654 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 5655/5655 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5656/5656 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5657/5657 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5658/5658 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5659/5659 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5660/5660 [00:00<00:00,  1.71trial/s, best loss: 1.0]\n",
      "100%|██████████| 5661/5661 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5662/5662 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5663/5663 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 5664/5664 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5665/5665 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5666/5666 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 5667/5667 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5668/5668 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5669/5669 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5670/5670 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5671/5671 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5672/5672 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5673/5673 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 5674/5674 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5675/5675 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5676/5676 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5677/5677 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5678/5678 [00:00<00:00,  1.64trial/s, best loss: 1.0]\n",
      "100%|██████████| 5679/5679 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5680/5680 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5681/5681 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5682/5682 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 5683/5683 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5684/5684 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5685/5685 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5686/5686 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5687/5687 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5688/5688 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5689/5689 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5690/5690 [00:00<00:00,  1.70trial/s, best loss: 1.0]\n",
      "100%|██████████| 5691/5691 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5692/5692 [00:00<00:00,  1.73trial/s, best loss: 1.0]\n",
      "100%|██████████| 5693/5693 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 5694/5694 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5695/5695 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5696/5696 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5697/5697 [00:00<00:00,  1.67trial/s, best loss: 1.0]\n",
      "100%|██████████| 5698/5698 [00:00<00:00,  1.68trial/s, best loss: 1.0]\n",
      "100%|██████████| 5699/5699 [00:00<00:00,  1.66trial/s, best loss: 1.0]\n",
      "100%|██████████| 5700/5700 [00:00<00:00,  1.69trial/s, best loss: 1.0]\n",
      "100%|██████████| 5701/5701 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 5702/5702 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 5703/5703 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 5704/5704 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 5705/5705 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 5706/5706 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 5707/5707 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 5708/5708 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 5709/5709 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 5710/5710 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 5711/5711 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 5712/5712 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 5713/5713 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 5714/5714 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 5715/5715 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 5716/5716 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 5717/5717 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 5718/5718 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 5719/5719 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 5720/5720 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 5721/5721 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 5722/5722 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 5723/5723 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 5724/5724 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 5725/5725 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 5726/5726 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 5727/5727 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 5728/5728 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 5729/5729 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 5730/5730 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 5731/5731 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 5732/5732 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 5733/5733 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 5734/5734 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 5735/5735 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 5736/5736 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 5737/5737 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 5738/5738 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 5739/5739 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 5740/5740 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 5741/5741 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 5742/5742 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 5743/5743 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 5744/5744 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 5745/5745 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 5746/5746 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 5747/5747 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 5748/5748 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 5749/5749 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 5750/5750 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 5751/5751 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 5752/5752 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 5753/5753 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 5754/5754 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 5755/5755 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 5756/5756 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 5757/5757 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 5758/5758 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 5759/5759 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 5760/5760 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 5761/5761 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 5762/5762 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 5763/5763 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 5764/5764 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 5765/5765 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 5766/5766 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 5767/5767 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 5768/5768 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 5769/5769 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 5770/5770 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 5771/5771 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 5772/5772 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 5773/5773 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 5774/5774 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 5775/5775 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 5776/5776 [00:00<00:00,  1.65trial/s, best loss: 1.0]\n",
      "100%|██████████| 5777/5777 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 5778/5778 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5779/5779 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5780/5780 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5781/5781 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 5782/5782 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 5783/5783 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 5784/5784 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 5785/5785 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 5786/5786 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 5787/5787 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5788/5788 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5789/5789 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5790/5790 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 5791/5791 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5792/5792 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 5793/5793 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5794/5794 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 5795/5795 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5796/5796 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5797/5797 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 5798/5798 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5799/5799 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 5800/5800 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 5801/5801 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5802/5802 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 5803/5803 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 5804/5804 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 5805/5805 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5806/5806 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 5807/5807 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5808/5808 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 5809/5809 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 5810/5810 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 5811/5811 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 5812/5812 [00:00<00:00,  1.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 5813/5813 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5814/5814 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 5815/5815 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 5816/5816 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 5817/5817 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 5818/5818 [00:00<00:00,  1.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 5819/5819 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 5820/5820 [00:00<00:00,  1.46trial/s, best loss: 1.0]\n",
      "100%|██████████| 5821/5821 [00:00<00:00,  1.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 5822/5822 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5823/5823 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 5824/5824 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 5825/5825 [00:00<00:00,  1.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 5826/5826 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 5827/5827 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5828/5828 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5829/5829 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5830/5830 [00:00<00:00,  1.49trial/s, best loss: 1.0]\n",
      "100%|██████████| 5831/5831 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5832/5832 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5833/5833 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5834/5834 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 5835/5835 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5836/5836 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5837/5837 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5838/5838 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5839/5839 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5840/5840 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5841/5841 [00:00<00:00,  1.47trial/s, best loss: 1.0]\n",
      "100%|██████████| 5842/5842 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5843/5843 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5844/5844 [00:00<00:00,  1.48trial/s, best loss: 1.0]\n",
      "100%|██████████| 5845/5845 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5846/5846 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5847/5847 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5848/5848 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5849/5849 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5850/5850 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5851/5851 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5852/5852 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5853/5853 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 5854/5854 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5855/5855 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5856/5856 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5857/5857 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5858/5858 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5859/5859 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5860/5860 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5861/5861 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5862/5862 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5863/5863 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 5864/5864 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5865/5865 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5866/5866 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5867/5867 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5868/5868 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5869/5869 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5870/5870 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5871/5871 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5872/5872 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5873/5873 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5874/5874 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5875/5875 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5876/5876 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5877/5877 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5878/5878 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5879/5879 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 5880/5880 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5881/5881 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5882/5882 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 5883/5883 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5884/5884 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5885/5885 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5886/5886 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5887/5887 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5888/5888 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5889/5889 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5890/5890 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5891/5891 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5892/5892 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5893/5893 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5894/5894 [00:00<00:00,  1.51trial/s, best loss: 1.0]\n",
      "100%|██████████| 5895/5895 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5896/5896 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5897/5897 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5898/5898 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5899/5899 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5900/5900 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5901/5901 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 5902/5902 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5903/5903 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5904/5904 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5905/5905 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5906/5906 [00:00<00:00,  1.50trial/s, best loss: 1.0]\n",
      "100%|██████████| 5907/5907 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5908/5908 [00:00<00:00,  1.53trial/s, best loss: 1.0]\n",
      "100%|██████████| 5909/5909 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5910/5910 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 5911/5911 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 5912/5912 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 5913/5913 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5914/5914 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5915/5915 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5916/5916 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5917/5917 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5918/5918 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 5919/5919 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 5920/5920 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 5921/5921 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 5922/5922 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 5923/5923 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5924/5924 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5925/5925 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 5926/5926 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 5927/5927 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 5928/5928 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 5929/5929 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 5930/5930 [00:00<00:00,  1.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 5931/5931 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 5932/5932 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 5933/5933 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 5934/5934 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 5935/5935 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5936/5936 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5937/5937 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 5938/5938 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 5939/5939 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5940/5940 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 5941/5941 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 5942/5942 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 5943/5943 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 5944/5944 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5945/5945 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 5946/5946 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 5947/5947 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5948/5948 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5949/5949 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 5950/5950 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 5951/5951 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 5952/5952 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5953/5953 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5954/5954 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 5955/5955 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5956/5956 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 5957/5957 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 5958/5958 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 5959/5959 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5960/5960 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 5961/5961 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5962/5962 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 5963/5963 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 5964/5964 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 5965/5965 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 5966/5966 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 5967/5967 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 5968/5968 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5969/5969 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5970/5970 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 5971/5971 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 5972/5972 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 5973/5973 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 5974/5974 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 5975/5975 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 5976/5976 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 5977/5977 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 5978/5978 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 5979/5979 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 5980/5980 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 5981/5981 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 5982/5982 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 5983/5983 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 5984/5984 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 5985/5985 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 5986/5986 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 5987/5987 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 5988/5988 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 5989/5989 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 5990/5990 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 5991/5991 [00:02<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 5992/5992 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 5993/5993 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 5994/5994 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 5995/5995 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 5996/5996 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 5997/5997 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 5998/5998 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 5999/5999 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 6000/6000 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 6001/6001 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 6002/6002 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 6003/6003 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 6004/6004 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 6005/6005 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 6006/6006 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 6007/6007 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 6008/6008 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 6009/6009 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 6010/6010 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 6011/6011 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 6012/6012 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 6013/6013 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 6014/6014 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 6015/6015 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 6016/6016 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 6017/6017 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 6018/6018 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 6019/6019 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 6020/6020 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 6021/6021 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 6022/6022 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 6023/6023 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 6024/6024 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 6025/6025 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 6026/6026 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 6027/6027 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 6028/6028 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 6029/6029 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 6030/6030 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 6031/6031 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 6032/6032 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 6033/6033 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 6034/6034 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 6035/6035 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 6036/6036 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 6037/6037 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 6038/6038 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 6039/6039 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 6040/6040 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 6041/6041 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 6042/6042 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 6043/6043 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 6044/6044 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 6045/6045 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 6046/6046 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 6047/6047 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 6048/6048 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 6049/6049 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 6050/6050 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 6051/6051 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 6052/6052 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 6053/6053 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 6054/6054 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 6055/6055 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 6056/6056 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 6057/6057 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 6058/6058 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 6059/6059 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 6060/6060 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 6061/6061 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 6062/6062 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 6063/6063 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 6064/6064 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 6065/6065 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 6066/6066 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 6067/6067 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 6068/6068 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 6069/6069 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 6070/6070 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 6071/6071 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 6072/6072 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 6073/6073 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 6074/6074 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 6075/6075 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 6076/6076 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 6077/6077 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 6078/6078 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 6079/6079 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 6080/6080 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 6081/6081 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6082/6082 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 6083/6083 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 6084/6084 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 6085/6085 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 6086/6086 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 6087/6087 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 6088/6088 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6089/6089 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6090/6090 [00:00<00:00,  1.63trial/s, best loss: 1.0]\n",
      "100%|██████████| 6091/6091 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6092/6092 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6093/6093 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6094/6094 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6095/6095 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6096/6096 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 6097/6097 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6098/6098 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6099/6099 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 6100/6100 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 6101/6101 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6102/6102 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 6103/6103 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6104/6104 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6105/6105 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6106/6106 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6107/6107 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6108/6108 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 6109/6109 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6110/6110 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6111/6111 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6112/6112 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6113/6113 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 6114/6114 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6115/6115 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6116/6116 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6117/6117 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6118/6118 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 6119/6119 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 6120/6120 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 6121/6121 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6122/6122 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6123/6123 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 6124/6124 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 6125/6125 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6126/6126 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 6127/6127 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6128/6128 [00:00<00:00,  1.62trial/s, best loss: 1.0]\n",
      "100%|██████████| 6129/6129 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 6130/6130 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6131/6131 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 6132/6132 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6133/6133 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 6134/6134 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6135/6135 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 6136/6136 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6137/6137 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 6138/6138 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 6139/6139 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 6140/6140 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6141/6141 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6142/6142 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 6143/6143 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6144/6144 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 6145/6145 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6146/6146 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 6147/6147 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6148/6148 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6149/6149 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6150/6150 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 6151/6151 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 6152/6152 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 6153/6153 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6154/6154 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6155/6155 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 6156/6156 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 6157/6157 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 6158/6158 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6159/6159 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6160/6160 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 6161/6161 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6162/6162 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6163/6163 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6164/6164 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6165/6165 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6166/6166 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 6167/6167 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 6168/6168 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6169/6169 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 6170/6170 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6171/6171 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6172/6172 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6173/6173 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 6174/6174 [00:00<00:00,  1.61trial/s, best loss: 1.0]\n",
      "100%|██████████| 6175/6175 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 6176/6176 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 6177/6177 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 6178/6178 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6179/6179 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6180/6180 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6181/6181 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6182/6182 [00:00<00:00,  1.60trial/s, best loss: 1.0]\n",
      "100%|██████████| 6183/6183 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6184/6184 [00:00<00:00,  1.56trial/s, best loss: 1.0]\n",
      "100%|██████████| 6185/6185 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 6186/6186 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6187/6187 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6188/6188 [00:00<00:00,  1.58trial/s, best loss: 1.0]\n",
      "100%|██████████| 6189/6189 [00:00<00:00,  1.52trial/s, best loss: 1.0]\n",
      "100%|██████████| 6190/6190 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6191/6191 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6192/6192 [00:00<00:00,  1.54trial/s, best loss: 1.0]\n",
      "100%|██████████| 6193/6193 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6194/6194 [00:00<00:00,  1.57trial/s, best loss: 1.0]\n",
      "100%|██████████| 6195/6195 [00:00<00:00,  1.59trial/s, best loss: 1.0]\n",
      "100%|██████████| 6196/6196 [00:00<00:00,  1.55trial/s, best loss: 1.0]\n",
      "100%|██████████| 6197/6197 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 6198/6198 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 6199/6199 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 6200/6200 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 6201/6201 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 6202/6202 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 6203/6203 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 6204/6204 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 6205/6205 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6206/6206 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 6207/6207 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 6208/6208 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 6209/6209 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 6210/6210 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6211/6211 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6212/6212 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 6213/6213 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 6214/6214 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 6215/6215 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 6216/6216 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6217/6217 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 6218/6218 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 6219/6219 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 6220/6220 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 6221/6221 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 6222/6222 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 6223/6223 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 6224/6224 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6225/6225 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 6226/6226 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 6227/6227 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 6228/6228 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 6229/6229 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 6230/6230 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 6231/6231 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 6232/6232 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 6233/6233 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 6234/6234 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6235/6235 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6236/6236 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 6237/6237 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6238/6238 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6239/6239 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6240/6240 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6241/6241 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6242/6242 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 6243/6243 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 6244/6244 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 6245/6245 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6246/6246 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 6247/6247 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6248/6248 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 6249/6249 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 6250/6250 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 6251/6251 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 6252/6252 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6253/6253 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 6254/6254 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 6255/6255 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 6256/6256 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 6257/6257 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 6258/6258 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 6259/6259 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 6260/6260 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 6261/6261 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6262/6262 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6263/6263 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6264/6264 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6265/6265 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6266/6266 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 6267/6267 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6268/6268 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6269/6269 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6270/6270 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 6271/6271 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6272/6272 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6273/6273 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6274/6274 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6275/6275 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 6276/6276 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6277/6277 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6278/6278 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6279/6279 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 6280/6280 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 6281/6281 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6282/6282 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6283/6283 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 6284/6284 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6285/6285 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6286/6286 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6287/6287 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6288/6288 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6289/6289 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6290/6290 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6291/6291 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6292/6292 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6293/6293 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6294/6294 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 6295/6295 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6296/6296 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6297/6297 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6298/6298 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6299/6299 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 6300/6300 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 6301/6301 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6302/6302 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6303/6303 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 6304/6304 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6305/6305 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6306/6306 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6307/6307 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6308/6308 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6309/6309 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6310/6310 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6311/6311 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6312/6312 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6313/6313 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6314/6314 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6315/6315 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6316/6316 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6317/6317 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 6318/6318 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 6319/6319 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6320/6320 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6321/6321 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6322/6322 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6323/6323 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6324/6324 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6325/6325 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6326/6326 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 6327/6327 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 6328/6328 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6329/6329 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6330/6330 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6331/6331 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6332/6332 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6333/6333 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6334/6334 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6335/6335 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6336/6336 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6337/6337 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 6338/6338 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6339/6339 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6340/6340 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 6341/6341 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6342/6342 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 6343/6343 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6344/6344 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6345/6345 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6346/6346 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6347/6347 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6348/6348 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6349/6349 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6350/6350 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6351/6351 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 6352/6352 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6353/6353 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6354/6354 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6355/6355 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6356/6356 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 6357/6357 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6358/6358 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6359/6359 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6360/6360 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 6361/6361 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6362/6362 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6363/6363 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 6364/6364 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6365/6365 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6366/6366 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 6367/6367 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6368/6368 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6369/6369 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 6370/6370 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6371/6371 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6372/6372 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6373/6373 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6374/6374 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6375/6375 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6376/6376 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 6377/6377 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6378/6378 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6379/6379 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 6380/6380 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6381/6381 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6382/6382 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6383/6383 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 6384/6384 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6385/6385 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6386/6386 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 6387/6387 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6388/6388 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6389/6389 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6390/6390 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 6391/6391 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6392/6392 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6393/6393 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6394/6394 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6395/6395 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6396/6396 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 6397/6397 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6398/6398 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6399/6399 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 6400/6400 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6401/6401 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6402/6402 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 6403/6403 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6404/6404 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6405/6405 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6406/6406 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 6407/6407 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6408/6408 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6409/6409 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 6410/6410 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6411/6411 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6412/6412 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6413/6413 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 6414/6414 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6415/6415 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6416/6416 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 6417/6417 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6418/6418 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6419/6419 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6420/6420 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6421/6421 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6422/6422 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6423/6423 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6424/6424 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 6425/6425 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 6426/6426 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 6427/6427 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6428/6428 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6429/6429 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6430/6430 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 6431/6431 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6432/6432 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6433/6433 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6434/6434 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6435/6435 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 6436/6436 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 6437/6437 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 6438/6438 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6439/6439 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6440/6440 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6441/6441 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 6442/6442 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6443/6443 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 6444/6444 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6445/6445 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6446/6446 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 6447/6447 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 6448/6448 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 6449/6449 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 6450/6450 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 6451/6451 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 6452/6452 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 6453/6453 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 6454/6454 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 6455/6455 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 6456/6456 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 6457/6457 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 6458/6458 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 6459/6459 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 6460/6460 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 6461/6461 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 6462/6462 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 6463/6463 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 6464/6464 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 6465/6465 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 6466/6466 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 6467/6467 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 6468/6468 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 6469/6469 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 6470/6470 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 6471/6471 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 6472/6472 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 6473/6473 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 6474/6474 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 6475/6475 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 6476/6476 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 6477/6477 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 6478/6478 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 6479/6479 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 6480/6480 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 6481/6481 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 6482/6482 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 6483/6483 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 6484/6484 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 6485/6485 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 6486/6486 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 6487/6487 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 6488/6488 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 6489/6489 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 6490/6490 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 6491/6491 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 6492/6492 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6492/6493 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6493/6493 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6493/6494 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6494/6494 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 6495/6495 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6495/6496 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6496/6496 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6496/6497 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6497/6497 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6497/6498 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6498/6498 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6498/6499 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6499/6499 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6499/6500 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6500/6500 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6500/6501 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6501/6501 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6501/6502 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6502/6502 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6502/6503 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6503/6503 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6503/6504 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6504/6504 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6504/6505 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6505/6505 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6505/6506 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6506/6506 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6506/6507 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6507/6507 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6507/6508 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6508/6508 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6508/6509 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6509/6509 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6509/6510 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6510/6510 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 6511/6511 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6511/6512 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6512/6512 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6512/6513 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6513/6513 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6513/6514 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6514/6514 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6514/6515 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6515/6515 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 6516/6516 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6516/6517 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6517/6517 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6517/6518 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6518/6518 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 6519/6519 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6519/6520 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6520/6520 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6520/6521 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6521/6521 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6521/6522 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6522/6522 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6522/6523 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6523/6523 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6523/6524 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6524/6524 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 6525/6525 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6525/6526 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6526/6526 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6526/6527 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6527/6527 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6527/6528 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6528/6528 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6528/6529 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6529/6529 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6529/6530 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6530/6530 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6530/6531 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6531/6531 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6531/6532 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6532/6532 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6532/6533 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6533/6533 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6533/6534 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6534/6534 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6534/6535 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6535/6535 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6535/6536 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6536/6536 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6536/6537 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6537/6537 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 6538/6538 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 6539/6539 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6539/6540 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6540/6540 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 6541/6541 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6541/6542 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6542/6542 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6542/6543 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6543/6543 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 6544/6544 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6544/6545 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6545/6545 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6545/6546 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6546/6546 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 6547/6547 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 6548/6548 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 6549/6549 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6549/6550 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6550/6550 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6550/6551 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6551/6551 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6551/6552 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6552/6552 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6552/6553 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6553/6553 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 6554/6554 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 6555/6555 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 6556/6556 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6556/6557 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6557/6557 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6557/6558 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6558/6558 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6558/6559 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6559/6559 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6559/6560 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6560/6560 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 6561/6561 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 6562/6562 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6562/6563 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6563/6563 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 6564/6564 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 6565/6565 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6565/6566 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6566/6566 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 6567/6567 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 6568/6568 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6568/6569 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6569/6569 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 6570/6570 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6570/6571 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6571/6571 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 6572/6572 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 6573/6573 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 6574/6574 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 6575/6575 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 6576/6576 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 6577/6577 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 6578/6578 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 6579/6579 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 6580/6580 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 6581/6581 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 6582/6582 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 6583/6583 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 6584/6584 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 6585/6585 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 6586/6586 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 6587/6587 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 6588/6588 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 6589/6589 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 6590/6590 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 6591/6591 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 6592/6592 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 6593/6593 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 6594/6594 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 6595/6595 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 6596/6596 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 6597/6597 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 6598/6598 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 6599/6599 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 6600/6600 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 6601/6601 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 6602/6602 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 6603/6603 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 6604/6604 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 6605/6605 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 6606/6606 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 6607/6607 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 6608/6608 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 6609/6609 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 6610/6610 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 6611/6611 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 6612/6612 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 6613/6613 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 6614/6614 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 6615/6615 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 6616/6616 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 6617/6617 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 6618/6618 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 6619/6619 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 6620/6620 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 6621/6621 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 6622/6622 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 6623/6623 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 6624/6624 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 6625/6625 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 6626/6626 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 6627/6627 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 6628/6628 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 6629/6629 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 6630/6630 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 6631/6631 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 6632/6632 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 6633/6633 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 6634/6634 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 6635/6635 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 6636/6636 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 6637/6637 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 6638/6638 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 6639/6639 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 6640/6640 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 6641/6641 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 6642/6642 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 6643/6643 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 6644/6644 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 6645/6645 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 6646/6646 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 6647/6647 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 6648/6648 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 6649/6649 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 6650/6650 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 6651/6651 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 6652/6652 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 6653/6653 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 6654/6654 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 6655/6655 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 6656/6656 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 6657/6657 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 6658/6658 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 6659/6659 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 6660/6660 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 6661/6661 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 6662/6662 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6662/6663 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6663/6663 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6663/6664 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6664/6664 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6664/6665 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6665/6665 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6665/6666 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6666/6666 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6666/6667 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6667/6667 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6667/6668 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6668/6668 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6668/6669 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6669/6669 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6669/6670 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6670/6670 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6670/6671 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6671/6671 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6671/6672 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6672/6672 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6672/6673 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6673/6673 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6673/6674 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6674/6674 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6674/6675 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6675/6675 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6675/6676 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6676/6676 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 6677/6677 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6677/6678 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6678/6678 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6678/6679 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6679/6679 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6679/6680 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6680/6681 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6681/6681 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6681/6682 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6682/6682 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6682/6683 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6683/6683 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6683/6684 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6684/6684 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6684/6685 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6685/6685 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6685/6686 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6686/6686 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6686/6687 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6687/6687 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6687/6688 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6688/6688 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6688/6689 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6689/6689 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6689/6690 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6690/6690 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6690/6691 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6691/6691 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6691/6692 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6692/6692 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6692/6693 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6693/6693 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6693/6694 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6694/6694 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6694/6695 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6695/6695 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6695/6696 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6696/6696 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6696/6697 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6697/6697 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6697/6698 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6698/6698 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6698/6699 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6699/6699 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6699/6700 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6700/6700 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6700/6701 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6701/6701 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6701/6702 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6702/6702 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6702/6703 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6703/6703 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 6704/6704 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6704/6705 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6705/6705 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6705/6706 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6706/6706 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6706/6707 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6707/6707 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 6708/6708 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6708/6709 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6709/6709 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6709/6710 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6710/6710 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6710/6711 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6711/6711 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6711/6712 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6712/6712 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6712/6713 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6713/6713 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6713/6714 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6714/6714 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6714/6715 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6715/6715 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6715/6716 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6716/6716 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6716/6717 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6717/6717 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6717/6718 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6718/6718 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6718/6719 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6719/6719 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6719/6720 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6720/6720 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6720/6721 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6721/6721 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 6722/6722 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 6723/6723 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6723/6724 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6724/6724 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6724/6725 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6725/6725 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6725/6726 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6726/6726 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6726/6727 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6727/6727 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 6728/6728 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6728/6729 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6729/6729 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 6730/6730 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6730/6731 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6731/6731 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6731/6732 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6732/6732 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6732/6733 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6733/6733 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6733/6734 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6734/6734 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6734/6735 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6735/6735 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6735/6736 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6736/6736 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6736/6737 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6737/6737 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 6738/6738 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6738/6739 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6739/6739 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6739/6740 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6740/6740 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 6741/6741 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 6742/6742 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6742/6743 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6743/6743 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 6744/6744 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6744/6745 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6745/6745 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6745/6746 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6746/6746 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 6747/6747 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 6748/6748 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6748/6749 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6749/6749 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6749/6750 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6750/6751 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6751/6751 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6751/6752 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6752/6752 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 6753/6753 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6753/6754 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6754/6754 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 6755/6755 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|█████████▉| 6755/6756 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6756/6756 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6756/6757 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6757/6757 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6757/6758 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6758/6758 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6758/6759 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6759/6759 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6759/6760 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6760/6760 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6760/6761 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6761/6761 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6761/6762 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6762/6762 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6762/6763 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6763/6763 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6763/6764 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6764/6764 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6764/6765 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6765/6765 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6765/6766 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6766/6766 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6766/6767 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6767/6767 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6767/6768 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6768/6768 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6768/6769 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6769/6769 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6769/6770 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6770/6770 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6770/6771 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6771/6771 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6771/6772 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6772/6772 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6772/6773 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6773/6773 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6773/6774 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6774/6774 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6774/6775 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6775/6775 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 6775/6776 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6776/6776 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 6777/6777 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 6778/6778 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 6779/6779 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 6780/6780 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 6781/6781 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 6782/6782 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 6783/6783 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 6784/6784 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 6785/6785 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 6786/6786 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 6787/6787 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 6788/6788 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 6789/6789 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 6790/6790 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 6791/6791 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 6792/6792 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 6793/6793 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 6794/6794 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 6795/6795 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 6796/6796 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 6797/6797 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 6798/6798 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 6799/6799 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 6800/6800 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 6801/6801 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 6802/6802 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 6803/6803 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 6804/6804 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 6805/6805 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 6806/6806 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 6807/6807 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 6808/6808 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 6809/6809 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 6810/6810 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 6811/6811 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 6812/6812 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 6813/6813 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 6814/6814 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 6815/6815 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 6816/6816 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 6817/6817 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 6818/6818 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 6819/6819 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 6820/6820 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 6821/6821 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 6822/6822 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 6823/6823 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 6824/6824 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 6825/6825 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 6826/6826 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 6827/6827 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 6828/6828 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 6829/6829 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 6830/6830 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 6831/6831 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 6832/6832 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 6833/6833 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 6834/6834 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 6835/6835 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 6836/6836 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 6837/6837 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 6838/6838 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 6839/6839 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 6840/6840 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 6841/6841 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 6842/6842 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 6843/6843 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 6844/6844 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 6845/6845 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 6846/6846 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 6847/6847 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 6848/6848 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 6849/6849 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 6850/6850 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 6851/6851 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 6852/6852 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 6853/6853 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 6854/6854 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 6855/6855 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 6856/6856 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 6857/6857 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 6858/6858 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 6859/6859 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 6860/6860 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 6861/6861 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 6862/6862 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 6863/6863 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 6864/6864 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 6865/6865 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 6866/6866 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 6867/6867 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 6868/6868 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 6869/6869 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 6870/6870 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 6871/6871 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 6872/6872 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 6873/6873 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 6874/6874 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 6875/6875 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 6876/6876 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 6877/6877 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 6878/6878 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 6879/6879 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 6880/6880 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 6881/6881 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 6882/6882 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 6883/6883 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 6884/6884 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 6885/6885 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 6886/6886 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 6887/6887 [00:04<00:00,  4.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 6888/6888 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 6889/6889 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 6890/6890 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 6891/6891 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 6892/6892 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 6893/6893 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 6894/6894 [00:04<00:00,  4.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 6895/6895 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 6896/6896 [00:04<00:00,  4.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 6897/6897 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 6898/6898 [00:04<00:00,  4.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 6899/6899 [00:04<00:00,  4.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 6900/6900 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 6901/6901 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 6902/6902 [00:04<00:00,  4.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 6903/6903 [00:04<00:00,  4.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 6904/6904 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 6905/6905 [00:05<00:00,  5.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 6906/6906 [00:04<00:00,  4.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 6907/6907 [00:04<00:00,  4.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 6908/6908 [00:05<00:00,  5.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 6909/6909 [00:04<00:00,  4.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 6910/6910 [00:04<00:00,  4.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 6911/6911 [00:05<00:00,  5.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 6912/6912 [00:04<00:00,  4.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 6913/6913 [00:05<00:00,  5.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 6914/6914 [00:05<00:00,  5.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 6915/6915 [00:06<00:00,  6.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 6916/6916 [00:05<00:00,  5.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 6917/6917 [00:05<00:00,  5.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 6918/6918 [00:05<00:00,  5.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 6919/6919 [00:06<00:00,  6.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 6920/6920 [00:06<00:00,  6.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 6921/6921 [00:06<00:00,  6.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 6922/6922 [00:05<00:00,  5.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 6923/6923 [00:06<00:00,  6.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 6924/6924 [00:05<00:00,  5.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 6925/6925 [00:05<00:00,  5.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 6926/6926 [00:06<00:00,  6.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 6927/6927 [00:06<00:00,  6.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 6928/6928 [00:05<00:00,  5.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 6929/6929 [00:06<00:00,  6.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 6930/6930 [00:05<00:00,  5.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 6931/6931 [00:06<00:00,  6.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 6932/6932 [00:06<00:00,  6.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 6933/6933 [00:05<00:00,  5.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 6934/6934 [00:07<00:00,  7.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 6935/6935 [00:06<00:00,  6.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 6936/6936 [00:05<00:00,  5.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 6937/6937 [00:06<00:00,  6.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 6938/6938 [00:05<00:00,  5.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 6939/6939 [00:05<00:00,  5.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 6940/6940 [00:06<00:00,  6.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 6941/6941 [00:05<00:00,  6.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 6942/6942 [00:06<00:00,  6.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 6943/6943 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 6944/6944 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 6945/6945 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6946/6946 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 6947/6947 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 6948/6948 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6949/6949 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 6950/6950 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 6951/6951 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6952/6952 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 6953/6953 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 6954/6954 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 6955/6955 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6956/6956 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 6957/6957 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6958/6958 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6959/6959 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 6960/6960 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 6961/6961 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 6962/6962 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 6963/6963 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 6964/6964 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 6965/6965 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6966/6966 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6967/6967 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6968/6968 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 6969/6969 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 6970/6970 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6971/6971 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 6972/6972 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 6973/6973 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6974/6974 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 6975/6975 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6976/6976 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6977/6977 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 6978/6978 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6979/6979 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 6980/6980 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6981/6981 [00:00<00:00,  1.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 6982/6982 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 6983/6983 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 6984/6984 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 6985/6985 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 6986/6986 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 6987/6987 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 6988/6988 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 6989/6989 [00:00<00:00,  1.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 6990/6990 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6991/6991 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 6992/6992 [00:00<00:00,  1.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 6993/6993 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6994/6994 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 6995/6995 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 6996/6996 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 6997/6997 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 6998/6998 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 6999/6999 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7000/7000 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7001/7001 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7002/7002 [00:00<00:00,  1.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 7003/7003 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7004/7004 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 7005/7005 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7006/7006 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7007/7007 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 7008/7008 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7009/7009 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7010/7010 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7011/7011 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7012/7012 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7013/7013 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7014/7014 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 7015/7015 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7016/7016 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7017/7017 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7018/7018 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7019/7019 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7020/7020 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7021/7021 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7022/7022 [00:00<00:00,  1.45trial/s, best loss: 1.0]\n",
      "100%|██████████| 7023/7023 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7024/7024 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 7025/7025 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7026/7026 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7027/7027 [00:00<00:00,  1.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 7028/7028 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7029/7029 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7030/7030 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7031/7031 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7032/7032 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7033/7033 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7034/7034 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 7035/7035 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7036/7036 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7037/7037 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7038/7038 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7039/7039 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7040/7040 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7041/7041 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7042/7042 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7043/7043 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7044/7044 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 7045/7045 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7046/7046 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7047/7047 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7048/7048 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7049/7049 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7050/7050 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7051/7051 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7052/7052 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7053/7053 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7054/7054 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 7055/7055 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7056/7056 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7057/7057 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7058/7058 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7059/7059 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7060/7060 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7061/7061 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7062/7062 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7063/7063 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7064/7064 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 7065/7065 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7066/7066 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7067/7067 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7068/7068 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7069/7069 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7070/7070 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7071/7071 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7072/7072 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7073/7073 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7074/7074 [00:00<00:00,  1.44trial/s, best loss: 1.0]\n",
      "100%|██████████| 7075/7075 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7076/7076 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7077/7077 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7078/7078 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7079/7079 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7080/7080 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7081/7081 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7082/7082 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7083/7083 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7084/7084 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 7085/7085 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7086/7086 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7087/7087 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7088/7088 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7089/7089 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7090/7090 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7091/7091 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7092/7092 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7093/7093 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7094/7094 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 7095/7095 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7096/7096 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7097/7097 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7098/7098 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7099/7099 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7100/7100 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7101/7101 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7102/7102 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7103/7103 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7104/7104 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7105/7105 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7106/7106 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7107/7107 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7108/7108 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7109/7109 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7110/7110 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7111/7111 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7112/7112 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7113/7113 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7114/7114 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7115/7115 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7116/7116 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7117/7117 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7118/7118 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7119/7119 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7120/7120 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7121/7121 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7122/7122 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7123/7123 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 7124/7124 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7125/7125 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7126/7126 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7127/7127 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7128/7128 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7129/7129 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7130/7130 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7131/7131 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7132/7132 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7133/7133 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7134/7134 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7135/7135 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7136/7136 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7137/7137 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7138/7138 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7139/7139 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7140/7140 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7141/7141 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7142/7142 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7143/7143 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7144/7144 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7145/7145 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7146/7146 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7147/7147 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7148/7148 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7149/7149 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7150/7150 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7151/7151 [00:00<00:00,  1.43trial/s, best loss: 1.0]\n",
      "100%|██████████| 7152/7152 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 7153/7153 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7154/7154 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7155/7155 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7156/7156 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7157/7157 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7158/7158 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7159/7159 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7160/7160 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7161/7161 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7162/7162 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 7163/7163 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 7164/7164 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7165/7165 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7166/7166 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7167/7167 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7168/7168 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7169/7169 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7170/7170 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7171/7171 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7172/7172 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 7173/7173 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7174/7174 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7175/7175 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7176/7176 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7177/7177 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7178/7178 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7179/7179 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7180/7180 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7181/7181 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7182/7182 [00:00<00:00,  1.42trial/s, best loss: 1.0]\n",
      "100%|██████████| 7183/7183 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7184/7184 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7185/7185 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7186/7186 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7187/7187 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7188/7188 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7189/7189 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7190/7190 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7191/7191 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7192/7192 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 7193/7193 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7194/7194 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7195/7195 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7196/7196 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7197/7197 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7198/7198 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7199/7199 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7200/7200 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7201/7201 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7202/7202 [00:00<00:00,  1.41trial/s, best loss: 1.0]\n",
      "100%|██████████| 7203/7203 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 7204/7204 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7205/7205 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7206/7206 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7207/7207 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7208/7208 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7209/7209 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7210/7210 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7211/7211 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7212/7212 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7213/7213 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7214/7214 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7215/7215 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7216/7216 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7217/7217 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7218/7218 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7219/7219 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7220/7220 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7221/7221 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 7222/7222 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7223/7223 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7224/7224 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7225/7225 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7226/7226 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7227/7227 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7228/7228 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7229/7229 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7230/7230 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7231/7231 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 7232/7232 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 7233/7233 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7234/7234 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7235/7235 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7236/7236 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7237/7237 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7238/7238 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7239/7239 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7240/7240 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7241/7241 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7242/7242 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7243/7243 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7244/7244 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7245/7245 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7246/7246 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7247/7247 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7248/7248 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7249/7249 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 7250/7250 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7251/7251 [00:00<00:00,  1.40trial/s, best loss: 1.0]\n",
      "100%|██████████| 7252/7252 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7253/7253 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7254/7254 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7255/7255 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7256/7256 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7257/7257 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7258/7258 [00:00<00:00,  1.39trial/s, best loss: 1.0]\n",
      "100%|██████████| 7259/7259 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7260/7260 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 7261/7261 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7262/7262 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7263/7263 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7264/7264 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7265/7265 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7266/7266 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 7267/7267 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7268/7268 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7269/7269 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7270/7270 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 7271/7271 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7272/7272 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7273/7273 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7274/7274 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7275/7275 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7276/7276 [00:00<00:00,  1.33trial/s, best loss: 1.0]\n",
      "100%|██████████| 7277/7277 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7278/7278 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 7279/7279 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7280/7280 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 7281/7281 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7282/7282 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7283/7283 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7284/7284 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 7285/7285 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7286/7286 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 7287/7287 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7288/7288 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7289/7289 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 7290/7290 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7291/7291 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 7292/7292 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 7293/7293 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 7294/7294 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7295/7295 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7296/7296 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7297/7297 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 7298/7298 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7299/7299 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 7300/7300 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7301/7301 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 7302/7302 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7303/7303 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7304/7304 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 7305/7305 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7306/7306 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7307/7307 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7308/7308 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7309/7309 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 7310/7310 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7311/7311 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7312/7312 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7313/7313 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7314/7314 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7315/7315 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7316/7316 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 7317/7317 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7318/7318 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 7319/7319 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 7320/7320 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7321/7321 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7322/7322 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7323/7323 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7324/7324 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 7325/7325 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7326/7326 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 7327/7327 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 7328/7328 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 7329/7329 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7330/7330 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7331/7331 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7332/7332 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 7333/7333 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7334/7334 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7335/7335 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7336/7336 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7337/7337 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7338/7338 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 7339/7339 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7340/7340 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7341/7341 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7342/7342 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7343/7343 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 7344/7344 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7345/7345 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7346/7346 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7347/7347 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7348/7348 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 7349/7349 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7350/7350 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7351/7351 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7352/7352 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7353/7353 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7354/7354 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7355/7355 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7356/7356 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7357/7357 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 7358/7358 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7359/7359 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7360/7360 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7361/7361 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7362/7362 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7363/7363 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7364/7364 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7365/7365 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7366/7366 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 7367/7367 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 7368/7368 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 7369/7369 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7370/7370 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7371/7371 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7372/7372 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 7373/7373 [00:00<00:00,  1.38trial/s, best loss: 1.0]\n",
      "100%|██████████| 7374/7374 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7375/7375 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7376/7376 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7377/7377 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 7378/7378 [00:00<00:00,  1.34trial/s, best loss: 1.0]\n",
      "100%|██████████| 7379/7379 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7380/7380 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 7381/7381 [00:00<00:00,  1.31trial/s, best loss: 1.0]\n",
      "100%|██████████| 7382/7382 [00:00<00:00,  1.36trial/s, best loss: 1.0]\n",
      "100%|██████████| 7383/7383 [00:00<00:00,  1.35trial/s, best loss: 1.0]\n",
      "100%|██████████| 7384/7384 [00:00<00:00,  1.37trial/s, best loss: 1.0]\n",
      "100%|██████████| 7385/7385 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7386/7386 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7387/7387 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 7388/7388 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7389/7389 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7390/7390 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7391/7391 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7392/7392 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 7393/7393 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7394/7394 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 7395/7395 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7396/7396 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7397/7397 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 7398/7398 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7399/7399 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7400/7400 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 7401/7401 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7402/7402 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7403/7403 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 7404/7404 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7405/7405 [00:00<00:00,  1.26trial/s, best loss: 1.0]\n",
      "100%|██████████| 7406/7406 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 7407/7407 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7408/7408 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7409/7409 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7410/7410 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7411/7411 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7412/7412 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 7413/7413 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 7414/7414 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 7415/7415 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 7416/7416 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 7417/7417 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 7418/7418 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 7419/7419 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 7420/7420 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 7421/7421 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 7422/7422 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 7423/7423 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 7424/7424 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 7425/7425 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 7426/7426 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 7427/7427 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 7428/7428 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 7429/7429 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 7430/7430 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 7431/7431 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 7432/7432 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 7433/7433 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 7434/7434 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 7435/7435 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 7436/7436 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 7437/7437 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 7438/7438 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 7439/7439 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 7440/7440 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 7441/7441 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 7442/7442 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 7443/7443 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 7444/7444 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 7445/7445 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 7446/7446 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 7447/7447 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 7448/7448 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 7449/7449 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 7450/7450 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 7451/7451 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 7452/7452 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 7453/7453 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 7454/7454 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 7455/7455 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 7456/7456 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 7457/7457 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 7458/7458 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 7459/7459 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 7460/7460 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 7461/7461 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 7462/7462 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 7463/7463 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 7464/7464 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 7465/7465 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 7466/7466 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 7467/7467 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 7468/7468 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 7469/7469 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 7470/7470 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 7471/7471 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 7472/7472 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 7473/7473 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 7474/7474 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 7475/7475 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 7476/7476 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 7477/7477 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 7478/7478 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 7479/7479 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 7480/7480 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 7481/7481 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 7482/7482 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 7483/7483 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 7484/7484 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 7485/7485 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 7486/7486 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 7487/7487 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 7488/7488 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 7489/7489 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 7490/7490 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 7491/7491 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 7492/7492 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 7493/7493 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 7494/7494 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 7495/7495 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 7496/7496 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 7497/7497 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 7498/7498 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 7499/7499 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 7500/7500 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 7501/7501 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 7502/7502 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 7503/7503 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 7504/7504 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 7505/7505 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 7506/7506 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 7507/7507 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 7508/7508 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 7509/7509 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 7510/7510 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 7511/7511 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 7512/7512 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 7513/7513 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 7514/7514 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 7515/7515 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 7516/7516 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 7517/7517 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 7518/7518 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 7519/7519 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 7520/7520 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 7521/7521 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 7522/7522 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 7523/7523 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 7524/7524 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 7525/7525 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 7526/7526 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 7527/7527 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 7528/7528 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 7529/7529 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 7530/7530 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 7531/7531 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 7532/7532 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 7533/7533 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 7534/7534 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 7535/7535 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 7536/7536 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 7537/7537 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 7538/7538 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 7539/7539 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 7540/7540 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 7541/7541 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 7542/7542 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 7543/7543 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 7544/7544 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 7545/7545 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 7546/7546 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 7547/7547 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 7548/7548 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 7549/7549 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 7550/7550 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 7551/7551 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 7552/7552 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 7553/7553 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 7554/7554 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 7555/7555 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 7556/7556 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 7557/7557 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 7558/7558 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 7559/7559 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 7560/7560 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 7561/7561 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 7562/7562 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 7563/7563 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 7564/7564 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 7565/7565 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 7566/7566 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 7567/7567 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 7568/7568 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 7569/7569 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 7570/7570 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 7571/7571 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 7572/7572 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 7573/7573 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 7574/7574 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 7575/7575 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 7576/7576 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 7577/7577 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 7578/7578 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 7579/7579 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 7580/7580 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 7581/7581 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 7582/7582 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 7583/7583 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 7584/7584 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 7585/7585 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 7586/7586 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 7587/7587 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 7588/7588 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 7589/7589 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 7590/7590 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 7591/7591 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 7592/7592 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 7593/7593 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 7594/7594 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 7595/7595 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 7596/7596 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 7597/7597 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 7598/7598 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 7599/7599 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 7600/7600 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 7601/7601 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 7602/7602 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 7603/7603 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 7604/7604 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 7605/7605 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 7606/7606 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 7607/7607 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 7608/7608 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 7609/7609 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 7610/7610 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 7611/7611 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 7612/7612 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 7613/7613 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 7614/7614 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 7615/7615 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 7616/7616 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 7617/7617 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 7618/7618 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 7619/7619 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 7620/7620 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 7621/7621 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 7622/7622 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 7623/7623 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 7624/7624 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 7625/7625 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 7626/7626 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 7627/7627 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 7628/7628 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 7629/7629 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 7630/7630 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 7631/7631 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 7632/7632 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 7633/7633 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 7634/7634 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 7635/7635 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 7636/7636 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 7637/7637 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 7638/7638 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 7639/7639 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 7640/7640 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 7641/7641 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 7642/7642 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 7643/7643 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 7644/7644 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 7645/7645 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 7646/7646 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 7647/7647 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 7648/7648 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 7649/7649 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 7650/7650 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 7651/7651 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 7652/7652 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 7653/7653 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 7654/7654 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 7655/7655 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 7656/7656 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 7657/7657 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 7658/7658 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 7659/7659 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 7660/7660 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 7661/7661 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 7662/7662 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 7663/7663 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 7664/7664 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 7665/7665 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 7666/7666 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 7667/7667 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 7668/7668 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 7669/7669 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 7670/7670 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 7671/7671 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 7672/7672 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 7673/7673 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 7674/7674 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 7675/7675 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 7676/7676 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 7677/7677 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 7678/7678 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 7679/7679 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 7680/7680 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 7681/7681 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 7682/7682 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 7683/7683 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 7684/7684 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 7685/7685 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 7686/7686 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 7687/7687 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 7688/7688 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 7689/7689 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 7690/7690 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 7691/7691 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 7692/7692 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 7693/7693 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 7694/7694 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 7695/7695 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 7696/7696 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 7697/7697 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 7698/7698 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 7699/7699 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 7700/7700 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 7701/7701 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 7702/7702 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 7703/7703 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7704/7704 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7705/7705 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7706/7706 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7707/7707 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7708/7708 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7709/7709 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7710/7710 [00:00<00:00,  1.28trial/s, best loss: 1.0]\n",
      "100%|██████████| 7711/7711 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7712/7712 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 7713/7713 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7714/7714 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 7715/7715 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7716/7716 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7717/7717 [00:00<00:00,  1.27trial/s, best loss: 1.0]\n",
      "100%|██████████| 7718/7718 [00:00<00:00,  1.29trial/s, best loss: 1.0]\n",
      "100%|██████████| 7719/7719 [00:00<00:00,  1.30trial/s, best loss: 1.0]\n",
      "100%|██████████| 7720/7720 [00:00<00:00,  1.32trial/s, best loss: 1.0]\n",
      "100%|██████████| 7721/7721 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7722/7722 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 7723/7723 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 7724/7724 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 7725/7725 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 7726/7726 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 7727/7727 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7728/7728 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7729/7729 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7730/7730 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 7731/7731 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7732/7732 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7733/7733 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7734/7734 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7735/7735 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7736/7736 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7737/7737 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7738/7738 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7739/7739 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7740/7740 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7741/7741 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 7742/7742 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7743/7743 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7744/7744 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7745/7745 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7746/7746 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 7747/7747 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 7748/7748 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7749/7749 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7750/7750 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7751/7751 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 7752/7752 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7753/7753 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7754/7754 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7755/7755 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 7756/7756 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7757/7757 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 7758/7758 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 7759/7759 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 7760/7760 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7761/7761 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 7762/7762 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7763/7763 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7764/7764 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7765/7765 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7766/7766 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7767/7767 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 7768/7768 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7769/7769 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7770/7770 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7771/7771 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 7772/7772 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7773/7773 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7774/7774 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 7775/7775 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7776/7776 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7777/7777 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 7778/7778 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 7779/7779 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7780/7780 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 7781/7781 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 7782/7782 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 7783/7783 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 7784/7784 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 7785/7785 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7786/7786 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 7787/7787 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7788/7788 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7789/7789 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7790/7790 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 7791/7791 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 7792/7792 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 7793/7793 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7794/7794 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7795/7795 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7796/7796 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7797/7797 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7798/7798 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7799/7799 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7800/7800 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7801/7801 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 7802/7802 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7803/7803 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 7804/7804 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 7805/7805 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 7806/7806 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 7807/7807 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 7808/7808 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 7809/7809 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7810/7810 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7811/7811 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 7812/7812 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7813/7813 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7814/7814 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7815/7815 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7816/7816 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7817/7817 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7818/7818 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 7819/7819 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7820/7820 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7821/7821 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 7822/7822 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7823/7823 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7824/7824 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7825/7825 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7826/7826 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7827/7827 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7828/7828 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7829/7829 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7830/7830 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7831/7831 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 7832/7832 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7833/7833 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7834/7834 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7835/7835 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7836/7836 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7837/7837 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 7838/7838 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7839/7839 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7840/7840 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7841/7841 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 7842/7842 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7843/7843 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7844/7844 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7845/7845 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7846/7846 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7847/7847 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7848/7848 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7849/7849 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7850/7850 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7851/7851 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 7852/7852 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7853/7853 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7854/7854 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7855/7855 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7856/7856 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7857/7857 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7858/7858 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7859/7859 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7860/7860 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7861/7861 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 7862/7862 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7863/7863 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7864/7864 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7865/7865 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7866/7866 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7867/7867 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7868/7868 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7869/7869 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7870/7870 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7871/7871 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 7872/7872 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7873/7873 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7874/7874 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7875/7875 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7876/7876 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7877/7877 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7878/7878 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7879/7879 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7880/7880 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7881/7881 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 7882/7882 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7883/7883 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7884/7884 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7885/7885 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7886/7886 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7887/7887 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7888/7888 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7889/7889 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7890/7890 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7891/7891 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 7892/7892 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7893/7893 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 7894/7894 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7895/7895 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7896/7896 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7897/7897 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7898/7898 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7899/7899 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7900/7900 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7901/7901 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 7902/7902 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7903/7903 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7904/7904 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7905/7905 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7906/7906 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7907/7907 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7908/7908 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7909/7909 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7910/7910 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7911/7911 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7912/7912 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7913/7913 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7914/7914 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7915/7915 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7916/7916 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7917/7917 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7918/7918 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7919/7919 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7920/7920 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7921/7921 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 7922/7922 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7923/7923 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7924/7924 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7925/7925 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7926/7926 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7927/7927 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7928/7928 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7929/7929 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7930/7930 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 7931/7931 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7932/7932 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7933/7933 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7934/7934 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7935/7935 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7936/7936 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7937/7937 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7938/7938 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7939/7939 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7940/7940 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7941/7941 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 7942/7942 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7943/7943 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7944/7944 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 7945/7945 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7946/7946 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 7947/7947 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7948/7948 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7949/7949 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 7950/7950 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7951/7951 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 7952/7952 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7953/7953 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7954/7954 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7955/7955 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7956/7956 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7957/7957 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7958/7958 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7959/7959 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7960/7960 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7961/7961 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7962/7962 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7963/7963 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7964/7964 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7965/7965 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7966/7966 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7967/7967 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7968/7968 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7969/7969 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7970/7970 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7971/7971 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 7972/7972 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7973/7973 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 7974/7974 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7975/7975 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7976/7976 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7977/7977 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7978/7978 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7979/7979 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 7980/7980 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7981/7981 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 7982/7982 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7983/7983 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 7984/7984 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 7985/7985 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7986/7986 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 7987/7987 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 7988/7988 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7989/7989 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7990/7990 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 7991/7991 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 7992/7992 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7993/7993 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 7994/7994 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7995/7995 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 7996/7996 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 7997/7997 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 7998/7998 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 7999/7999 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8000/8000 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8001/8001 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 8002/8002 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8003/8003 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8004/8004 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 8005/8005 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8006/8006 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8007/8007 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8008/8008 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8009/8009 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8010/8010 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8011/8011 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 8012/8012 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8013/8013 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8014/8014 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8015/8015 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8016/8016 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 8017/8017 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8018/8018 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 8019/8019 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 8020/8020 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 8021/8021 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 8022/8022 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8023/8023 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8024/8024 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8025/8025 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8026/8026 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8027/8027 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8028/8028 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8029/8029 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8030/8030 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8031/8031 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8032/8032 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8033/8033 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8034/8034 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8035/8035 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8036/8036 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8037/8037 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 8038/8038 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8039/8039 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8040/8040 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 8041/8041 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8042/8042 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8043/8043 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8044/8044 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8045/8045 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8046/8046 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 8047/8047 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8048/8048 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8049/8049 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8050/8050 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 8051/8051 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8052/8052 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 8053/8053 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8054/8054 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8055/8055 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8056/8056 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8057/8057 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8058/8058 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8059/8059 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8060/8060 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 8061/8061 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 8062/8062 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8063/8063 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8064/8064 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8065/8065 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8066/8066 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 8067/8067 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8068/8068 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8069/8069 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8070/8070 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 8071/8071 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 8072/8072 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8073/8073 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 8074/8074 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8075/8075 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8076/8076 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8077/8077 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8078/8078 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8079/8079 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8080/8080 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 8081/8081 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8082/8082 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8083/8083 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8084/8084 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8085/8085 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8086/8086 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8087/8087 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8088/8088 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8089/8089 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 8090/8090 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 8091/8091 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 8092/8092 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8093/8093 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 8094/8094 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8095/8095 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8096/8096 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8097/8097 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8098/8098 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 8099/8099 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8100/8100 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 8101/8101 [00:05<00:00,  5.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 8102/8102 [00:05<00:00,  5.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 8103/8103 [00:06<00:00,  6.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 8104/8104 [00:05<00:00,  5.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 8105/8105 [00:06<00:00,  6.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 8106/8106 [00:05<00:00,  5.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 8107/8107 [00:06<00:00,  6.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 8108/8108 [00:05<00:00,  5.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 8109/8109 [00:05<00:00,  5.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 8110/8110 [00:06<00:00,  6.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 8111/8111 [00:05<00:00,  5.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 8112/8112 [00:06<00:00,  6.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 8113/8113 [00:05<00:00,  5.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 8114/8114 [00:05<00:00,  5.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 8115/8115 [00:04<00:00,  4.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 8116/8116 [00:05<00:00,  5.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 8117/8117 [00:04<00:00,  4.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 8118/8118 [00:05<00:00,  5.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 8119/8119 [00:05<00:00,  5.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 8120/8120 [00:06<00:00,  6.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 8121/8121 [00:04<00:00,  5.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 8122/8122 [00:04<00:00,  4.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 8123/8123 [00:04<00:00,  4.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 8124/8124 [00:04<00:00,  4.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 8125/8125 [00:04<00:00,  4.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 8126/8126 [00:04<00:00,  4.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 8127/8127 [00:04<00:00,  4.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 8128/8128 [00:04<00:00,  4.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 8129/8129 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 8130/8130 [00:05<00:00,  5.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 8131/8131 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 8132/8132 [00:04<00:00,  4.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 8133/8133 [00:04<00:00,  4.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 8134/8134 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 8135/8135 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 8136/8136 [00:04<00:00,  4.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 8137/8137 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 8138/8138 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 8139/8139 [00:03<00:00,  3.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 8140/8140 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 8141/8141 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 8142/8142 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 8143/8143 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 8144/8144 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 8145/8145 [00:03<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 8146/8146 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 8147/8147 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 8148/8148 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 8149/8149 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 8150/8150 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 8151/8151 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 8152/8152 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 8153/8153 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 8154/8154 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 8155/8155 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 8156/8156 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 8157/8157 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 8158/8158 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 8159/8159 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 8160/8160 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 8161/8161 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 8162/8162 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 8163/8163 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8164/8164 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8165/8165 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8166/8166 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8167/8167 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8168/8168 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8169/8169 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8170/8170 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 8171/8171 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8172/8172 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8173/8173 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8174/8174 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8175/8175 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8176/8176 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8177/8177 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8178/8178 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8179/8179 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8180/8180 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 8181/8181 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8182/8182 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8183/8183 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8184/8184 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8185/8185 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8186/8186 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8187/8187 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8188/8188 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8189/8189 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8190/8190 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 8191/8191 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8192/8192 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8193/8193 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8194/8194 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8195/8195 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8196/8196 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8197/8197 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8198/8198 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8199/8199 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8200/8200 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 8201/8201 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8202/8202 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8203/8203 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8204/8204 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8205/8205 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8206/8206 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8207/8207 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8208/8208 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8209/8209 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8210/8210 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8211/8211 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8212/8212 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8213/8213 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8214/8214 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8215/8215 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8216/8216 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8217/8217 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8218/8218 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8219/8219 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8220/8220 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8221/8221 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8222/8222 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8223/8223 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8224/8224 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8225/8225 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8226/8226 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8227/8227 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8228/8228 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8229/8229 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8230/8230 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 8231/8231 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8232/8232 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8233/8233 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8234/8234 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8235/8235 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8236/8236 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8237/8237 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8238/8238 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8239/8239 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8240/8240 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 8241/8241 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8242/8242 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8243/8243 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8244/8244 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8245/8245 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8246/8246 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8247/8247 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8248/8248 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8249/8249 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8250/8250 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8251/8251 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8252/8252 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8253/8253 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8254/8254 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8255/8255 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8256/8256 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8257/8257 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8258/8258 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8259/8259 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8260/8260 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8261/8261 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8262/8262 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8263/8263 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8264/8264 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8265/8265 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8266/8266 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8267/8267 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8268/8268 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8269/8269 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8270/8270 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8271/8271 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8272/8272 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8273/8273 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8274/8274 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8275/8275 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8276/8276 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8277/8277 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8278/8278 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8279/8279 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8280/8280 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8281/8281 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8282/8282 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8283/8283 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8284/8284 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8285/8285 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8286/8286 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8287/8287 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8288/8288 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8289/8289 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8290/8290 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8291/8291 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8292/8292 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8293/8293 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8294/8294 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8295/8295 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8296/8296 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8297/8297 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8298/8298 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8299/8299 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8300/8300 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8301/8301 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8302/8302 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8303/8303 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8304/8304 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8305/8305 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8306/8306 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8307/8307 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8308/8308 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8309/8309 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8310/8310 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8311/8311 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8312/8312 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8313/8313 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8314/8314 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8315/8315 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8316/8316 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8317/8317 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8318/8318 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8319/8319 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8320/8320 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8321/8321 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8322/8322 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8323/8323 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8324/8324 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8325/8325 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8326/8326 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8327/8327 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8328/8328 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 8329/8329 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 8330/8330 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 8331/8331 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 8332/8332 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8333/8333 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8334/8334 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8335/8335 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8336/8336 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 8337/8337 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8338/8338 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8339/8339 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 8340/8340 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8341/8341 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8342/8342 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8343/8343 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8344/8344 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8345/8345 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8346/8346 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8347/8347 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8348/8348 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 8349/8349 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8350/8350 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8351/8351 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 8352/8352 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8353/8353 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8354/8354 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 8355/8355 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 8356/8356 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8357/8357 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 8358/8358 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8359/8359 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 8360/8360 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8361/8361 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 8362/8362 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8363/8363 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8364/8364 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 8365/8365 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8366/8366 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 8367/8367 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 8368/8368 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8369/8369 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 8370/8370 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 8371/8371 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8372/8372 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 8373/8373 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8374/8374 [00:00<00:00,  1.25trial/s, best loss: 1.0]\n",
      "100%|██████████| 8375/8375 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 8376/8376 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8377/8377 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 8378/8378 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8379/8379 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8380/8380 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8381/8381 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8382/8382 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8383/8383 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8384/8384 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 8385/8385 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8386/8386 [00:00<00:00,  1.24trial/s, best loss: 1.0]\n",
      "100%|██████████| 8387/8387 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8388/8388 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8389/8389 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8390/8390 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8391/8391 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8392/8392 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8393/8393 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 8394/8394 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8395/8395 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8396/8396 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8397/8397 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8398/8398 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8399/8399 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8400/8400 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8401/8401 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8402/8402 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 8403/8403 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8404/8404 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8405/8405 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8406/8406 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8407/8407 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8408/8408 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8409/8409 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8410/8410 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8411/8411 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 8412/8412 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8413/8413 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8414/8414 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8415/8415 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8416/8416 [00:00<00:00,  1.22trial/s, best loss: 1.0]\n",
      "100%|██████████| 8417/8417 [00:00<00:00,  1.23trial/s, best loss: 1.0]\n",
      "100%|██████████| 8418/8418 [00:00<00:00,  1.21trial/s, best loss: 1.0]\n",
      "100%|██████████| 8419/8419 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8420/8420 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 8421/8421 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8422/8422 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8423/8423 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8424/8424 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8425/8425 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8426/8426 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8427/8427 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8428/8428 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8429/8429 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 8430/8430 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8431/8431 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8432/8432 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8433/8433 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8434/8434 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8435/8435 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8436/8436 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8437/8437 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8438/8438 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 8439/8439 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8440/8440 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8441/8441 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8442/8442 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8443/8443 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8444/8444 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8445/8445 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8446/8446 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8447/8447 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 8448/8448 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8449/8449 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8450/8450 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8451/8451 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8452/8452 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8453/8453 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8454/8454 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8455/8455 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8456/8456 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 8457/8457 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8458/8458 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8459/8459 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8460/8460 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8461/8461 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8462/8462 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8463/8463 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8464/8464 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8465/8465 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 8466/8466 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8467/8467 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8468/8468 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8469/8469 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8470/8470 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8471/8471 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8472/8472 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8473/8473 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8474/8474 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 8475/8475 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8476/8476 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8477/8477 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8478/8478 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8479/8479 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8480/8480 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8481/8481 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8482/8482 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8483/8483 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 8484/8484 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8485/8485 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8486/8486 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8487/8487 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8488/8488 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8489/8489 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8490/8490 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8491/8491 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8492/8492 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 8493/8493 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8494/8494 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8495/8495 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8496/8496 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8497/8497 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8498/8498 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8499/8499 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8500/8500 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8501/8501 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 8502/8502 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8503/8503 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8504/8504 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8505/8505 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8506/8506 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8507/8507 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8508/8508 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8509/8509 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8510/8510 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 8511/8511 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8512/8512 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8513/8513 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8514/8514 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8515/8515 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8516/8516 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8517/8517 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8518/8518 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8519/8519 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 8520/8520 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8521/8521 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8522/8522 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8523/8523 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8524/8524 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8525/8525 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8526/8526 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8527/8527 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8528/8528 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 8529/8529 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8530/8530 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8531/8531 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8532/8532 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8533/8533 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8534/8534 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8535/8535 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8536/8536 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8537/8537 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 8538/8538 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8539/8539 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8540/8540 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8541/8541 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8542/8542 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8543/8543 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8544/8544 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8545/8545 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8546/8546 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8547/8547 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8548/8548 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8549/8549 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8550/8550 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8551/8551 [00:00<00:00,  1.16trial/s, best loss: 1.0]\n",
      "100%|██████████| 8552/8552 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8553/8553 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8554/8554 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8555/8555 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 8556/8556 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 8557/8557 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 8558/8558 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 8559/8559 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 8560/8560 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 8561/8561 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 8562/8562 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 8563/8563 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 8564/8564 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 8565/8565 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 8566/8566 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 8567/8567 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 8568/8568 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 8569/8569 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 8570/8570 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 8571/8571 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 8572/8572 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 8573/8573 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 8574/8574 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8575/8575 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 8576/8576 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 8577/8577 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 8578/8578 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 8579/8579 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 8580/8580 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 8581/8581 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 8582/8582 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 8583/8583 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 8584/8584 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 8585/8585 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 8586/8586 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 8587/8587 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 8588/8588 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 8589/8589 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 8590/8590 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 8591/8591 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 8592/8592 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 8593/8593 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 8594/8594 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 8595/8595 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8596/8596 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 8597/8597 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 8598/8598 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 8599/8599 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 8600/8600 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 8601/8601 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 8602/8602 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 8603/8603 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 8604/8604 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 8605/8605 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 8606/8606 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 8607/8607 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 8608/8608 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 8609/8609 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 8610/8610 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 8611/8611 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 8612/8612 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 8613/8613 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 8614/8614 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 8615/8615 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 8616/8616 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 8617/8617 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 8618/8618 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 8619/8619 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 8620/8620 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 8621/8621 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 8622/8622 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 8623/8623 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 8624/8624 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 8625/8625 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 8626/8626 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 8627/8627 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 8628/8628 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 8629/8629 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 8630/8630 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 8631/8631 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 8632/8632 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 8633/8633 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 8634/8634 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 8635/8635 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 8636/8636 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 8637/8637 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 8638/8638 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 8639/8639 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 8640/8640 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 8641/8641 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 8642/8642 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 8643/8643 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 8644/8644 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 8645/8645 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 8646/8646 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 8647/8647 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 8648/8648 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 8649/8649 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 8650/8650 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 8651/8651 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 8652/8652 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 8653/8653 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 8654/8654 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 8655/8655 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 8656/8656 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 8657/8657 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 8658/8658 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 8659/8659 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 8660/8660 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 8661/8661 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 8662/8662 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 8663/8663 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8664/8664 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 8665/8665 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 8666/8666 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 8667/8667 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 8668/8668 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 8669/8669 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8670/8670 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8671/8671 [00:00<00:00,  1.17trial/s, best loss: 1.0]\n",
      "100%|██████████| 8672/8672 [00:00<00:00,  1.20trial/s, best loss: 1.0]\n",
      "100%|██████████| 8673/8673 [00:00<00:00,  1.18trial/s, best loss: 1.0]\n",
      "100%|██████████| 8674/8674 [00:00<00:00,  1.19trial/s, best loss: 1.0]\n",
      "100%|██████████| 8675/8675 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8676/8676 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8677/8677 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 8678/8678 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8679/8679 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8680/8680 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8681/8681 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8682/8682 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8683/8683 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8684/8684 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8685/8685 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8686/8686 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8687/8687 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 8688/8688 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8689/8689 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8690/8690 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8691/8691 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8692/8692 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8693/8693 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8694/8694 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8695/8695 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8696/8696 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8697/8697 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8698/8698 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8699/8699 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8700/8700 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 8701/8701 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8702/8702 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8703/8703 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8704/8704 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8705/8705 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8706/8706 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 8707/8707 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8708/8708 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8709/8709 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8710/8710 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8711/8711 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8712/8712 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8713/8713 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8714/8714 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8715/8715 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8716/8716 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8717/8717 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8718/8718 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8719/8719 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8720/8720 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8721/8721 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8722/8722 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 8723/8723 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8724/8724 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8725/8725 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8726/8726 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 8727/8727 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8728/8728 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8729/8729 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8730/8730 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8731/8731 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 8732/8732 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8733/8733 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8734/8734 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8735/8735 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 8736/8736 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8737/8737 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8738/8738 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8739/8739 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8740/8740 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8741/8741 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8742/8742 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8743/8743 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8744/8744 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8745/8745 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8746/8746 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8747/8747 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8748/8748 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8749/8749 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8750/8750 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8751/8751 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8752/8752 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8753/8753 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 8754/8754 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8755/8755 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8756/8756 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 8757/8757 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 8758/8758 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8759/8759 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8760/8760 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8761/8761 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8762/8762 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 8763/8763 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8764/8764 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 8765/8765 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8766/8766 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 8767/8767 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8768/8768 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8769/8769 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8770/8770 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8771/8771 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8772/8772 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8773/8773 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8774/8774 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8775/8775 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8776/8776 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 8777/8777 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8778/8778 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8779/8779 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8780/8780 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8781/8781 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8782/8782 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8783/8783 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8784/8784 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8785/8785 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8786/8786 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 8787/8787 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8788/8788 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8789/8789 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8790/8790 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8791/8791 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8792/8792 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8793/8793 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8794/8794 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8795/8795 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8796/8796 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 8797/8797 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8798/8798 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8799/8799 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8800/8800 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8801/8801 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8802/8802 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8803/8803 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8804/8804 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8805/8805 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8806/8806 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8807/8807 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8808/8808 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8809/8809 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8810/8810 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8811/8811 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8812/8812 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8813/8813 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8814/8814 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8815/8815 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8816/8816 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8817/8817 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 8818/8818 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8819/8819 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8820/8820 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8821/8821 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8822/8822 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8823/8823 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8824/8824 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8825/8825 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8826/8826 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8827/8827 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 8828/8828 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 8829/8829 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8830/8830 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8831/8831 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8832/8832 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8833/8833 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8834/8834 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8835/8835 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8836/8836 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8837/8837 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 8838/8838 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8839/8839 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8840/8840 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8841/8841 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8842/8842 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8843/8843 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8844/8844 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8845/8845 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8846/8846 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8847/8847 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8848/8848 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8849/8849 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8850/8850 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8851/8851 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8852/8852 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8853/8853 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8854/8854 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8855/8855 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8856/8856 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8857/8857 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8858/8858 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 8859/8859 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8860/8860 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8861/8861 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8862/8862 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8863/8863 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8864/8864 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8865/8865 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8866/8866 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8867/8867 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8868/8868 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8869/8869 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8870/8870 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8871/8871 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8872/8872 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 8873/8873 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8874/8874 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8875/8875 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8876/8876 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8877/8877 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8878/8878 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8879/8879 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8880/8880 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8881/8881 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8882/8882 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8883/8883 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8884/8884 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8885/8885 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8886/8886 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8887/8887 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 8888/8888 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8889/8889 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 8890/8890 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 8891/8891 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 8892/8892 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 8893/8893 [00:00<00:00,  1.03trial/s, best loss: 1.0]\n",
      "100%|██████████| 8894/8894 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 8895/8895 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 8896/8896 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8897/8897 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 8898/8898 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 8899/8899 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 8900/8900 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 8901/8901 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 8902/8902 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 8903/8903 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 8904/8904 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8905/8905 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 8906/8906 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 8907/8907 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 8908/8908 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 8909/8909 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 8910/8910 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 8911/8911 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8912/8912 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8913/8913 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8914/8914 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8915/8915 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 8916/8916 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 8917/8917 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 8918/8918 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 8919/8919 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 8920/8920 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 8921/8921 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 8922/8922 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 8923/8923 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 8924/8924 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 8925/8925 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 8926/8926 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 8927/8927 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8928/8928 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 8929/8929 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8930/8930 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 8931/8931 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 8932/8932 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8933/8933 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 8934/8934 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 8935/8935 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8936/8936 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8937/8937 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 8938/8938 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 8939/8939 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 8940/8940 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8941/8941 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8942/8942 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8943/8943 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8944/8944 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8945/8945 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 8946/8946 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 8947/8947 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8948/8948 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8949/8949 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8950/8950 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 8951/8951 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 8952/8952 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8953/8953 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 8954/8954 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 8955/8955 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 8956/8956 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 8957/8957 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 8958/8958 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 8959/8959 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 8960/8960 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 8961/8961 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 8962/8962 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 8963/8963 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 8964/8964 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 8965/8965 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 8966/8966 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 8967/8967 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 8968/8968 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 8969/8969 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 8970/8970 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 8971/8971 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 8972/8972 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 8973/8973 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 8974/8974 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 8975/8975 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 8976/8976 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 8977/8977 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 8978/8978 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 8979/8979 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 8980/8980 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 8981/8981 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 8982/8982 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 8983/8983 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 8984/8984 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 8985/8985 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 8986/8986 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 8987/8987 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 8988/8988 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 8989/8989 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 8990/8990 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 8991/8991 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 8992/8992 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 8993/8993 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 8994/8994 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 8995/8995 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 8996/8996 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 8997/8997 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 8998/8998 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 8999/8999 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 9000/9000 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 9001/9001 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 9002/9002 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 9003/9003 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 9004/9004 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 9005/9005 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 9006/9006 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 9007/9007 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 9008/9008 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 9009/9009 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 9010/9010 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 9011/9011 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 9012/9012 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 9013/9013 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 9014/9014 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 9015/9015 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 9016/9016 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 9017/9017 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 9018/9018 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 9019/9019 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9020/9020 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 9021/9021 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 9022/9022 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 9023/9023 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 9024/9024 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 9025/9025 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 9026/9026 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 9027/9027 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 9028/9028 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 9029/9029 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 9030/9030 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 9031/9031 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 9032/9032 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 9033/9033 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 9034/9034 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 9035/9035 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 9036/9036 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 9037/9037 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 9038/9038 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 9039/9039 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 9040/9040 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 9041/9041 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 9042/9042 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 9043/9043 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 9044/9044 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 9045/9045 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 9046/9046 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 9047/9047 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 9048/9048 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 9049/9049 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 9050/9050 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 9051/9051 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 9052/9052 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 9053/9053 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 9054/9054 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 9055/9055 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9056/9056 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 9057/9057 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 9058/9058 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9059/9059 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 9060/9060 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9061/9061 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 9062/9062 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 9063/9063 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 9064/9064 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 9065/9065 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9066/9066 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 9067/9067 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 9068/9068 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9069/9069 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 9070/9070 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 9071/9071 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 9072/9072 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 9073/9073 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9074/9074 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 9075/9075 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 9076/9076 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9077/9077 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 9078/9078 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9079/9079 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9080/9080 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9081/9081 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9082/9082 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 9083/9083 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9084/9084 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 9085/9085 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 9086/9086 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9087/9087 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 9088/9088 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9089/9089 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9090/9090 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 9091/9091 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9092/9092 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 9093/9093 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 9094/9094 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 9095/9095 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 9096/9096 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9097/9097 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 9098/9098 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 9099/9099 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 9100/9100 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9101/9101 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 9102/9102 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 9103/9103 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 9104/9104 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 9105/9105 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 9106/9106 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 9107/9107 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 9108/9108 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 9109/9109 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 9110/9110 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 9111/9111 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 9112/9112 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 9113/9113 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 9114/9114 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 9115/9115 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 9116/9116 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 9117/9117 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 9118/9118 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 9119/9119 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 9120/9120 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 9121/9121 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 9122/9122 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 9123/9123 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9124/9124 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9125/9125 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 9126/9126 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9127/9127 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 9128/9128 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9129/9129 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 9130/9130 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 9131/9131 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9132/9132 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 9133/9133 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9134/9134 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 9135/9135 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 9136/9136 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 9137/9137 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9138/9138 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 9139/9139 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 9140/9140 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9141/9141 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9142/9142 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9143/9143 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9144/9144 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 9145/9145 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9146/9146 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9147/9147 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 9148/9148 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 9149/9149 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 9150/9150 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9151/9151 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9152/9152 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 9153/9153 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9154/9154 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9155/9155 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9156/9156 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9157/9157 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9158/9158 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9159/9159 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9160/9160 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9161/9161 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9162/9162 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 9163/9163 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9164/9164 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9165/9165 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9166/9166 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9167/9167 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9168/9168 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9169/9169 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9170/9170 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9171/9171 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9172/9172 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9173/9173 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 9174/9174 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9175/9175 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9176/9176 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9177/9177 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9178/9178 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9179/9179 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9180/9180 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 9181/9181 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 9182/9182 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 9183/9183 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9184/9184 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9185/9185 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9186/9186 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9187/9187 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9188/9188 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 9189/9189 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9190/9190 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9191/9191 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9192/9192 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9193/9193 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9194/9194 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 9195/9195 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9196/9196 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9197/9197 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9198/9198 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9199/9199 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9200/9200 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9201/9201 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9202/9202 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9203/9203 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9204/9204 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9205/9205 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9206/9206 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9207/9207 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9208/9208 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 9209/9209 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9210/9210 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9211/9211 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9212/9212 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9213/9213 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 9214/9214 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9215/9215 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9216/9216 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 9217/9217 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9218/9218 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 9219/9219 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 9220/9220 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 9221/9221 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 9222/9222 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 9223/9223 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 9224/9224 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 9225/9225 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 9226/9226 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9227/9227 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 9228/9228 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 9229/9229 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 9230/9230 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 9231/9231 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 9232/9232 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 9233/9233 [00:00<00:00,  1.04trial/s, best loss: 1.0]\n",
      "100%|██████████| 9234/9234 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 9235/9235 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 9236/9236 [00:00<00:00,  1.05trial/s, best loss: 1.0]\n",
      "100%|██████████| 9237/9237 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 9238/9238 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 9239/9239 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 9240/9240 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 9241/9241 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 9242/9242 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 9243/9243 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 9244/9244 [00:00<00:00,  1.06trial/s, best loss: 1.0]\n",
      "100%|██████████| 9245/9245 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 9246/9246 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 9247/9247 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 9248/9248 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 9249/9249 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 9250/9250 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9251/9251 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 9252/9252 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9253/9253 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9254/9254 [00:00<00:00,  1.08trial/s, best loss: 1.0]\n",
      "100%|██████████| 9255/9255 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9256/9256 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9257/9257 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9258/9258 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9259/9259 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9260/9260 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9261/9261 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9262/9262 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9263/9263 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9264/9264 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9265/9265 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9266/9266 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9267/9267 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9268/9268 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9269/9269 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 9270/9270 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9271/9271 [00:00<00:00,  1.07trial/s, best loss: 1.0]\n",
      "100%|██████████| 9272/9272 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9273/9273 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9274/9274 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9275/9275 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9276/9276 [00:00<00:00,  1.15trial/s, best loss: 1.0]\n",
      "100%|██████████| 9277/9277 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9278/9278 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9279/9279 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9280/9280 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9281/9281 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9282/9282 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9283/9283 [00:00<00:00,  1.11trial/s, best loss: 1.0]\n",
      "100%|██████████| 9284/9284 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9285/9285 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9286/9286 [00:00<00:00,  1.13trial/s, best loss: 1.0]\n",
      "100%|██████████| 9287/9287 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9288/9288 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9289/9289 [00:00<00:00,  1.14trial/s, best loss: 1.0]\n",
      "100%|██████████| 9290/9290 [00:00<00:00,  1.12trial/s, best loss: 1.0]\n",
      "100%|██████████| 9291/9291 [00:00<00:00,  1.09trial/s, best loss: 1.0]\n",
      "100%|██████████| 9292/9292 [00:00<00:00,  1.10trial/s, best loss: 1.0]\n",
      "100%|██████████| 9293/9293 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9294/9294 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9295/9295 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9296/9296 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9297/9297 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9298/9298 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9299/9299 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9300/9300 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9301/9301 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9302/9302 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9303/9303 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9304/9304 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9305/9305 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9306/9306 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9307/9307 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9308/9308 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9309/9309 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 9310/9310 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9311/9311 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9312/9312 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9313/9313 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9314/9314 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9315/9315 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9316/9316 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9317/9317 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9318/9318 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9319/9319 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 9320/9320 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9321/9321 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9322/9322 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9323/9323 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9324/9324 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9325/9325 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 9326/9326 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9327/9327 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9328/9328 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9329/9329 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9330/9330 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9331/9331 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9332/9332 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9333/9333 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 9334/9334 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9335/9335 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9336/9336 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9337/9337 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9338/9338 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9339/9339 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9340/9340 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9341/9341 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9342/9342 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9343/9343 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9344/9344 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9345/9345 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9346/9346 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9347/9347 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9348/9348 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9349/9349 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9350/9350 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9351/9351 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9352/9352 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9353/9353 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9354/9354 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9355/9355 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9356/9356 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9357/9357 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9358/9358 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9359/9359 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9360/9360 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9361/9361 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9362/9362 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9363/9363 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9364/9364 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9365/9365 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9366/9366 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9367/9367 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9368/9368 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9369/9369 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9370/9370 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 9371/9371 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9372/9372 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9373/9373 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9374/9374 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9375/9375 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9376/9376 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9377/9377 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9378/9378 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9379/9379 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 9380/9380 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 9381/9381 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9382/9382 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9383/9383 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9384/9384 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9385/9385 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9386/9386 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9387/9387 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9388/9388 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9389/9389 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9390/9390 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9391/9391 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9392/9392 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9393/9393 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9394/9394 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9395/9395 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9396/9396 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9397/9397 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9398/9398 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9399/9399 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9400/9400 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 9401/9401 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9402/9402 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 9403/9403 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9404/9404 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9405/9405 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9406/9406 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9407/9407 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9408/9408 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9409/9409 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9410/9410 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 9411/9411 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9412/9412 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9413/9413 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9414/9414 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9415/9415 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9416/9416 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9417/9417 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9418/9418 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9419/9419 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9420/9420 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 9421/9421 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9422/9422 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9423/9423 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9424/9424 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 9425/9425 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9426/9426 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9427/9427 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 9428/9428 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9429/9429 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9430/9430 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 9431/9431 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9432/9432 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9433/9433 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9434/9434 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9435/9435 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9436/9436 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9437/9437 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9438/9438 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9439/9439 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9440/9440 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 9441/9441 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9442/9442 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9443/9443 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9444/9444 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9445/9445 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9446/9446 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9447/9447 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9448/9448 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9449/9449 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9450/9450 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 9451/9451 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9452/9452 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9453/9453 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9454/9454 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9455/9455 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9456/9456 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9457/9457 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9458/9458 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9459/9459 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9460/9460 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 9461/9461 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9462/9462 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9463/9463 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9464/9464 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9465/9465 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9466/9466 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9467/9467 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9468/9468 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9469/9469 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9470/9470 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 9471/9471 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9472/9472 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9473/9473 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9474/9474 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 9475/9475 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9476/9476 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9477/9477 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9478/9478 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9479/9479 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9480/9480 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 9481/9481 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9482/9482 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9483/9483 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9484/9484 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9485/9485 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9486/9486 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9487/9487 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9488/9488 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9489/9489 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 9490/9490 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 9491/9491 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9492/9492 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9493/9493 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9494/9494 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9495/9495 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9496/9496 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9497/9497 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9498/9498 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9499/9499 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9500/9500 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 9501/9501 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9502/9502 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9503/9503 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9504/9504 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9505/9505 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9506/9506 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9507/9507 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9508/9508 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9509/9509 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9510/9510 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 9511/9511 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9512/9512 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9513/9513 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9514/9514 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9515/9515 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9516/9516 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9517/9517 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9518/9518 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9519/9519 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9520/9520 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 9521/9521 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9522/9522 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9523/9523 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9524/9524 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9525/9525 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9526/9526 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9527/9527 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9528/9528 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9529/9529 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9530/9530 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 9531/9531 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9532/9532 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9533/9533 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9534/9534 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9535/9535 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9536/9536 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9537/9537 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9538/9538 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9539/9539 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 9540/9540 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 9541/9541 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9542/9542 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9543/9543 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9544/9544 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9545/9545 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9546/9546 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9547/9547 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9548/9548 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9549/9549 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9550/9550 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 9551/9551 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9552/9552 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9553/9553 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9554/9554 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9555/9555 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9556/9556 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9557/9557 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9558/9558 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9559/9559 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9560/9560 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 9561/9561 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9562/9562 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9563/9563 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9564/9564 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9565/9565 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9566/9566 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9567/9567 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9568/9568 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9569/9569 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 9570/9570 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 9571/9571 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9572/9572 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9573/9573 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9574/9574 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9575/9575 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9576/9576 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9577/9577 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9578/9578 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9579/9579 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9580/9580 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 9581/9581 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9582/9582 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9583/9583 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9584/9584 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9585/9585 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9586/9586 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9587/9587 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9588/9588 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 9589/9589 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 9590/9590 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 9591/9591 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9592/9592 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9593/9593 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9594/9594 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 9595/9595 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 9596/9596 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9597/9597 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 9598/9598 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9599/9599 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 9600/9600 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 9601/9601 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9602/9602 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9603/9603 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9604/9604 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9605/9605 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9606/9606 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9607/9607 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9608/9608 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9609/9609 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9610/9610 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 9611/9611 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9612/9612 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9613/9613 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9614/9614 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9615/9615 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9616/9616 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9617/9617 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9618/9618 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9619/9619 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9620/9620 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 9621/9621 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9622/9622 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9623/9623 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9624/9624 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9625/9625 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9626/9626 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9627/9627 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9628/9628 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9629/9629 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 9630/9630 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 9631/9631 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9632/9632 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9633/9633 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 9634/9634 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9635/9635 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9636/9636 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9637/9637 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9638/9638 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9639/9639 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9640/9640 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 9641/9641 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9642/9642 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9643/9643 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9644/9644 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9645/9645 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9646/9646 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9647/9647 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9648/9648 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9649/9649 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9650/9650 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 9651/9651 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9652/9652 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9653/9653 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9654/9654 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9655/9655 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9656/9656 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 9657/9657 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9658/9658 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9659/9659 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 9660/9660 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 9661/9661 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9662/9662 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9663/9663 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9664/9664 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 9665/9665 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 9666/9666 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 9667/9667 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9668/9668 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9669/9669 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 9670/9670 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 9671/9671 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 9672/9672 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 9673/9673 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9674/9674 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9675/9675 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 9676/9676 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 9677/9677 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 9678/9678 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 9679/9679 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 9680/9680 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 9681/9681 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 9682/9682 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9683/9683 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9684/9684 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 9685/9685 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 9686/9686 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 9687/9687 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 9688/9688 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9689/9689 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9690/9690 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 9691/9691 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 9692/9692 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 9693/9693 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 9694/9694 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 9695/9695 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9696/9696 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9697/9697 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9698/9698 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9699/9699 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 9700/9700 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 9701/9701 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 9702/9702 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 9703/9703 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 9704/9704 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9705/9705 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 9706/9706 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 9707/9707 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9708/9708 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9709/9709 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 9710/9710 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9711/9711 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 9712/9712 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 9713/9713 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 9714/9714 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 9715/9715 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 9716/9716 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 9717/9717 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9718/9718 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9719/9719 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9720/9720 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9721/9721 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 9722/9722 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 9723/9723 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9724/9724 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 9725/9725 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9726/9726 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9727/9727 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 9728/9728 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9729/9729 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9730/9730 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 9731/9731 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 9732/9732 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9733/9733 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9734/9734 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9735/9735 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9736/9736 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9737/9737 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9738/9738 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 9739/9739 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 9740/9740 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9740/9741 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9741/9741 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9741/9742 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9742/9742 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9742/9743 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9743/9743 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9743/9744 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9744/9744 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9744/9745 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9745/9745 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9745/9746 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9746/9746 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9746/9747 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9747/9747 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9747/9748 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9748/9748 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9748/9749 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9749/9749 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9749/9750 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9750/9750 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9750/9751 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9751/9751 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9751/9752 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9752/9752 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9752/9753 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9753/9753 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 9754/9754 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9754/9755 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9755/9755 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 9756/9756 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9756/9757 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9757/9757 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9757/9758 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9758/9758 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9758/9759 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9759/9759 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9759/9760 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9760/9760 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9760/9761 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9761/9761 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 9762/9762 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9762/9763 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9763/9763 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9763/9764 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9764/9764 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9764/9765 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9765/9765 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9765/9766 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9766/9766 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9766/9767 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9767/9767 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9767/9768 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9768/9768 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9768/9769 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9769/9769 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9769/9770 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9770/9770 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9770/9771 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9771/9771 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9771/9772 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9772/9772 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9772/9773 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9773/9773 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9773/9774 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9774/9774 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9774/9775 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9775/9775 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9775/9776 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9776/9776 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9776/9777 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9777/9777 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 9778/9778 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9778/9779 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9779/9779 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9779/9780 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9780/9780 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9780/9781 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9781/9781 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9781/9782 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9782/9782 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9782/9783 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9783/9783 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9783/9784 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9784/9784 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9784/9785 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9785/9785 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 9786/9786 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9786/9787 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9787/9787 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 9788/9788 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9788/9789 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9789/9789 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9789/9790 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9790/9790 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9790/9791 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9791/9791 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9791/9792 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9792/9792 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9792/9793 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9793/9793 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9793/9794 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9794/9794 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9794/9795 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9795/9795 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9795/9796 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9796/9796 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9796/9797 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9797/9797 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 9798/9798 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9798/9799 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9799/9799 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9799/9800 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9800/9800 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9800/9801 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9801/9801 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9801/9802 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9802/9802 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9802/9803 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9803/9803 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9803/9804 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9804/9804 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9804/9805 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9805/9805 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 9806/9806 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9806/9807 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9807/9807 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9807/9808 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9808/9808 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9808/9809 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9809/9809 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 9810/9810 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 9811/9811 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9811/9812 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9812/9812 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9812/9813 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9813/9813 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 9814/9814 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9814/9815 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9815/9815 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9815/9816 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9816/9816 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9816/9817 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9817/9817 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9817/9818 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9818/9818 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 9819/9819 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 9820/9820 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 9821/9821 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9821/9822 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9822/9822 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 9823/9823 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9823/9824 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9824/9824 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9824/9825 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9825/9825 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9825/9826 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9826/9826 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 9827/9827 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9827/9828 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9828/9828 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9828/9829 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9829/9829 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 9830/9830 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 9831/9831 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9831/9832 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9832/9832 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9832/9833 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9833/9833 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9833/9834 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9834/9834 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 9835/9835 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9835/9836 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9836/9836 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9836/9837 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9837/9837 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 9838/9838 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9838/9839 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9839/9839 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9839/9840 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9840/9840 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 9841/9841 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9841/9842 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9842/9842 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9842/9843 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9843/9843 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9843/9844 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9844/9844 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9844/9845 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9845/9845 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 9846/9846 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 9847/9847 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 9848/9848 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9848/9849 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9849/9849 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 9850/9850 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9850/9851 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9851/9851 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9851/9852 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9852/9852 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 9853/9853 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 9854/9854 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9855/9855 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 9856/9856 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 9857/9857 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 9858/9858 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 9859/9859 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9859/9860 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9860/9860 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 9861/9861 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 9862/9862 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 9863/9863 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 9864/9864 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 9865/9865 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 9866/9866 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 9867/9867 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 9868/9868 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 9869/9869 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 9870/9870 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 9871/9871 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 9872/9872 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 9873/9873 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 9874/9874 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 9875/9875 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 9876/9876 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 9877/9877 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 9878/9878 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 9879/9879 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 9880/9880 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 9881/9881 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 9882/9882 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 9883/9883 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 9884/9884 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 9885/9885 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 9886/9886 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 9887/9887 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9888/9888 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 9889/9889 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 9890/9890 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9891/9891 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 9892/9892 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 9893/9893 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9894/9894 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 9895/9895 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9896/9896 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 9897/9897 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9898/9898 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9899/9899 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 9900/9900 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9901/9901 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 9902/9902 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 9903/9903 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9904/9904 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9905/9905 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9906/9906 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9907/9907 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 9908/9908 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 9909/9909 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 9910/9910 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 9911/9911 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9912/9912 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 9913/9913 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 9914/9914 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9915/9915 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9916/9916 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 9917/9917 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 9918/9918 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 9919/9919 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9920/9920 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9921/9921 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9922/9922 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 9923/9923 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 9924/9924 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9925/9925 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9926/9926 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9927/9927 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 9928/9928 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9929/9929 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 9930/9930 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 9931/9931 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 9932/9932 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 9933/9933 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 9934/9934 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9935/9935 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 9936/9936 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9937/9937 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 9938/9938 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9939/9939 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 9940/9940 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 9941/9941 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 9942/9942 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 9943/9943 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9944/9944 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 9945/9945 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 9946/9946 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9947/9947 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9948/9948 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9949/9949 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 9950/9950 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 9951/9951 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 9952/9952 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 9953/9953 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 9954/9954 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 9955/9955 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 9956/9956 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 9957/9957 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 9958/9958 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 9959/9959 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9960/9960 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 9961/9961 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 9962/9962 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 9963/9963 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 9964/9964 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 9965/9965 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 9966/9966 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 9967/9967 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9968/9968 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 9969/9969 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 9970/9970 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 9971/9971 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9972/9972 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 9973/9973 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 9974/9974 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9975/9975 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9976/9976 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 9977/9977 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9978/9978 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 9979/9979 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9980/9980 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9981/9981 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9982/9982 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 9983/9983 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9984/9984 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 9985/9985 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9986/9986 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9987/9987 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 9988/9988 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 9989/9989 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 9990/9990 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 9991/9991 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 9992/9992 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 9993/9993 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 9994/9994 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 9995/9995 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 9996/9996 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9996/9997 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9997/9997 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9997/9998 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9998/9998 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9998/9999 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 9999/10000 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10000/10001 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10001/10001 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10001/10002 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10002/10002 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10002/10003 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10003/10003 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10003/10004 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10004/10004 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10004/10005 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10005/10005 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10005/10006 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10006/10006 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10006/10007 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10007/10007 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10008/10008 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10008/10009 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10009/10009 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10009/10010 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10010/10010 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10010/10011 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10011/10011 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10011/10012 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10012/10012 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10012/10013 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10013/10013 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10013/10014 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10014/10014 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10014/10015 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10015/10015 [00:02<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10015/10016 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10016/10016 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10016/10017 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10017/10017 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10017/10018 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10018/10018 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10018/10019 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10019/10019 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 10020/10020 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10020/10021 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10021/10021 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10021/10022 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10022/10022 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10022/10023 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10023/10023 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10023/10024 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10024/10024 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 10025/10025 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10025/10026 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10026/10026 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10026/10027 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10027/10027 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10027/10028 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10028/10028 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10028/10029 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10029/10029 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10029/10030 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10030/10030 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10030/10031 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10031/10031 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10031/10032 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10032/10032 [00:03<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10032/10033 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10033/10033 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10033/10034 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10034/10034 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10034/10035 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10035/10035 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10035/10036 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10036/10036 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10036/10037 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10037/10037 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10037/10038 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10038/10038 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10038/10039 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10039/10039 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10039/10040 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10040/10040 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10040/10041 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10041/10041 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10041/10042 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10042/10042 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10042/10043 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10043/10043 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10043/10044 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10044/10044 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10044/10045 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10045/10045 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10045/10046 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10046/10046 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10046/10047 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10047/10047 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10047/10048 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10048/10048 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10048/10049 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10049/10049 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10049/10050 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10050/10050 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10050/10051 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10051/10051 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10051/10052 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10052/10052 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10052/10053 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10053/10053 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10053/10054 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10054/10054 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10054/10055 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10055/10055 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10055/10056 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10056/10056 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 10057/10057 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10057/10058 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10058/10058 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10058/10059 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10059/10059 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10059/10060 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10060/10060 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 10061/10061 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10061/10062 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10062/10062 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10062/10063 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10063/10063 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10063/10064 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10064/10064 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 10065/10065 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10065/10066 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10066/10066 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10066/10067 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10067/10067 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10067/10068 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10068/10068 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10068/10069 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10069/10069 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10069/10070 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10070/10070 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10070/10071 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10071/10071 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 10072/10072 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 10073/10073 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10073/10074 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10074/10074 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10074/10075 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10075/10075 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 10076/10076 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10076/10077 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10077/10077 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10077/10078 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10078/10078 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 10079/10079 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 10080/10080 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10080/10081 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10081/10081 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10081/10082 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10082/10082 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10082/10083 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10083/10083 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10083/10084 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10084/10084 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10084/10085 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10085/10085 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10085/10086 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10086/10086 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 10087/10087 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10087/10088 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10088/10088 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10088/10089 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10089/10089 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10089/10090 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10090/10090 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10090/10091 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10091/10091 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10091/10092 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10092/10092 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10092/10093 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10093/10093 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10093/10094 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10094/10094 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 10095/10095 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10095/10096 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10096/10096 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 10097/10097 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10097/10098 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10098/10098 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 10099/10099 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10099/10100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10100/10100 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10100/10101 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10101/10101 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10101/10102 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10102/10102 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10102/10103 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10103/10103 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 10104/10104 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 10105/10105 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10105/10106 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10106/10106 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10106/10107 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10107/10107 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10107/10108 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10108/10108 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10108/10109 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10109/10109 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 10110/10110 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 10111/10111 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10111/10112 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10112/10112 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10112/10113 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10113/10113 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10113/10114 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10114/10114 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10114/10115 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10115/10115 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 10116/10116 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10116/10117 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10117/10117 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 10118/10118 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10118/10119 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10119/10119 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10119/10120 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10120/10120 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10120/10121 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10121/10121 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10121/10122 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10122/10122 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10122/10123 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10123/10123 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10123/10124 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10124/10124 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 10125/10125 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 10126/10126 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 10127/10127 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10127/10128 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10128/10128 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10128/10129 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10129/10129 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10129/10130 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10130/10130 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 10131/10131 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 10132/10132 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10132/10133 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10133/10133 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10133/10134 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10134/10134 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10134/10135 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10135/10135 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10135/10136 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10136/10136 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10136/10137 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10137/10137 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10137/10138 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10138/10138 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10138/10139 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10139/10139 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10139/10140 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10140/10140 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10140/10141 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10141/10141 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10141/10142 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10142/10142 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10142/10143 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10143/10143 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10143/10144 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10144/10144 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10144/10145 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10145/10145 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10145/10146 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10146/10146 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10146/10147 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10147/10147 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10147/10148 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10148/10148 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10148/10149 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10149/10149 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10149/10150 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10150/10150 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10150/10151 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10151/10151 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10151/10152 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10152/10152 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10152/10153 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10153/10153 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10153/10154 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10154/10154 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10154/10155 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10155/10155 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10155/10156 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10156/10156 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10156/10157 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10157/10157 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10157/10158 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10158/10158 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10158/10159 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10159/10159 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10159/10160 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10160/10160 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10160/10161 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10161/10161 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10161/10162 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10162/10162 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 10162/10163 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10163/10163 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 10164/10164 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 10165/10165 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 10166/10166 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 10167/10167 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10168/10168 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 10169/10169 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 10170/10170 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10171/10171 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10172/10172 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 10173/10173 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10174/10174 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10175/10175 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 10176/10176 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10177/10177 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10178/10178 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10179/10179 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 10180/10180 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 10181/10181 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 10182/10182 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10183/10183 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10184/10184 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 10185/10185 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 10186/10186 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 10187/10187 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 10188/10188 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 10189/10189 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 10190/10190 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 10191/10191 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 10192/10192 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 10193/10193 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 10194/10194 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 10195/10195 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 10196/10196 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 10197/10197 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 10198/10198 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 10199/10199 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 10200/10200 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 10201/10201 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 10202/10202 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 10203/10203 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 10204/10204 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10205/10205 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 10206/10206 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 10207/10207 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 10208/10208 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 10209/10209 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 10210/10210 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 10211/10211 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 10212/10212 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 10213/10213 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 10214/10214 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 10215/10215 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 10216/10216 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 10217/10217 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 10218/10218 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 10219/10219 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 10220/10220 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 10221/10221 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 10222/10222 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 10223/10223 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 10224/10224 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 10225/10225 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 10226/10226 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 10227/10227 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 10228/10228 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 10229/10229 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 10230/10230 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 10231/10231 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 10232/10232 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 10233/10233 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 10234/10234 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 10235/10235 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 10236/10236 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 10237/10237 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 10238/10238 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 10239/10239 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 10240/10240 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 10241/10241 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 10242/10242 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 10243/10243 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 10244/10244 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 10245/10245 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 10246/10246 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 10247/10247 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 10248/10248 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 10249/10249 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 10250/10250 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 10251/10251 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 10252/10252 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 10253/10253 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 10254/10254 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 10255/10255 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 10256/10256 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 10257/10257 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 10258/10258 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 10259/10259 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 10260/10260 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 10261/10261 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 10262/10262 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 10263/10263 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 10264/10264 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 10265/10265 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 10266/10266 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 10267/10267 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 10268/10268 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 10269/10269 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 10270/10270 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 10271/10271 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 10272/10272 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 10273/10273 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 10274/10274 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 10275/10275 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10276/10276 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10277/10277 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 10278/10278 [00:02<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10279/10279 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 10280/10280 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10281/10281 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 10282/10282 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 10283/10283 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10284/10284 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 10285/10285 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 10286/10286 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 10287/10287 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 10288/10288 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 10289/10289 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 10290/10290 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 10291/10291 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 10292/10292 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 10293/10293 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 10294/10294 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 10295/10295 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 10296/10296 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 10297/10297 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 10298/10298 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 10299/10299 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 10300/10300 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 10301/10301 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 10302/10302 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 10303/10303 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 10304/10304 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 10305/10305 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 10306/10306 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 10307/10307 [00:03<00:00,  3.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 10308/10308 [00:04<00:00,  4.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 10309/10309 [00:04<00:00,  4.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10310/10310 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 10311/10311 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 10312/10312 [00:04<00:00,  4.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10313/10313 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 10314/10314 [00:04<00:00,  4.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 10315/10315 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 10316/10316 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 10317/10317 [00:04<00:00,  4.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 10318/10318 [00:05<00:00,  5.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 10319/10319 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 10320/10320 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 10321/10321 [00:04<00:00,  4.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 10322/10322 [00:04<00:00,  4.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 10323/10323 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 10324/10324 [00:04<00:00,  4.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 10325/10325 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 10326/10326 [00:04<00:00,  4.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 10327/10327 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 10328/10328 [00:05<00:00,  5.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 10329/10329 [00:04<00:00,  4.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 10330/10330 [00:04<00:00,  4.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 10331/10331 [00:04<00:00,  4.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 10332/10332 [00:04<00:00,  4.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 10333/10333 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 10334/10334 [00:04<00:00,  5.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10335/10335 [00:04<00:00,  4.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 10336/10336 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 10337/10337 [00:04<00:00,  4.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 10338/10338 [00:05<00:00,  5.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 10339/10339 [00:04<00:00,  4.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 10340/10340 [00:05<00:00,  5.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 10341/10341 [00:04<00:00,  5.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10342/10342 [00:05<00:00,  5.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10343/10343 [00:04<00:00,  4.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 10344/10344 [00:05<00:00,  5.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 10345/10345 [00:05<00:00,  5.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 10346/10346 [00:05<00:00,  5.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 10347/10347 [00:05<00:00,  5.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 10348/10348 [00:06<00:00,  6.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 10349/10349 [00:05<00:00,  5.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 10350/10350 [00:05<00:00,  5.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 10351/10351 [00:05<00:00,  5.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 10352/10352 [00:05<00:00,  5.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 10353/10353 [00:05<00:00,  5.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 10354/10354 [00:05<00:00,  5.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 10355/10355 [00:05<00:00,  5.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 10356/10356 [00:05<00:00,  5.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 10357/10357 [00:05<00:00,  5.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 10358/10358 [00:06<00:00,  6.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 10359/10359 [00:05<00:00,  5.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 10360/10360 [00:05<00:00,  5.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 10361/10361 [00:05<00:00,  5.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 10362/10362 [00:06<00:00,  6.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10363/10363 [00:06<00:00,  6.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 10364/10364 [00:06<00:00,  6.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 10365/10365 [00:06<00:00,  6.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 10366/10366 [00:06<00:00,  6.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 10367/10367 [00:06<00:00,  6.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 10368/10368 [00:06<00:00,  6.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 10369/10369 [00:07<00:00,  7.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 10370/10370 [00:06<00:00,  6.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 10371/10371 [00:06<00:00,  6.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 10372/10372 [00:06<00:00,  6.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 10373/10373 [00:06<00:00,  6.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 10374/10374 [00:06<00:00,  6.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 10375/10375 [00:06<00:00,  6.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 10376/10376 [00:05<00:00,  6.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10377/10377 [00:06<00:00,  6.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 10378/10378 [00:06<00:00,  6.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 10379/10379 [00:07<00:00,  7.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10380/10380 [00:06<00:00,  6.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 10381/10381 [00:06<00:00,  6.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 10382/10382 [00:06<00:00,  6.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 10383/10383 [00:06<00:00,  6.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 10384/10384 [00:06<00:00,  6.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 10385/10385 [00:06<00:00,  6.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 10386/10386 [00:06<00:00,  6.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 10387/10387 [00:06<00:00,  6.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 10388/10388 [00:06<00:00,  6.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 10389/10389 [00:06<00:00,  6.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 10390/10390 [00:07<00:00,  7.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 10391/10391 [00:06<00:00,  6.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 10392/10392 [00:06<00:00,  6.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 10393/10393 [00:06<00:00,  6.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10394/10394 [00:05<00:00,  5.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 10395/10395 [00:06<00:00,  6.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10396/10396 [00:06<00:00,  6.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 10397/10397 [00:06<00:00,  6.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 10398/10398 [00:06<00:00,  6.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 10399/10399 [00:06<00:00,  6.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 10400/10400 [00:06<00:00,  6.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 10401/10401 [00:05<00:00,  5.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 10402/10402 [00:05<00:00,  5.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 10403/10403 [00:06<00:00,  6.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 10404/10404 [00:06<00:00,  6.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10405/10405 [00:05<00:00,  5.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 10406/10406 [00:06<00:00,  6.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 10407/10407 [00:06<00:00,  6.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 10408/10408 [00:06<00:00,  6.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 10409/10409 [00:06<00:00,  6.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 10410/10410 [00:06<00:00,  6.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 10411/10411 [00:06<00:00,  6.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 10412/10412 [00:06<00:00,  6.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 10413/10413 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10414/10414 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10415/10415 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10416/10416 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10417/10417 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10418/10418 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10419/10419 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10420/10420 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10421/10421 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10422/10422 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10423/10423 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10424/10424 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10425/10425 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10426/10426 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10427/10427 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10428/10428 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10429/10429 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10430/10430 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10431/10431 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 10432/10432 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10433/10433 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10434/10434 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10435/10435 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10436/10436 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10437/10437 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10438/10438 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10439/10439 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10440/10440 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10441/10441 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 10442/10442 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10443/10443 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10444/10444 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10445/10445 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10446/10446 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10447/10447 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10448/10448 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 10449/10449 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10450/10450 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10451/10451 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10452/10452 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10453/10453 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10454/10454 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10455/10455 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10456/10456 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10457/10457 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10458/10458 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10459/10459 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10460/10460 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10461/10461 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10462/10462 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10463/10463 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10464/10464 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10465/10465 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10466/10466 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10467/10467 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10468/10468 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10469/10469 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10470/10470 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10471/10471 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10472/10472 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10473/10473 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10474/10474 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10475/10475 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10476/10476 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10477/10477 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10478/10478 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10479/10479 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10480/10480 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10481/10481 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10482/10482 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 10483/10483 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10484/10484 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10485/10485 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10486/10486 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10487/10487 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10488/10488 [00:00<00:00,  1.02trial/s, best loss: 1.0]\n",
      "100%|██████████| 10489/10489 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10490/10490 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10491/10491 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10492/10492 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 10493/10493 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10494/10494 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10495/10495 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10496/10496 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10497/10497 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10498/10498 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10499/10499 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10500/10500 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10501/10501 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10502/10502 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 10503/10503 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10504/10504 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10505/10505 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10506/10506 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10507/10507 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10508/10508 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10509/10509 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10510/10510 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10511/10511 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10512/10512 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 10513/10513 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10514/10514 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10515/10515 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10516/10516 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10517/10517 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10518/10518 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10519/10519 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10520/10520 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10521/10521 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10522/10522 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10523/10523 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10524/10524 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10525/10525 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10526/10526 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10527/10527 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10528/10528 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10529/10529 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10530/10530 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10531/10531 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10532/10532 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10533/10533 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 10534/10534 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10535/10535 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10536/10536 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10537/10537 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10538/10538 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10539/10539 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10540/10540 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10541/10541 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10542/10542 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10543/10543 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10544/10544 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10545/10545 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10546/10546 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10547/10547 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10548/10548 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10549/10549 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10550/10550 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10551/10551 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10552/10552 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10553/10553 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10554/10554 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 10555/10555 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10556/10556 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10557/10557 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10558/10558 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10559/10559 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10560/10560 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10561/10561 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 10562/10562 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10563/10563 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10564/10564 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 10565/10565 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10566/10566 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10567/10567 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10568/10568 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10569/10569 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10570/10570 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10571/10571 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10572/10572 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10573/10573 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10574/10574 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10575/10575 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10576/10576 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10577/10577 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10578/10578 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10579/10579 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10580/10580 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10581/10581 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10582/10582 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10583/10583 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10584/10584 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10585/10585 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 10586/10586 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10587/10587 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10588/10588 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10589/10589 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10590/10590 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10591/10591 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10592/10592 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10593/10593 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10594/10594 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10595/10595 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 10596/10596 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10597/10597 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10598/10598 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10599/10599 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10600/10600 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10601/10601 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10602/10602 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10603/10603 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10604/10604 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10605/10605 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 10606/10606 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10607/10607 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10608/10608 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10609/10609 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10610/10610 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10611/10611 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10612/10612 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10613/10613 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10614/10614 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10615/10615 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10616/10616 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10617/10617 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10618/10618 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10619/10619 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10620/10620 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10621/10621 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10622/10622 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10623/10623 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10624/10624 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10625/10625 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10626/10626 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 10627/10627 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10628/10628 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10629/10629 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10630/10630 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10631/10631 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10632/10632 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10633/10633 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10634/10634 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10635/10635 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10636/10636 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 10637/10637 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10638/10638 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10639/10639 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10640/10640 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10641/10641 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10642/10642 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10643/10643 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10644/10644 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10645/10645 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10646/10646 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10647/10647 [00:00<00:00,  1.01trial/s, best loss: 1.0]\n",
      "100%|██████████| 10648/10648 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10649/10649 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10650/10650 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10651/10651 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10652/10652 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10653/10653 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10654/10654 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10655/10655 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10656/10656 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10657/10657 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 10658/10658 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10659/10659 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10660/10660 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10661/10661 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10662/10662 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10663/10663 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10664/10664 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10665/10665 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10666/10666 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10667/10667 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 10668/10668 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10669/10669 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10670/10670 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10671/10671 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10672/10672 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10673/10673 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10674/10674 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10675/10675 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10676/10676 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10677/10677 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10678/10678 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10679/10679 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10680/10680 [00:00<00:00,  1.00trial/s, best loss: 1.0]\n",
      "100%|██████████| 10681/10681 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10682/10682 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10683/10683 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10684/10684 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10685/10685 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10686/10686 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10687/10687 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10688/10688 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 10689/10689 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10690/10690 [00:01<00:00,  1.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 10691/10691 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10692/10692 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10693/10693 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10694/10694 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10695/10695 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10696/10696 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10697/10697 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10698/10698 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 10699/10699 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10700/10700 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10701/10701 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10702/10702 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10703/10703 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10704/10704 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10705/10705 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10706/10706 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10707/10707 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10708/10708 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 10709/10709 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10710/10710 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10711/10711 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10712/10712 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10713/10713 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10714/10714 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10715/10715 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10716/10716 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10717/10717 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10718/10718 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10719/10719 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 10720/10720 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10721/10721 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10722/10722 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10723/10723 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10724/10724 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10725/10725 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10726/10726 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10727/10727 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10728/10728 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10729/10729 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 10730/10730 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10731/10731 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10732/10732 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10733/10733 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10734/10734 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10735/10735 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10736/10736 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10737/10737 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10738/10738 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10739/10739 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10740/10740 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 10741/10741 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10742/10742 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10743/10743 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10744/10744 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10745/10745 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10746/10746 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10747/10747 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10748/10748 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10749/10749 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10750/10750 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 10751/10751 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10752/10752 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10753/10753 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10754/10754 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10755/10755 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10756/10756 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10757/10757 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10758/10758 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10759/10759 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10760/10760 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10761/10761 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10762/10762 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10763/10763 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10764/10764 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10765/10765 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10766/10766 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10767/10767 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10768/10768 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10769/10769 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10770/10770 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10771/10771 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10772/10772 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10773/10773 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10774/10774 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10775/10775 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10776/10776 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10777/10777 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10778/10778 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10779/10779 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10780/10780 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10781/10781 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 10782/10782 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10783/10783 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10784/10784 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10785/10785 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10786/10786 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10787/10787 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10788/10788 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10789/10789 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10790/10790 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10791/10791 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 10792/10792 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10793/10793 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10794/10794 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10795/10795 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10796/10796 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10797/10797 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10798/10798 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10799/10799 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10800/10800 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10801/10801 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10802/10802 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 10803/10803 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10804/10804 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10805/10805 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10806/10806 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10807/10807 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10808/10808 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10809/10809 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10810/10810 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10811/10811 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10812/10812 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 10813/10813 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10814/10814 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10815/10815 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10816/10816 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10817/10817 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10818/10818 [00:01<00:00,  1.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 10819/10819 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10820/10820 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10821/10821 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10822/10822 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 10823/10823 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10824/10824 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10825/10825 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10826/10826 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10827/10827 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10828/10828 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10829/10829 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10830/10830 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10831/10831 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10832/10832 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10833/10833 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10834/10834 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10835/10835 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10836/10836 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10837/10837 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10838/10838 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10839/10839 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10840/10840 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10841/10841 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10842/10842 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10843/10843 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10844/10844 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10845/10845 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10846/10846 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10847/10847 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10848/10848 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10849/10849 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10850/10850 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10851/10851 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10852/10852 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10853/10853 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 10854/10854 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10855/10855 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10856/10856 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10857/10857 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10858/10858 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10859/10859 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10860/10860 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10861/10861 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10862/10862 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10863/10863 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 10864/10864 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10865/10865 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10866/10866 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10867/10867 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10868/10868 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10869/10869 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10870/10870 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10871/10871 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10872/10872 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10873/10873 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 10874/10874 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10875/10875 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10876/10876 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10877/10877 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10878/10878 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10879/10879 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10880/10880 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10881/10881 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10882/10882 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10883/10883 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 10884/10884 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10885/10885 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10886/10886 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10887/10887 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10888/10888 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10889/10889 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10890/10890 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10891/10891 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10892/10892 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10893/10893 [00:01<00:00,  1.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 10894/10894 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10895/10895 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10896/10896 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10897/10897 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10898/10898 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10899/10899 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10900/10900 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10901/10901 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10902/10902 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10903/10903 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10904/10904 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 10905/10905 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10906/10906 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10907/10907 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10908/10908 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10909/10909 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10910/10910 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10911/10911 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10912/10912 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10913/10913 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10914/10914 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 10915/10915 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10916/10916 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10917/10917 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10918/10918 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10919/10919 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10920/10920 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10921/10921 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10922/10922 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10923/10923 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10924/10924 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 10925/10925 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10926/10926 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10927/10927 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10928/10928 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10929/10929 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10930/10930 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10931/10931 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10932/10932 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10933/10933 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10934/10934 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10935/10935 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 10936/10936 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10937/10937 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10938/10938 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10939/10939 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10940/10940 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10941/10941 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10942/10942 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10943/10943 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10944/10944 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10945/10945 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 10946/10946 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10947/10947 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10948/10948 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10949/10949 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10950/10950 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10951/10951 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10952/10952 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10953/10953 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10954/10954 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10955/10955 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 10956/10956 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10957/10957 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10958/10958 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10959/10959 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10960/10960 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10961/10961 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10962/10962 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10963/10963 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10964/10964 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10965/10965 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10966/10966 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10967/10967 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10968/10968 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 10969/10969 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10970/10970 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10971/10971 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10972/10972 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10973/10973 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10974/10974 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10975/10975 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10976/10976 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 10977/10977 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 10978/10978 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10979/10979 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10980/10980 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10981/10981 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10982/10982 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10983/10983 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 10984/10984 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10985/10985 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 10986/10986 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 10987/10987 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10988/10988 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 10989/10989 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10990/10990 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10991/10991 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10992/10992 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10993/10993 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 10994/10994 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 10995/10995 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10996/10996 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 10997/10997 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 10998/10998 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 10999/10999 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11000/11000 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 11001/11001 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11002/11002 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11003/11003 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11004/11004 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11005/11005 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11006/11006 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11007/11007 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 11008/11008 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11009/11009 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11010/11010 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11011/11011 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11012/11012 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11013/11013 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11014/11014 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11015/11015 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11016/11016 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11017/11017 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11018/11018 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 11019/11019 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11020/11020 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11021/11021 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11022/11022 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11023/11023 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11024/11024 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11025/11025 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11026/11026 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11027/11027 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11028/11028 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 11029/11029 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11030/11030 [00:01<00:00,  1.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 11031/11031 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11032/11032 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11033/11033 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11034/11034 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11035/11035 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11036/11036 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11037/11037 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11038/11038 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11039/11039 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 11040/11040 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11041/11041 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11042/11042 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11043/11043 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11044/11044 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11045/11045 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11046/11046 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11047/11047 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11048/11048 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11049/11049 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 11050/11050 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11051/11051 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11052/11052 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11053/11053 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11054/11054 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11055/11055 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 11056/11056 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11057/11057 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11058/11058 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11059/11059 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11060/11060 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 11061/11061 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11062/11062 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11063/11063 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11064/11064 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11065/11065 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11066/11066 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11067/11067 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11068/11068 [00:01<00:00,  1.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11069/11069 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 11070/11070 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 11071/11071 [00:01<00:00,  1.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 11072/11072 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11073/11073 [00:01<00:00,  1.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11074/11074 [00:01<00:00,  1.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11075/11075 [00:01<00:00,  1.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11076/11076 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11077/11077 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 11078/11078 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11079/11079 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11080/11080 [00:01<00:00,  1.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11081/11081 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 11082/11082 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11083/11083 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 11084/11084 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11085/11085 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 11086/11086 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 11087/11087 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11088/11088 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11089/11089 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11090/11090 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11091/11091 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 11092/11092 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11093/11093 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11094/11094 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11095/11095 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11096/11096 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11097/11097 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11098/11098 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11099/11099 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11100/11100 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 11101/11101 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11102/11102 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11103/11103 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11104/11104 [00:01<00:00,  1.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 11105/11105 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11106/11106 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11107/11107 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11108/11108 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 11109/11109 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11110/11110 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11111/11111 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 11112/11112 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 11113/11113 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11114/11114 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 11115/11115 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11116/11116 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11117/11117 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11118/11118 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11119/11119 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11120/11120 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11121/11121 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11122/11122 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11123/11123 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11124/11124 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11125/11125 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11126/11126 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11127/11127 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11128/11128 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11129/11129 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11130/11130 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11131/11131 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11132/11132 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11133/11133 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11134/11134 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11135/11135 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 11136/11136 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11137/11137 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11138/11138 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11139/11139 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11140/11140 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11141/11141 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11142/11142 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11143/11143 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11144/11144 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11145/11145 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 11146/11146 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 11147/11147 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11148/11148 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11149/11149 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11150/11150 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 11151/11151 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11152/11152 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11153/11153 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11154/11154 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11155/11155 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 11156/11156 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11157/11157 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 11158/11158 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11159/11159 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11160/11160 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 11161/11161 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 11162/11162 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11163/11163 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 11164/11164 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 11165/11165 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 11166/11166 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 11167/11167 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 11168/11168 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11169/11169 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 11170/11170 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 11171/11171 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11172/11172 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 11173/11173 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 11174/11174 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 11175/11175 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 11176/11176 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 11177/11177 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 11178/11178 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 11179/11179 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 11180/11180 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 11181/11181 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 11182/11182 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 11183/11183 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 11184/11184 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 11185/11185 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 11186/11186 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 11187/11187 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 11188/11188 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 11189/11189 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 11190/11190 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 11191/11191 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 11192/11192 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 11193/11193 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 11194/11194 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 11195/11195 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 11196/11196 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 11197/11197 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 11198/11198 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 11199/11199 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 11200/11200 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 11201/11201 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 11202/11202 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 11203/11203 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 11204/11204 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 11205/11205 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 11206/11206 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 11207/11207 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 11208/11208 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 11209/11209 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 11210/11210 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 11211/11211 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 11212/11212 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 11213/11213 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 11214/11214 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 11215/11215 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 11216/11216 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 11217/11217 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 11218/11218 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 11219/11219 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 11220/11220 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 11221/11221 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 11222/11222 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 11223/11223 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 11224/11224 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 11225/11225 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 11226/11226 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 11227/11227 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 11228/11228 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 11229/11229 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 11230/11230 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 11231/11231 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 11232/11232 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 11233/11233 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 11234/11234 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 11235/11235 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 11236/11236 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11237/11237 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 11238/11238 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 11239/11239 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 11240/11240 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 11241/11241 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 11242/11242 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 11243/11243 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 11244/11244 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 11245/11245 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 11246/11246 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 11247/11247 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11248/11248 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 11249/11249 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 11250/11250 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 11251/11251 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 11252/11252 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 11253/11253 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 11254/11254 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 11255/11255 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 11256/11256 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 11257/11257 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11258/11258 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 11259/11259 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 11260/11260 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 11261/11261 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 11262/11262 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 11263/11263 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 11264/11264 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 11265/11265 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 11266/11266 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 11267/11267 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11268/11268 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 11269/11269 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 11270/11270 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 11271/11271 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 11272/11272 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 11273/11273 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 11274/11274 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 11275/11275 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 11276/11276 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 11277/11277 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 11278/11278 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11279/11279 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 11280/11280 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 11281/11281 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 11282/11282 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 11283/11283 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 11284/11284 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 11285/11285 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 11286/11286 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 11287/11287 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 11288/11288 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11289/11289 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 11290/11290 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 11291/11291 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 11292/11292 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 11293/11293 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 11294/11294 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 11295/11295 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 11296/11296 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 11297/11297 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 11298/11298 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11299/11299 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 11300/11300 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 11301/11301 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 11302/11302 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 11303/11303 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 11304/11304 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 11305/11305 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 11306/11306 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 11307/11307 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 11308/11308 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 11309/11309 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11310/11310 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 11311/11311 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 11312/11312 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 11313/11313 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 11314/11314 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 11315/11315 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 11316/11316 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 11317/11317 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 11318/11318 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 11319/11319 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11320/11320 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 11321/11321 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 11322/11322 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 11323/11323 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 11324/11324 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 11325/11325 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 11326/11326 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 11327/11327 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 11328/11328 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 11329/11329 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11330/11330 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 11331/11331 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 11332/11332 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 11333/11333 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 11334/11334 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 11335/11335 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 11336/11336 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 11337/11337 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 11338/11338 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 11339/11339 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 11340/11340 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 11341/11341 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 11342/11342 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 11343/11343 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 11344/11344 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 11345/11345 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 11346/11346 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 11347/11347 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 11348/11348 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 11349/11349 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 11350/11350 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 11351/11351 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 11352/11352 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 11353/11353 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 11354/11354 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 11355/11355 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 11356/11356 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 11357/11357 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 11358/11358 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 11359/11359 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 11360/11360 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 11361/11361 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 11362/11362 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 11363/11363 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 11364/11364 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 11365/11365 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 11366/11366 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 11367/11367 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 11368/11368 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 11369/11369 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 11370/11370 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 11371/11371 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 11372/11372 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 11373/11373 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 11374/11374 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 11375/11375 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 11376/11376 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 11377/11377 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 11378/11378 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 11379/11379 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 11380/11380 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 11381/11381 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 11382/11382 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 11383/11383 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 11384/11384 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 11385/11385 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 11386/11386 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 11387/11387 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 11388/11388 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 11389/11389 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 11390/11390 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 11391/11391 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 11392/11392 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 11393/11393 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 11394/11394 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 11395/11395 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 11396/11396 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 11397/11397 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 11398/11398 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 11399/11399 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 11400/11400 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 11401/11401 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 11402/11402 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 11403/11403 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 11404/11404 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 11405/11405 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 11406/11406 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 11407/11407 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 11408/11408 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 11409/11409 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 11410/11410 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 11411/11411 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 11412/11412 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 11413/11413 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 11414/11414 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 11415/11415 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 11416/11416 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 11417/11417 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 11418/11418 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 11419/11419 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 11420/11420 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 11421/11421 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 11422/11422 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 11423/11423 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 11424/11424 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 11425/11425 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 11426/11426 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 11427/11427 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 11428/11428 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 11429/11429 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 11430/11430 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 11431/11431 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 11432/11432 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 11433/11433 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 11434/11434 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 11435/11435 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 11436/11436 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 11437/11437 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 11438/11438 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 11439/11439 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 11440/11440 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 11441/11441 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 11442/11442 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 11443/11443 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 11444/11444 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 11445/11445 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 11446/11446 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 11447/11447 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 11448/11448 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 11449/11449 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 11450/11450 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 11451/11451 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 11452/11452 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11453/11453 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11454/11454 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 11455/11455 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 11456/11456 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 11457/11457 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 11458/11458 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 11459/11459 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11460/11460 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 11461/11461 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 11462/11462 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11463/11463 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11464/11464 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11465/11465 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 11466/11466 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11467/11467 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11468/11468 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11469/11469 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11470/11470 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11471/11471 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11472/11472 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11473/11473 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11474/11474 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 11475/11475 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11476/11476 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 11477/11477 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11478/11478 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11479/11479 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11480/11480 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11481/11481 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11482/11482 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 11483/11483 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11484/11484 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 11485/11485 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 11486/11486 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 11487/11487 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11488/11488 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11489/11489 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 11490/11490 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 11491/11491 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 11492/11492 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 11493/11493 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 11494/11494 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11495/11495 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 11496/11496 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 11497/11497 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 11498/11498 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11499/11499 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11500/11500 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11501/11501 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11502/11502 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11503/11503 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 11504/11504 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11505/11505 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11506/11506 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11507/11507 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 11508/11508 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11509/11509 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11510/11510 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11511/11511 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11512/11512 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11513/11513 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11514/11514 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11515/11515 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11516/11516 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11517/11517 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 11518/11518 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 11519/11519 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 11520/11520 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11521/11521 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11522/11522 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11523/11523 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 11524/11524 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 11525/11525 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11526/11526 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11527/11527 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 11528/11528 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11529/11529 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11530/11530 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 11531/11531 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11532/11532 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11533/11533 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11534/11534 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11535/11535 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 11536/11536 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 11537/11537 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 11538/11538 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 11539/11539 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11540/11540 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 11541/11541 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 11542/11542 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 11543/11543 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11544/11544 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 11545/11545 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 11546/11546 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 11547/11547 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 11548/11548 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 11549/11549 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 11550/11550 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 11551/11551 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 11552/11552 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11553/11553 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11554/11554 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11555/11555 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 11556/11556 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11557/11557 [00:01<00:00,  1.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 11558/11558 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11559/11559 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 11560/11560 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11561/11561 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11562/11562 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11563/11563 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11564/11564 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11565/11565 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11566/11566 [00:01<00:00,  1.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11567/11567 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11568/11568 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11569/11569 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 11570/11570 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11571/11571 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11572/11572 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11573/11573 [00:01<00:00,  1.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 11574/11574 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11575/11575 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11576/11576 [00:01<00:00,  1.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 11577/11577 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11578/11578 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11579/11579 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 11580/11580 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11581/11581 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11582/11582 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11583/11583 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11584/11584 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11585/11585 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11586/11586 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11587/11587 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11588/11588 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11589/11589 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11590/11590 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11591/11591 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11592/11592 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11593/11593 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11594/11594 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11595/11595 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11596/11596 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11597/11597 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11598/11598 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11599/11599 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11600/11600 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 11601/11601 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11602/11602 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11603/11603 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11604/11604 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11605/11605 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11606/11606 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11607/11607 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 11608/11608 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11609/11609 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11610/11610 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 11611/11611 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11612/11612 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11613/11613 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11614/11614 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11615/11615 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11616/11616 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11617/11617 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11618/11618 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11619/11619 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11620/11620 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11621/11621 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 11622/11622 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11623/11623 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11624/11624 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11625/11625 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11626/11626 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11627/11627 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11628/11628 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11629/11629 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11630/11630 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11631/11631 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 11632/11632 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11633/11633 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11634/11634 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11635/11635 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 11636/11636 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 11637/11637 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11638/11638 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 11639/11639 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11640/11640 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11641/11641 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11642/11642 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11643/11643 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11644/11644 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11645/11645 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11646/11646 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11647/11647 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11648/11648 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11649/11649 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11650/11650 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11651/11651 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11652/11652 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 11653/11653 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11654/11654 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11655/11655 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11656/11656 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11657/11657 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11658/11658 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 11659/11659 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11660/11660 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11661/11661 [00:01<00:00,  1.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 11662/11662 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 11663/11663 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 11664/11664 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11665/11665 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11666/11666 [00:01<00:00,  1.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 11667/11667 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11668/11668 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11669/11669 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11670/11670 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11671/11671 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11672/11672 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 11673/11673 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11674/11674 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11675/11675 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11676/11676 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11677/11677 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11678/11678 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11679/11679 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11680/11680 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11681/11681 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11682/11682 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11683/11683 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 11684/11684 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11685/11685 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11686/11686 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11687/11687 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11688/11688 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11689/11689 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 11690/11690 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11691/11691 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11692/11692 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11693/11693 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11694/11694 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 11695/11695 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11696/11696 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11697/11697 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11698/11698 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11699/11699 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11700/11700 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 11701/11701 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11702/11702 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11703/11703 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11704/11704 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11705/11705 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11706/11706 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11707/11707 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11708/11708 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11709/11709 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11710/11710 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 11711/11711 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11712/11712 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11713/11713 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11714/11714 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11715/11715 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11716/11716 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11717/11717 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11718/11718 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11719/11719 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11720/11720 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11721/11721 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11722/11722 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11723/11723 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11724/11724 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11725/11725 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11726/11726 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11727/11727 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11728/11728 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11729/11729 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11730/11730 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11731/11731 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11732/11732 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11733/11733 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11734/11734 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11735/11735 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11736/11736 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 11737/11737 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11738/11738 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11739/11739 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11740/11740 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11741/11741 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11742/11742 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11743/11743 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11744/11744 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11745/11745 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11746/11746 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 11747/11747 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11748/11748 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11749/11749 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11750/11750 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11751/11751 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11752/11752 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11753/11753 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11754/11754 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11755/11755 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11756/11756 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11757/11757 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11758/11758 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11759/11759 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11760/11760 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11761/11761 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11762/11762 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11763/11763 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 11764/11764 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 11765/11765 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11766/11766 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11767/11767 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11768/11768 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11769/11769 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11770/11770 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 11771/11771 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11772/11772 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11773/11773 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11774/11774 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11775/11775 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11776/11776 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11777/11777 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11778/11778 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11779/11779 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11780/11780 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11781/11781 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11782/11782 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11783/11783 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 11784/11784 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11785/11785 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11786/11786 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 11787/11787 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11788/11788 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11789/11789 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11790/11790 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11791/11791 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11792/11792 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 11793/11793 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11794/11794 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11795/11795 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11796/11796 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11797/11797 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11798/11798 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11799/11799 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11800/11800 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11801/11801 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11802/11802 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11803/11803 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11804/11804 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11805/11805 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11806/11806 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 11807/11807 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11808/11808 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 11809/11809 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11810/11810 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11811/11811 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 11812/11812 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11813/11813 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11814/11814 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11815/11815 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11816/11816 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11817/11817 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11818/11818 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11819/11819 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 11820/11820 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11821/11821 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11822/11822 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11823/11823 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11824/11824 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11825/11825 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11826/11826 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11827/11827 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11828/11828 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11829/11829 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11830/11830 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 11831/11831 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11832/11832 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11833/11833 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11834/11834 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11835/11835 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11836/11836 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11837/11837 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11838/11838 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11839/11839 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11840/11840 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 11841/11841 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11842/11842 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11843/11843 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11844/11844 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11845/11845 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11846/11846 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11847/11847 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11848/11848 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11849/11849 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11850/11850 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11851/11851 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11852/11852 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11853/11853 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11854/11854 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11855/11855 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11856/11856 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11857/11857 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11858/11858 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11859/11859 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11860/11860 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11861/11861 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 11862/11862 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11863/11863 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11864/11864 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11865/11865 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11866/11866 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11867/11867 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11868/11868 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11869/11869 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11870/11870 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11871/11871 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 11872/11872 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11873/11873 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11874/11874 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11875/11875 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11876/11876 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 11877/11877 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11878/11878 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11879/11879 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11880/11880 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11881/11881 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 11882/11882 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11883/11883 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11884/11884 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11885/11885 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11886/11886 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11887/11887 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11888/11888 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11889/11889 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11890/11890 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11891/11891 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11892/11892 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 11893/11893 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 11894/11894 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11895/11895 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11896/11896 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11897/11897 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11898/11898 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11899/11899 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11900/11900 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11901/11901 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11902/11902 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 11903/11903 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11904/11904 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11905/11905 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11906/11906 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11907/11907 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11908/11908 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11909/11909 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11910/11910 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11911/11911 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11912/11912 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11913/11913 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 11914/11914 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11915/11915 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11916/11916 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11917/11917 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11918/11918 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11919/11919 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11920/11920 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 11921/11921 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 11922/11922 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11923/11923 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 11924/11924 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11925/11925 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11926/11926 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11927/11927 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11928/11928 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11929/11929 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11930/11930 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11931/11931 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11932/11932 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 11933/11933 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 11934/11934 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 11935/11935 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 11936/11936 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11937/11937 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11938/11938 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11939/11939 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11940/11940 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11941/11941 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11942/11942 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11943/11943 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11944/11944 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 11945/11945 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 11946/11946 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 11947/11947 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 11948/11948 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 11949/11949 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11950/11950 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11951/11951 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11952/11952 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11953/11953 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 11954/11954 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 11955/11955 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 11956/11956 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11957/11957 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 11958/11958 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11959/11959 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11960/11960 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 11961/11961 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 11962/11962 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11963/11963 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11964/11964 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11965/11965 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 11966/11966 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11967/11967 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11968/11968 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11969/11969 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11970/11970 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11971/11971 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11972/11972 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11973/11973 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11974/11974 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11975/11975 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11976/11976 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 11977/11977 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 11978/11978 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11979/11979 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11980/11980 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 11981/11981 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11982/11982 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11983/11983 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 11984/11984 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 11985/11985 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 11986/11986 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 11987/11987 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 11988/11988 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11989/11989 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 11990/11990 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 11991/11991 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11992/11992 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11993/11993 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11994/11994 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 11995/11995 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 11996/11996 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 11997/11997 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 11998/11998 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 11999/11999 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12000/12000 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 12001/12001 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12002/12002 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 12003/12003 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12004/12004 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 12005/12005 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 12006/12006 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12007/12007 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 12008/12008 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12009/12009 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12010/12010 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 12011/12011 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 12012/12012 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12013/12013 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 12014/12014 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12015/12015 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 12016/12016 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12017/12017 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 12018/12018 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 12019/12019 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12020/12020 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12021/12021 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12022/12022 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 12023/12023 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12024/12024 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 12025/12025 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12026/12026 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12027/12027 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 12028/12028 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 12029/12029 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 12030/12030 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 12031/12031 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 12032/12032 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 12033/12033 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 12034/12034 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12035/12035 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12036/12036 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 12037/12037 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 12038/12038 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 12039/12039 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 12040/12040 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12041/12041 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 12042/12042 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12043/12043 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 12044/12044 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12045/12045 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12046/12046 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12047/12047 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12048/12048 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 12049/12049 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 12050/12050 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12051/12051 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 12052/12052 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12053/12053 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12054/12054 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12055/12055 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12056/12056 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 12057/12057 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 12058/12058 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12059/12059 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12060/12060 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 12061/12061 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 12062/12062 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12063/12063 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12064/12064 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12065/12065 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 12066/12066 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12067/12067 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12068/12068 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12069/12069 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12070/12070 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12071/12071 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12072/12072 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12073/12073 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12074/12074 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12075/12075 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 12076/12076 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12077/12077 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12078/12078 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12079/12079 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 12080/12080 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12081/12081 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12082/12082 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12083/12083 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 12084/12084 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12085/12085 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 12086/12086 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12087/12087 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12088/12088 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12089/12089 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12090/12090 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12091/12091 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 12092/12092 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 12093/12093 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12094/12094 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 12095/12095 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12096/12096 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12097/12097 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12098/12098 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12099/12099 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12100/12100 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12101/12101 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12102/12102 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12103/12103 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12104/12104 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 12105/12105 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12106/12106 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 12107/12107 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12108/12108 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12109/12109 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12110/12110 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12111/12111 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12112/12112 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 12113/12113 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 12114/12114 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12115/12115 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12116/12116 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12117/12117 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12118/12118 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12119/12119 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 12120/12120 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 12121/12121 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12122/12122 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12123/12123 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12124/12124 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12125/12125 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12126/12126 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 12127/12127 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12128/12128 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12129/12129 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12130/12130 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12131/12131 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12132/12132 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 12133/12133 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12134/12134 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12135/12135 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 12136/12136 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 12137/12137 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12138/12138 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 12139/12139 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12140/12140 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 12141/12141 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12142/12142 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12143/12143 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12144/12144 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12145/12145 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 12146/12146 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12147/12147 [00:05<00:00,  5.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 12148/12148 [00:05<00:00,  5.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 12149/12149 [00:05<00:00,  5.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 12150/12150 [00:05<00:00,  5.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 12151/12151 [00:05<00:00,  5.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 12152/12152 [00:05<00:00,  5.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 12153/12153 [00:05<00:00,  5.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 12154/12154 [00:05<00:00,  5.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 12155/12155 [00:06<00:00,  6.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 12156/12156 [00:05<00:00,  5.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 12157/12157 [00:05<00:00,  5.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 12158/12158 [00:05<00:00,  5.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 12159/12159 [00:05<00:00,  5.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 12160/12160 [00:06<00:00,  6.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 12161/12161 [00:06<00:00,  6.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 12162/12162 [00:06<00:00,  6.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 12163/12163 [00:05<00:00,  5.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 12164/12164 [00:05<00:00,  5.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 12165/12165 [00:06<00:00,  6.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 12166/12166 [00:05<00:00,  5.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 12167/12167 [00:05<00:00,  5.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 12168/12168 [00:05<00:00,  5.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 12169/12169 [00:05<00:00,  5.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12170/12170 [00:05<00:00,  5.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 12171/12171 [00:05<00:00,  5.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12172/12172 [00:05<00:00,  5.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12173/12173 [00:05<00:00,  5.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 12174/12174 [00:05<00:00,  5.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 12175/12175 [00:05<00:00,  5.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 12176/12176 [00:05<00:00,  5.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 12177/12177 [00:04<00:00,  4.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 12178/12178 [00:05<00:00,  5.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12179/12179 [00:05<00:00,  5.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 12180/12180 [00:05<00:00,  5.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 12181/12181 [00:04<00:00,  4.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 12182/12182 [00:04<00:00,  4.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 12183/12183 [00:04<00:00,  4.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 12184/12184 [00:04<00:00,  4.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 12185/12185 [00:05<00:00,  5.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 12186/12186 [00:05<00:00,  5.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 12187/12187 [00:04<00:00,  4.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 12188/12188 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 12189/12189 [00:04<00:00,  4.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 12190/12190 [00:04<00:00,  4.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 12191/12191 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 12192/12192 [00:04<00:00,  4.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 12193/12193 [00:04<00:00,  4.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 12194/12194 [00:04<00:00,  4.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 12195/12195 [00:04<00:00,  4.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 12196/12196 [00:04<00:00,  4.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 12197/12197 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 12198/12198 [00:04<00:00,  4.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 12199/12199 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12200/12200 [00:04<00:00,  4.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 12201/12201 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 12202/12202 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 12203/12203 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 12204/12204 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 12205/12205 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 12206/12206 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12207/12207 [00:05<00:00,  5.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 12208/12208 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12209/12209 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12210/12210 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 12211/12211 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 12212/12212 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 12213/12213 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 12214/12214 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 12215/12215 [00:04<00:00,  4.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 12216/12216 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 12217/12217 [00:04<00:00,  4.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 12218/12218 [00:04<00:00,  4.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 12219/12219 [00:04<00:00,  4.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 12220/12220 [00:04<00:00,  4.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12221/12221 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 12222/12222 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12223/12223 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 12224/12224 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 12225/12225 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 12226/12226 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 12227/12227 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 12228/12228 [00:04<00:00,  4.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 12229/12229 [00:03<00:00,  3.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 12230/12230 [00:03<00:00,  3.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 12231/12231 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 12232/12232 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 12233/12233 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 12234/12234 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 12235/12235 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 12236/12236 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 12237/12237 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 12238/12238 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12239/12239 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12240/12240 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12241/12241 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12242/12242 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12243/12243 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12244/12244 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12245/12245 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12246/12246 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12247/12247 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12248/12248 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12249/12249 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 12250/12250 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12251/12251 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12252/12252 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12253/12253 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12254/12254 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12255/12255 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12256/12256 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12257/12257 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12258/12258 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12259/12259 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12260/12260 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12261/12261 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12262/12262 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12263/12263 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12264/12264 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12265/12265 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12266/12266 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12267/12267 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12268/12268 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12269/12269 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12270/12270 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12271/12271 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12272/12272 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12273/12273 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12274/12274 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12275/12275 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12276/12276 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12277/12277 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12278/12278 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12279/12279 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12280/12280 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12281/12281 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12282/12282 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12283/12283 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12284/12284 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 12285/12285 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12286/12286 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12287/12287 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12288/12288 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 12289/12289 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12290/12290 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12291/12291 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12292/12292 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12293/12293 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12294/12294 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12295/12295 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12296/12296 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12297/12297 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12298/12298 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12299/12299 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12300/12300 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12301/12301 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 12302/12302 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12303/12303 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12304/12304 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12305/12305 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12306/12306 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12307/12307 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12308/12308 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12309/12309 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12310/12310 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12311/12311 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12312/12312 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12313/12313 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12314/12314 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12315/12315 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12316/12316 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12317/12317 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12318/12318 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12319/12319 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12320/12320 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12321/12321 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12322/12322 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12323/12323 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12324/12324 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12325/12325 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12326/12326 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12327/12327 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12328/12328 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12329/12329 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12330/12330 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12331/12331 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12332/12332 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12333/12333 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12334/12334 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12335/12335 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12336/12336 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12337/12337 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12338/12338 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12339/12339 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12340/12340 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12341/12341 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12342/12342 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12343/12343 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 12344/12344 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12345/12345 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12346/12346 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12347/12347 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12348/12348 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12349/12349 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12350/12350 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12351/12351 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12352/12352 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12353/12353 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 12354/12354 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12355/12355 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12356/12356 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12357/12357 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12358/12358 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12359/12359 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12360/12360 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12361/12361 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12362/12362 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12363/12363 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12364/12364 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12365/12365 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12366/12366 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12367/12367 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12368/12368 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12369/12369 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12370/12370 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 12371/12371 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12372/12372 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12373/12373 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12374/12374 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 12375/12375 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12376/12376 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12377/12377 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12378/12378 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12379/12379 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12380/12380 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12381/12381 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12382/12382 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12383/12383 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12384/12384 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12385/12385 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12386/12386 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12387/12387 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12388/12388 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12389/12389 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12390/12390 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12391/12391 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12392/12392 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12393/12393 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12394/12394 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12395/12395 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 12396/12396 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12397/12397 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12398/12398 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12399/12399 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12400/12400 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12401/12401 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12402/12402 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12403/12403 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12404/12404 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12405/12405 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12406/12406 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 12407/12407 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12408/12408 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12409/12409 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12410/12410 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12411/12411 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12412/12412 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12413/12413 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 12414/12414 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12415/12415 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12416/12416 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 12417/12417 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12418/12418 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12419/12419 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12420/12420 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12421/12421 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 12422/12422 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12423/12423 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 12424/12424 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12425/12425 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12426/12426 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 12427/12427 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12428/12428 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12429/12429 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12430/12430 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12431/12431 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12432/12432 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12433/12433 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12434/12434 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12435/12435 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12436/12436 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12437/12437 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 12438/12438 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12439/12439 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12440/12440 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12441/12441 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12442/12442 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12443/12443 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12444/12444 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 12445/12445 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12446/12446 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 12447/12447 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 12448/12448 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 12449/12449 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12450/12450 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12451/12451 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12452/12452 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12453/12453 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12454/12454 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12455/12455 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 12456/12456 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12457/12457 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12458/12458 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 12459/12459 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12460/12460 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 12461/12461 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12462/12462 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12463/12463 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12464/12464 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12465/12465 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 12466/12466 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12467/12467 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12468/12468 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12469/12469 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 12470/12470 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12471/12471 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 12472/12472 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12473/12473 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 12474/12474 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12475/12475 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12476/12476 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 12477/12477 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 12478/12478 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12479/12479 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 12480/12480 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12481/12481 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12482/12482 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12483/12483 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 12484/12484 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12485/12485 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 12486/12486 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12487/12487 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12488/12488 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12489/12489 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12490/12490 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12491/12491 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12492/12492 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12493/12493 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12494/12494 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12495/12495 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12496/12496 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12497/12497 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12498/12498 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12499/12499 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12500/12500 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12501/12501 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12502/12502 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12503/12503 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12504/12504 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12505/12505 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12506/12506 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12507/12507 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12508/12508 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12509/12509 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12510/12510 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12511/12511 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 12512/12512 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12513/12513 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12514/12514 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12515/12515 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12516/12516 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12517/12517 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12518/12518 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12519/12519 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12520/12520 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12521/12521 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 12522/12522 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12523/12523 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12524/12524 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12525/12525 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12526/12526 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12527/12527 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12528/12528 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12529/12529 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12530/12530 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12531/12531 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12532/12532 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 12533/12533 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12534/12534 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12535/12535 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12536/12536 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12537/12537 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12538/12538 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12539/12539 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12540/12540 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12541/12541 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12542/12542 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 12543/12543 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12544/12544 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12545/12545 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12546/12546 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12547/12547 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12548/12548 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12549/12549 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12550/12550 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12551/12551 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12552/12552 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12553/12553 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12554/12554 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12555/12555 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12556/12556 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12557/12557 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12558/12558 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12559/12559 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12560/12560 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12561/12561 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12562/12562 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12563/12563 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12564/12564 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 12565/12565 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12566/12566 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12567/12567 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12568/12568 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12569/12569 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12570/12570 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12571/12571 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12572/12572 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12573/12573 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12574/12574 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12575/12575 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12576/12576 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12577/12577 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12578/12578 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12579/12579 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12580/12580 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12581/12581 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12582/12582 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12583/12583 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12584/12584 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12585/12585 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12586/12586 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 12587/12587 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12588/12588 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12589/12589 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12590/12590 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12591/12591 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12592/12592 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12593/12593 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12594/12594 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12595/12595 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12596/12596 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 12597/12597 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12598/12598 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12599/12599 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12600/12600 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12601/12601 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12602/12602 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12603/12603 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12604/12604 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12605/12605 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12606/12606 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12607/12607 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12608/12608 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12609/12609 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12610/12610 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12611/12611 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12612/12612 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12613/12613 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12614/12614 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12615/12615 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12616/12616 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12617/12617 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12618/12618 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12619/12619 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12620/12620 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12621/12621 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12622/12622 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12623/12623 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12624/12624 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12625/12625 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12626/12626 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12627/12627 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12628/12628 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 12629/12629 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12630/12630 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12631/12631 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12632/12632 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12633/12633 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12634/12634 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12635/12635 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12636/12636 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12637/12637 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12638/12638 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12639/12639 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 12640/12640 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12641/12641 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12642/12642 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12643/12643 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12644/12644 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12645/12645 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12646/12646 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12647/12647 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12648/12648 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12649/12649 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12650/12650 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12651/12651 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12652/12652 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12653/12653 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12654/12654 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12655/12655 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12656/12656 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12657/12657 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12658/12658 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 12659/12659 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12660/12660 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12661/12661 [00:01<00:00,  1.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 12662/12662 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12663/12663 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12664/12664 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12665/12665 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12666/12666 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12667/12667 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12668/12668 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 12669/12669 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12670/12670 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12671/12671 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12672/12672 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12673/12673 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12674/12674 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12675/12675 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12676/12676 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12677/12677 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12678/12678 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12679/12679 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12680/12680 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12681/12681 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12682/12682 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12683/12683 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12684/12684 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12685/12685 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12686/12686 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12687/12687 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12688/12688 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 12689/12689 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12690/12690 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12691/12691 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12692/12692 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12693/12693 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12694/12694 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12695/12695 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12696/12696 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12697/12697 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12698/12698 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 12699/12699 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 12700/12700 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12701/12701 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 12702/12702 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12703/12703 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 12704/12704 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 12705/12705 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12706/12706 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12707/12707 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 12708/12708 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 12709/12709 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12710/12710 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12711/12711 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 12712/12712 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12713/12713 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12714/12714 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12715/12715 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12716/12716 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12717/12717 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12718/12718 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12719/12719 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12720/12720 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12721/12721 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12722/12722 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12723/12723 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12724/12724 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12725/12725 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12726/12726 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12727/12727 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12728/12728 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 12729/12729 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12730/12730 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12731/12731 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12732/12732 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12733/12733 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12734/12734 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12735/12735 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12736/12736 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12737/12737 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12738/12738 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12739/12739 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12740/12740 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12741/12741 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12742/12742 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12743/12743 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12744/12744 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12745/12745 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12746/12746 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12747/12747 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12748/12748 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 12749/12749 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12750/12750 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12751/12751 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12752/12752 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12753/12753 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12754/12754 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12755/12755 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12756/12756 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12757/12757 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12758/12758 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12759/12759 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12760/12760 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12761/12761 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12762/12762 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12763/12763 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12764/12764 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12765/12765 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12766/12766 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12767/12767 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12768/12768 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 12769/12769 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 12770/12770 [00:01<00:00,  1.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 12771/12771 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12772/12772 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12773/12773 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12774/12774 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12775/12775 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12776/12776 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12777/12777 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12778/12778 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 12779/12779 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12780/12780 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12781/12781 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12782/12782 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12783/12783 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12784/12784 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12785/12785 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12786/12786 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12787/12787 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12788/12788 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 12789/12789 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12790/12790 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12791/12791 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12792/12792 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12793/12793 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12794/12794 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12795/12795 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12796/12796 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12797/12797 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12798/12798 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12799/12799 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12800/12800 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12801/12801 [00:01<00:00,  1.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12802/12802 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12803/12803 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12804/12804 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12805/12805 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12806/12806 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12807/12807 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12808/12808 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 12809/12809 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 12810/12810 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12811/12811 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12812/12812 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12813/12813 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12814/12814 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12815/12815 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12816/12816 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12817/12817 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12818/12818 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 12819/12819 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12820/12820 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12821/12821 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12822/12822 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12823/12823 [00:01<00:00,  1.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12824/12824 [00:01<00:00,  1.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12825/12825 [00:01<00:00,  1.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 12826/12826 [00:01<00:00,  1.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 12827/12827 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 12828/12828 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12829/12829 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 12830/12830 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 12831/12831 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 12832/12832 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 12833/12833 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 12834/12834 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 12835/12835 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 12836/12836 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 12837/12837 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 12838/12838 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12839/12839 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 12840/12840 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 12841/12841 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 12842/12842 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 12843/12843 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 12844/12844 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 12845/12845 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 12846/12846 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 12847/12847 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 12848/12848 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12849/12849 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 12850/12850 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 12851/12851 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 12852/12852 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 12853/12853 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 12854/12854 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 12855/12855 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 12856/12856 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 12857/12857 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 12858/12858 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 12859/12859 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 12860/12860 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 12861/12861 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 12862/12862 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 12863/12863 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 12864/12864 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 12865/12865 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 12866/12866 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 12867/12867 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 12868/12868 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 12869/12869 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 12870/12870 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 12871/12871 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 12872/12872 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 12873/12873 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 12874/12874 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 12875/12875 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 12876/12876 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 12877/12877 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 12878/12878 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 12879/12879 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 12880/12880 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 12881/12881 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 12882/12882 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 12883/12883 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 12884/12884 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 12885/12885 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 12886/12886 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 12887/12887 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 12888/12888 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 12889/12889 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 12890/12890 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 12891/12891 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 12892/12892 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 12893/12893 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 12894/12894 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 12895/12895 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 12896/12896 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 12897/12897 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 12898/12898 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 12899/12899 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 12900/12900 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 12901/12901 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 12902/12902 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 12903/12903 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 12904/12904 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 12905/12905 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 12906/12906 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 12907/12907 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 12908/12908 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 12909/12909 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 12910/12910 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 12911/12911 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 12912/12912 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 12913/12913 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 12914/12914 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 12915/12915 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 12916/12916 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 12917/12917 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 12918/12918 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 12919/12919 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 12920/12920 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 12921/12921 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 12922/12922 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 12923/12923 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 12924/12924 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 12925/12925 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 12926/12926 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 12927/12927 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 12928/12928 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 12929/12929 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 12930/12930 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 12931/12931 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 12932/12932 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 12933/12933 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 12934/12934 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 12935/12935 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 12936/12936 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 12937/12937 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 12938/12938 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 12939/12939 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 12940/12940 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 12941/12941 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 12942/12942 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 12943/12943 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 12944/12944 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 12945/12945 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 12946/12946 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 12947/12947 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 12948/12948 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 12949/12949 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 12950/12950 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12951/12951 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 12952/12952 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 12953/12953 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 12954/12954 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 12955/12955 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12956/12956 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 12957/12957 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12958/12958 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 12959/12959 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 12960/12960 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 12961/12961 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 12962/12962 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 12963/12963 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12964/12964 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 12965/12965 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12966/12966 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 12967/12967 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 12968/12968 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 12969/12969 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 12970/12970 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 12971/12971 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 12972/12972 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12973/12973 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 12974/12974 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 12975/12975 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 12976/12976 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 12977/12977 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 12978/12978 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 12979/12979 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 12980/12980 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 12981/12981 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 12982/12982 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 12983/12983 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 12984/12984 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 12985/12985 [00:03<00:00,  3.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 12986/12986 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 12987/12987 [00:03<00:00,  3.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 12988/12988 [00:03<00:00,  3.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 12989/12989 [00:04<00:00,  4.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 12990/12990 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 12991/12991 [00:04<00:00,  4.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 12992/12992 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 12993/12993 [00:04<00:00,  4.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 12994/12994 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 12995/12995 [00:04<00:00,  4.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 12996/12996 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 12997/12997 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 12998/12998 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 12999/12999 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 13000/13000 [00:01<00:00,  1.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 13001/13001 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 13002/13002 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 13003/13003 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13004/13004 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13005/13005 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13006/13006 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13007/13007 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13008/13008 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13009/13009 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13010/13010 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13011/13011 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 13012/13012 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13013/13013 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13014/13014 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 13015/13015 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13016/13016 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13017/13017 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13018/13018 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 13019/13019 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13020/13020 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13021/13021 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13022/13022 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13023/13023 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13024/13024 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13025/13025 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13026/13026 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13027/13027 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13028/13028 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13029/13029 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13030/13030 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13031/13031 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13032/13032 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13033/13033 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13034/13034 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13035/13035 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 13036/13036 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 13037/13037 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13038/13038 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13039/13039 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13040/13040 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13041/13041 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13042/13042 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13043/13043 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13044/13044 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13045/13045 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13046/13046 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13047/13047 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13048/13048 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 13049/13049 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13050/13050 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13051/13051 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13052/13052 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13053/13053 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13054/13054 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13055/13055 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13056/13056 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 13057/13057 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 13058/13058 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 13059/13059 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13060/13060 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13061/13061 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13062/13062 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13063/13063 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13064/13064 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13065/13065 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13066/13066 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13067/13067 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13068/13068 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13069/13069 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13070/13070 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13071/13071 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13072/13072 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13073/13073 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13074/13074 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13075/13075 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 13076/13076 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 13077/13077 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 13078/13078 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13079/13079 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13080/13080 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13081/13081 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13082/13082 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13083/13083 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 13084/13084 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13085/13085 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13086/13086 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13087/13087 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13088/13088 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13089/13089 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13090/13090 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13091/13091 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13092/13092 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13093/13093 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13094/13094 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13095/13095 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13096/13096 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13097/13097 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13098/13098 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13099/13099 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13100/13100 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13101/13101 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13102/13102 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13103/13103 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13104/13104 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13105/13105 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13106/13106 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13107/13107 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13108/13108 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13109/13109 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 13110/13110 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13111/13111 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13112/13112 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13113/13113 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13114/13114 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13115/13115 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13116/13116 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13117/13117 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13118/13118 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13119/13119 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13120/13120 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13121/13121 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13122/13122 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13123/13123 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13124/13124 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13125/13125 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13126/13126 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13127/13127 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13128/13128 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13129/13129 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13130/13130 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 13131/13131 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13132/13132 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13133/13133 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13134/13134 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13135/13135 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13136/13136 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13137/13137 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13138/13138 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13139/13139 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13140/13140 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 13141/13141 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13142/13142 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13143/13143 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13144/13144 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13145/13145 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13146/13146 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13147/13147 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13148/13148 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13149/13149 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13150/13150 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13151/13151 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 13152/13152 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13153/13153 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13154/13154 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13155/13155 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13156/13156 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13157/13157 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13158/13158 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13159/13159 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13160/13160 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13161/13161 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13162/13162 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13163/13163 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13164/13164 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13165/13165 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13166/13166 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 13167/13167 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 13168/13168 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13169/13169 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13170/13170 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13171/13171 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13172/13172 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 13173/13173 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13174/13174 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 13175/13175 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13176/13176 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13177/13177 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13178/13178 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13179/13179 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13180/13180 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13181/13181 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13182/13182 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13183/13183 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13184/13184 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 13185/13185 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13186/13186 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13187/13187 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13188/13188 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13189/13189 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13190/13190 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13191/13191 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13192/13192 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13193/13193 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 13194/13194 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 13195/13195 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13196/13196 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13197/13197 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13198/13198 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13199/13199 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13200/13200 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13201/13201 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13202/13202 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13203/13203 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13204/13204 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 13205/13205 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13206/13206 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13207/13207 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13208/13208 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13209/13209 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13210/13210 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13211/13211 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13212/13212 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13213/13213 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13214/13214 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13215/13215 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 13216/13216 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13217/13217 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13218/13218 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13219/13219 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13220/13220 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13221/13221 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13222/13222 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13223/13223 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13224/13224 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13225/13225 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13226/13226 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 13227/13227 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13228/13228 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13229/13229 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13230/13230 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13231/13231 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13232/13232 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13233/13233 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13234/13234 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13235/13235 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13236/13236 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13237/13237 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13238/13238 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13239/13239 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13240/13240 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13241/13241 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13242/13242 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13243/13243 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13244/13244 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13245/13245 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13246/13246 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13247/13247 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 13248/13248 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13249/13249 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13250/13250 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13251/13251 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13252/13252 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13253/13253 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13254/13254 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13255/13255 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13256/13256 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13257/13257 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 13258/13258 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13259/13259 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13260/13260 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13261/13261 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13262/13262 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13263/13263 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13264/13264 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13265/13265 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13266/13266 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13267/13267 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13268/13268 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 13269/13269 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13270/13270 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13271/13271 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13272/13272 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13273/13273 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13274/13274 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13275/13275 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13276/13276 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13277/13277 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13278/13278 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 13279/13279 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13280/13280 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13281/13281 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13282/13282 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13283/13283 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13284/13284 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13285/13285 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13286/13286 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13287/13287 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13288/13288 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13289/13289 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 13290/13290 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13291/13291 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13292/13292 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13293/13293 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13294/13294 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13295/13295 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13296/13296 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13297/13297 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 13298/13298 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13299/13299 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13300/13300 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 13301/13301 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13302/13302 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13303/13303 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 13304/13304 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13305/13305 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13306/13306 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13307/13307 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13308/13308 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13309/13309 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13310/13310 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 13311/13311 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13312/13312 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13313/13313 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13314/13314 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13315/13315 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13316/13316 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13317/13317 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13318/13318 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13319/13319 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13320/13320 [00:01<00:00,  1.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 13321/13321 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 13322/13322 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13323/13323 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13324/13324 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13325/13325 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13326/13326 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13327/13327 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13328/13328 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13329/13329 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13330/13330 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13331/13331 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13332/13332 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13333/13333 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13334/13334 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13335/13335 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13336/13336 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13337/13337 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13338/13338 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13339/13339 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13340/13340 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13341/13341 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13342/13342 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13343/13343 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13344/13344 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13345/13345 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13346/13346 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13347/13347 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13348/13348 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13349/13349 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13350/13350 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13351/13351 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13352/13352 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13353/13353 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13354/13354 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13355/13355 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13356/13356 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13357/13357 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13358/13358 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13359/13359 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13360/13360 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13361/13361 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13362/13362 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13363/13363 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13364/13364 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13365/13365 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13366/13366 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13367/13367 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13368/13368 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13369/13369 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13370/13370 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13371/13371 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13372/13372 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13373/13373 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13374/13374 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13375/13375 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13376/13376 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13377/13377 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13378/13378 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13379/13379 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 13380/13380 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 13381/13381 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 13382/13382 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13383/13383 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 13384/13384 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13385/13385 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13386/13386 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 13387/13387 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13388/13388 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13389/13389 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13390/13390 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13391/13391 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13392/13392 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13393/13393 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13394/13394 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13395/13395 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13396/13396 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13397/13397 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13398/13398 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13399/13399 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 13400/13400 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13401/13401 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13402/13402 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13403/13403 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13404/13404 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13405/13405 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13406/13406 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13407/13407 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13408/13408 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13409/13409 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13410/13410 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13411/13411 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13412/13412 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13413/13413 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 13414/13414 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13415/13415 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13416/13416 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13417/13417 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 13418/13418 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13419/13419 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 13420/13420 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13421/13421 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13422/13422 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13423/13423 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13424/13424 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13425/13425 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13426/13426 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13427/13427 [00:04<00:00,  4.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 13428/13428 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 13429/13429 [00:04<00:00,  4.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 13430/13430 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13431/13431 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 13432/13432 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 13433/13433 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 13434/13434 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 13435/13435 [00:04<00:00,  4.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 13436/13436 [00:04<00:00,  4.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 13437/13437 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 13438/13438 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 13439/13439 [00:04<00:00,  4.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 13440/13440 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 13441/13441 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 13442/13442 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 13443/13443 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 13444/13444 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 13445/13445 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 13446/13446 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 13447/13447 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 13448/13448 [00:03<00:00,  3.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 13449/13449 [00:03<00:00,  3.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 13450/13450 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 13451/13451 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 13452/13452 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 13453/13453 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 13454/13454 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 13455/13455 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 13456/13456 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 13457/13457 [00:04<00:00,  4.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 13458/13458 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 13459/13459 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 13460/13460 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 13461/13461 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 13462/13462 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 13463/13463 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 13464/13464 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 13465/13465 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 13466/13466 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 13467/13467 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 13468/13468 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 13469/13469 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 13470/13470 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 13471/13471 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13472/13472 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13473/13473 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 13474/13474 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13475/13475 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 13476/13476 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13477/13477 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13478/13478 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13479/13479 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13480/13480 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13481/13481 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13482/13482 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13483/13483 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13484/13484 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13485/13485 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13486/13486 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13487/13487 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13488/13488 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13489/13489 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 13490/13490 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13491/13491 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13492/13492 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13493/13493 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13494/13494 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13495/13495 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13496/13496 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 13497/13497 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 13498/13498 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13499/13499 [00:04<00:00,  4.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 13500/13500 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 13501/13501 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 13502/13502 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 13503/13503 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 13504/13504 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 13505/13505 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 13506/13506 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 13507/13507 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 13508/13508 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 13509/13509 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 13510/13510 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 13511/13511 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 13512/13512 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 13513/13513 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 13514/13514 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 13515/13515 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 13516/13516 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 13517/13517 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 13518/13518 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 13519/13519 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 13520/13520 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 13521/13521 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 13522/13522 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 13523/13523 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 13524/13524 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 13525/13525 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 13526/13526 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 13527/13527 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 13528/13528 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 13529/13529 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 13530/13530 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 13531/13531 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 13532/13532 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 13533/13533 [00:02<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 13534/13534 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 13535/13535 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 13536/13536 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 13537/13537 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 13538/13538 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 13539/13539 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 13540/13540 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 13541/13541 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 13542/13542 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 13543/13543 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 13544/13544 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 13545/13545 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 13546/13546 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 13547/13547 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 13548/13548 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 13549/13549 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 13550/13550 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 13551/13551 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 13552/13552 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 13553/13553 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 13554/13554 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 13555/13555 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 13556/13556 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 13557/13557 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 13558/13558 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 13559/13559 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 13560/13560 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 13561/13561 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 13562/13562 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 13563/13563 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 13564/13564 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 13565/13565 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 13566/13566 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 13567/13567 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 13568/13568 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 13569/13569 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 13570/13570 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 13571/13571 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 13572/13572 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 13573/13573 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 13574/13574 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 13575/13575 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 13576/13576 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 13577/13577 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 13578/13578 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 13579/13579 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 13580/13580 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 13581/13581 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 13582/13582 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 13583/13583 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 13584/13584 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 13585/13585 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 13586/13586 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 13587/13587 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 13588/13588 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 13589/13589 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 13590/13590 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 13591/13591 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 13592/13592 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 13593/13593 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 13594/13594 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 13595/13595 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 13596/13596 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 13597/13597 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 13598/13598 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 13599/13599 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 13600/13600 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 13601/13601 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 13602/13602 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 13603/13603 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 13604/13604 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 13605/13605 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13606/13606 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 13607/13607 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 13608/13608 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 13609/13609 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 13610/13610 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 13611/13611 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 13612/13612 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 13613/13613 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 13614/13614 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 13615/13615 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 13616/13616 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 13617/13617 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 13618/13618 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 13619/13619 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 13620/13620 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 13621/13621 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 13622/13622 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 13623/13623 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 13624/13624 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 13625/13625 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 13626/13626 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 13627/13627 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 13628/13628 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 13629/13629 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 13630/13630 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 13631/13631 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 13632/13632 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 13633/13633 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 13634/13634 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 13635/13635 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 13636/13636 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 13637/13637 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 13638/13638 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 13639/13639 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 13640/13640 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 13641/13641 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 13642/13642 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 13643/13643 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 13644/13644 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 13645/13645 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 13646/13646 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 13647/13647 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 13648/13648 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 13649/13649 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 13650/13650 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 13651/13651 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 13652/13652 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 13653/13653 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 13654/13654 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 13655/13655 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 13656/13656 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 13657/13657 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 13658/13658 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 13659/13659 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 13660/13660 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 13661/13661 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 13662/13662 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 13663/13663 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 13664/13664 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 13665/13665 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 13666/13666 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 13667/13667 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 13668/13668 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 13669/13669 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 13670/13670 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 13671/13671 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13672/13672 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13673/13673 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13674/13674 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 13675/13675 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13676/13676 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13677/13677 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13678/13678 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13679/13679 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 13680/13680 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13681/13681 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13682/13682 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13683/13683 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13684/13684 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13685/13685 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13686/13686 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13687/13687 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13688/13688 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13689/13689 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13690/13690 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13691/13691 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13692/13692 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13693/13693 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13694/13694 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13695/13695 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13696/13696 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13697/13697 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13698/13698 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13699/13699 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13700/13700 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 13701/13701 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13702/13702 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13703/13703 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13704/13704 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13705/13705 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13706/13706 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13707/13707 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13708/13708 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13709/13709 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13710/13710 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13711/13711 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13712/13712 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13713/13713 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13714/13714 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13715/13715 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13716/13716 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13717/13717 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13718/13718 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13719/13719 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13720/13720 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13721/13721 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 13722/13722 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13723/13723 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13724/13724 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13725/13725 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13726/13726 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13727/13727 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13728/13728 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13729/13729 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13730/13730 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13731/13731 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13732/13732 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 13733/13733 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13734/13734 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13735/13735 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13736/13736 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13737/13737 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13738/13738 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13739/13739 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13740/13740 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13741/13741 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13742/13742 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13743/13743 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 13744/13744 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13745/13745 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13746/13746 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13747/13747 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13748/13748 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13749/13749 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13750/13750 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13751/13751 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13752/13752 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13753/13753 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13754/13754 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13755/13755 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13756/13756 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13757/13757 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13758/13758 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13759/13759 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13760/13760 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13761/13761 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13762/13762 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13763/13763 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13764/13764 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 13765/13765 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13766/13766 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13767/13767 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13768/13768 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13769/13769 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13770/13770 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13771/13771 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13772/13772 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13773/13773 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13774/13774 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13775/13775 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13776/13776 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13777/13777 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13778/13778 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13779/13779 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13780/13780 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13781/13781 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13782/13782 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13783/13783 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13784/13784 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13785/13785 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 13786/13786 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13787/13787 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13788/13788 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13789/13789 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13790/13790 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13791/13791 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13792/13792 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13793/13793 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13794/13794 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13795/13795 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13796/13796 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 13797/13797 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13798/13798 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13799/13799 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13800/13800 [00:01<00:00,  1.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13801/13801 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13802/13802 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13803/13803 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13804/13804 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13805/13805 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13806/13806 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 13807/13807 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13808/13808 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13809/13809 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13810/13810 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13811/13811 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13812/13812 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13813/13813 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13814/13814 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13815/13815 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13816/13816 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13817/13817 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13818/13818 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13819/13819 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13820/13820 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13821/13821 [00:01<00:00,  1.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 13822/13822 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13823/13823 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13824/13824 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13825/13825 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13826/13826 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13827/13827 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 13828/13828 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13829/13829 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13830/13830 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13831/13831 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13832/13832 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13833/13833 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13834/13834 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13835/13835 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13836/13836 [00:01<00:00,  1.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 13837/13837 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13838/13838 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 13839/13839 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13840/13840 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13841/13841 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13842/13842 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13843/13843 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13844/13844 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13845/13845 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13846/13846 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13847/13847 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13848/13848 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13849/13849 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 13850/13850 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13851/13851 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13852/13852 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13853/13853 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13854/13854 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13855/13855 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13856/13856 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13857/13857 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13858/13858 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13859/13859 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13860/13860 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 13861/13861 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13862/13862 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13863/13863 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13864/13864 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13865/13865 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13866/13866 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13867/13867 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13868/13868 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13869/13869 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13870/13870 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13871/13871 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 13872/13872 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13873/13873 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13874/13874 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13875/13875 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13876/13876 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13877/13877 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13878/13878 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13879/13879 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13880/13880 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13881/13881 [00:01<00:00,  1.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13882/13882 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 13883/13883 [00:01<00:00,  1.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 13884/13884 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13885/13885 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13886/13886 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13887/13887 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13888/13888 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13889/13889 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13890/13890 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 13891/13891 [00:01<00:00,  1.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 13892/13892 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13893/13893 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 13894/13894 [00:01<00:00,  1.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 13895/13895 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13896/13896 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13897/13897 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13898/13898 [00:01<00:00,  1.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 13899/13899 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13900/13900 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13901/13901 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13902/13902 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13903/13903 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 13904/13904 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13905/13905 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13906/13906 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13907/13907 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13908/13908 [00:01<00:00,  1.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 13909/13909 [00:01<00:00,  1.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13910/13910 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13911/13911 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13912/13912 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13913/13913 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13914/13914 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 13915/13915 [00:01<00:00,  1.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 13916/13916 [00:01<00:00,  1.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13917/13917 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13918/13918 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13919/13919 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13920/13920 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13921/13921 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13922/13922 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13923/13923 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13924/13924 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13925/13925 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 13926/13926 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13927/13927 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13928/13928 [00:01<00:00,  1.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 13929/13929 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13930/13930 [00:01<00:00,  1.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13931/13931 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13932/13932 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13933/13933 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 13934/13934 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13935/13935 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 13936/13936 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13937/13937 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13938/13938 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13939/13939 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13940/13940 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 13941/13941 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13942/13942 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13943/13943 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 13944/13944 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 13945/13945 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13946/13946 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13947/13947 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 13948/13948 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13949/13949 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13950/13950 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13951/13951 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13952/13952 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 13953/13953 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13954/13954 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13955/13955 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13956/13956 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13957/13957 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13958/13958 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13959/13959 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13960/13960 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13961/13961 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 13962/13962 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13963/13963 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13964/13964 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13965/13965 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 13966/13966 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13967/13967 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 13968/13968 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 13969/13969 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13970/13970 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13971/13971 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 13972/13972 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13973/13973 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13974/13974 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13975/13975 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13976/13976 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13977/13977 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 13978/13978 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13979/13979 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13980/13980 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13981/13981 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13982/13982 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 13983/13983 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 13984/13984 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13985/13985 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13986/13986 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13987/13987 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13988/13988 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 13989/13989 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 13990/13990 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13991/13991 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13992/13992 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13993/13993 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 13994/13994 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13995/13995 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 13996/13996 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 13997/13997 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 13998/13998 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 13999/13999 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14000/14000 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14001/14001 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 14002/14002 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14003/14003 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14004/14004 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14005/14005 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14006/14006 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14007/14007 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14008/14008 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14009/14009 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 14010/14010 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14011/14011 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14012/14012 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14013/14013 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 14014/14014 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14015/14015 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14016/14016 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14017/14017 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14018/14018 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14019/14019 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 14020/14020 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14021/14021 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14022/14022 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14023/14023 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14024/14024 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14025/14025 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 14026/14026 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14027/14027 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14028/14028 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14029/14029 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14030/14030 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 14031/14031 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 14032/14032 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14033/14033 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14034/14034 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14035/14035 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14036/14036 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14037/14037 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14038/14038 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14039/14039 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14040/14040 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14041/14041 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14042/14042 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14043/14043 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14044/14044 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14045/14045 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14046/14046 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14047/14047 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14048/14048 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14049/14049 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 14050/14050 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 14051/14051 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 14052/14052 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14053/14053 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 14054/14054 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14055/14055 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14056/14056 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 14057/14057 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14058/14058 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14059/14059 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14060/14060 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14061/14061 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 14062/14062 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14063/14063 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14064/14064 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14065/14065 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 14066/14066 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14067/14067 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14068/14068 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14069/14069 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 14070/14070 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14071/14071 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 14072/14072 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14073/14073 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14074/14074 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14075/14075 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14076/14076 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14077/14077 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14078/14078 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14079/14079 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14080/14080 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 14081/14081 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14082/14082 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 14083/14083 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14084/14084 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14085/14085 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14086/14086 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14087/14087 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14088/14088 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14089/14089 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14090/14090 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14091/14091 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14092/14092 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14093/14093 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 14094/14094 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14095/14095 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14096/14096 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14097/14097 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14098/14098 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14099/14099 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14100/14100 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14101/14101 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14102/14102 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14103/14103 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14104/14104 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 14105/14105 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14106/14106 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14107/14107 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 14108/14108 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14109/14109 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14110/14110 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14111/14111 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14112/14112 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14113/14113 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14114/14114 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14115/14115 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 14116/14116 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14117/14117 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14118/14118 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14119/14119 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 14120/14120 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14121/14121 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14122/14122 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14123/14123 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14124/14124 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14125/14125 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14126/14126 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 14127/14127 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14128/14128 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14129/14129 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14130/14130 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14131/14131 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14132/14132 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14133/14133 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14134/14134 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14135/14135 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 14136/14136 [00:01<00:00,  1.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 14137/14137 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 14138/14138 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14139/14139 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14140/14140 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14141/14141 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14142/14142 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14143/14143 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14144/14144 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14145/14145 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14146/14146 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14147/14147 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14148/14148 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 14149/14149 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14150/14150 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14151/14151 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14152/14152 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14153/14153 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14154/14154 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14155/14155 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14156/14156 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 14157/14157 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14158/14158 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14159/14159 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 14160/14160 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14161/14161 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14162/14162 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14163/14163 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14164/14164 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14165/14165 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14166/14166 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14167/14167 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14168/14168 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14169/14169 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14170/14170 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 14171/14171 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 14172/14172 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14173/14173 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14174/14174 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14175/14175 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14176/14176 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14177/14177 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14178/14178 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14179/14179 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14180/14180 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14181/14181 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 14182/14182 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14183/14183 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14184/14184 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14185/14185 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14186/14186 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14187/14187 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14188/14188 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14189/14189 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14190/14190 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14191/14191 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 14192/14192 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14193/14193 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 14194/14194 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 14195/14195 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 14196/14196 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14197/14197 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 14198/14198 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14199/14199 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 14200/14200 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 14201/14201 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14202/14202 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 14203/14203 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 14204/14204 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14205/14205 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14206/14206 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14207/14207 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14208/14208 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14209/14209 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14210/14210 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14211/14211 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14212/14212 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 14213/14213 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14214/14214 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14215/14215 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14216/14216 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14217/14217 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 14218/14218 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 14219/14219 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14220/14220 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 14221/14221 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14222/14222 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14223/14223 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 14224/14224 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 14225/14225 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 14226/14226 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14227/14227 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 14228/14228 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14229/14229 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14230/14230 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14231/14231 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14232/14232 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14233/14233 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14234/14234 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 14235/14235 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14236/14236 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14237/14237 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14238/14238 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14239/14239 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14240/14240 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 14241/14241 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14242/14242 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 14243/14243 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14244/14244 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 14245/14245 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 14246/14246 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 14247/14247 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14248/14248 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14249/14249 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14250/14250 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 14251/14251 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 14252/14252 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14253/14253 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 14254/14254 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14255/14255 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14256/14256 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14257/14257 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 14258/14258 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 14259/14259 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14260/14260 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14261/14261 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 14262/14262 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14263/14263 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14264/14264 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14265/14265 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14266/14266 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14267/14267 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14268/14268 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14269/14269 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14270/14270 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 14271/14271 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14272/14272 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14273/14273 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14274/14274 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14275/14275 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14276/14276 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 14277/14277 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14278/14278 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14279/14279 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14280/14280 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14281/14281 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14282/14282 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14283/14283 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14284/14284 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14285/14285 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14286/14286 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14287/14287 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14288/14288 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 14289/14289 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14290/14290 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14291/14291 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14292/14292 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 14293/14293 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14294/14294 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 14295/14295 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14296/14296 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14297/14297 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14298/14298 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 14299/14299 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14300/14300 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14301/14301 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14302/14302 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14303/14303 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14304/14304 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14305/14305 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14306/14306 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14307/14307 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14308/14308 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14309/14309 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14310/14310 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14311/14311 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14312/14312 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14313/14313 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14314/14314 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14315/14315 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14316/14316 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14317/14317 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14318/14318 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14319/14319 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 14320/14320 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14321/14321 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14322/14322 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14323/14323 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14324/14324 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14325/14325 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14326/14326 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14327/14327 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14328/14328 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14329/14329 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 14330/14330 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14331/14331 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14332/14332 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14333/14333 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14334/14334 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14335/14335 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14336/14336 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14337/14337 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14338/14338 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14339/14339 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14340/14340 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 14341/14341 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14342/14342 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14343/14343 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14344/14344 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14345/14345 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14346/14346 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14347/14347 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 14348/14348 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14349/14349 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14350/14350 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14351/14351 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14352/14352 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 14353/14353 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14354/14354 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14355/14355 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 14356/14356 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14357/14357 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14358/14358 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 14359/14359 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14360/14360 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14361/14361 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 14362/14362 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14363/14363 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14364/14364 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14365/14365 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14366/14366 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14367/14367 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14368/14368 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14369/14369 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14370/14370 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14371/14371 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14372/14372 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14373/14373 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14374/14374 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14375/14375 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14376/14376 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14377/14377 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14378/14378 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14379/14379 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14380/14380 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14381/14381 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14382/14382 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 14383/14383 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14384/14384 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14385/14385 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14386/14386 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14387/14387 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14388/14388 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14389/14389 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14390/14390 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14391/14391 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14392/14392 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14393/14393 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 14394/14394 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14395/14395 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14396/14396 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14397/14397 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14398/14398 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14399/14399 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14400/14400 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14401/14401 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14402/14402 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14403/14403 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 14404/14404 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14405/14405 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14406/14406 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14407/14407 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14408/14408 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14409/14409 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14410/14410 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14411/14411 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14412/14412 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14413/14413 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14414/14414 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 14415/14415 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14416/14416 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14417/14417 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14418/14418 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14419/14419 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14420/14420 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14421/14421 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14422/14422 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14423/14423 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14424/14424 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14425/14425 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 14426/14426 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14427/14427 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14428/14428 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14429/14429 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14430/14430 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14431/14431 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14432/14432 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14433/14433 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14434/14434 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14435/14435 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14436/14436 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14437/14437 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14438/14438 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14439/14439 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14440/14440 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14441/14441 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14442/14442 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 14443/14443 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14444/14444 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14445/14445 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14446/14446 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 14447/14447 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14448/14448 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14449/14449 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14450/14450 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14451/14451 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14452/14452 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14453/14453 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14454/14454 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14455/14455 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14456/14456 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14457/14457 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 14458/14458 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14459/14459 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14460/14460 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14461/14461 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14462/14462 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14463/14463 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14464/14464 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14465/14465 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14466/14466 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14467/14467 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 14468/14468 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14469/14469 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14470/14470 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14471/14471 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14472/14472 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14473/14473 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14474/14474 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14475/14475 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14476/14476 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14477/14477 [00:01<00:00,  1.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 14478/14478 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 14479/14479 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14480/14480 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14481/14481 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14482/14482 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14483/14483 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 14484/14484 [00:01<00:00,  1.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 14485/14485 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14486/14486 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14487/14487 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14488/14488 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14489/14489 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 14490/14490 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14491/14491 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 14492/14492 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 14493/14493 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 14494/14494 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14495/14495 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14496/14496 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 14497/14497 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 14498/14498 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14499/14499 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 14500/14500 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 14501/14501 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 14502/14502 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 14503/14503 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 14504/14504 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 14505/14505 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 14506/14506 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 14507/14507 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 14508/14508 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 14509/14509 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 14510/14510 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14511/14511 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 14512/14512 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14513/14513 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 14514/14514 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 14515/14515 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 14516/14516 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 14517/14517 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14518/14518 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 14519/14519 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14520/14520 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 14521/14521 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 14522/14522 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 14523/14523 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 14524/14524 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 14525/14525 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 14526/14526 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 14527/14527 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 14528/14528 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 14529/14529 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14530/14530 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14531/14531 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14532/14532 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 14533/14533 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 14534/14534 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 14535/14535 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 14536/14536 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 14537/14537 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 14538/14538 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 14539/14539 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 14540/14540 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14541/14541 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 14542/14542 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 14543/14543 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 14544/14544 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14545/14545 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14546/14546 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 14547/14547 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 14548/14548 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14549/14549 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14550/14550 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14551/14551 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14552/14552 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14553/14553 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 14554/14554 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 14555/14555 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 14556/14556 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 14557/14557 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 14558/14558 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 14559/14559 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 14560/14560 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 14561/14561 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 14562/14562 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 14563/14563 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 14564/14564 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 14565/14565 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 14566/14566 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 14567/14567 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 14568/14568 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 14569/14569 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 14570/14570 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 14571/14571 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 14572/14572 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 14573/14573 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 14574/14574 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 14575/14575 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 14576/14576 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 14577/14577 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 14578/14578 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 14579/14579 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 14580/14580 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 14581/14581 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 14582/14582 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 14583/14583 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 14584/14584 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 14585/14585 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 14586/14586 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 14587/14587 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 14588/14588 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 14589/14589 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 14590/14590 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 14591/14591 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 14592/14592 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 14593/14593 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 14594/14594 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 14595/14595 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 14596/14596 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 14597/14597 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 14598/14598 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 14599/14599 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 14600/14600 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 14601/14601 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 14602/14602 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 14603/14603 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 14604/14604 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14604/14605 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14605/14605 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14605/14606 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14606/14606 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14606/14607 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14607/14607 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14607/14608 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14608/14608 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14608/14609 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14609/14609 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14609/14610 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14610/14610 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14610/14611 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14611/14611 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 14612/14612 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14612/14613 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14613/14613 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14613/14614 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14614/14614 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14614/14615 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14615/14615 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14615/14616 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14616/14616 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14616/14617 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14617/14617 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14617/14618 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14618/14618 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14618/14619 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14619/14619 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14619/14620 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14620/14620 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14620/14621 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14621/14621 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 14622/14622 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14622/14623 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14623/14623 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14623/14624 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14624/14624 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14624/14625 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14625/14625 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14625/14626 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14626/14626 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 14627/14627 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14627/14628 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14628/14628 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14628/14629 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14629/14629 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14629/14630 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14630/14630 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 14631/14631 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 14632/14632 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14632/14633 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14633/14633 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14633/14634 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14634/14634 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14634/14635 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14635/14635 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14635/14636 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14636/14636 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 14637/14637 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14637/14638 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14638/14638 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14638/14639 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14639/14639 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14639/14640 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14640/14640 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14640/14641 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14641/14641 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 14642/14642 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14642/14643 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14643/14643 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14643/14644 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14644/14644 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14644/14645 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14645/14645 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14645/14646 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14646/14646 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 14647/14647 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14647/14648 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14648/14648 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14648/14649 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14649/14649 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14649/14650 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14650/14650 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14650/14651 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14651/14651 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14651/14652 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14652/14652 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14652/14653 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14653/14653 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14653/14654 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14654/14654 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 14655/14655 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14655/14656 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14656/14656 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14656/14657 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14657/14657 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14657/14658 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14658/14658 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14658/14659 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14659/14659 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14659/14660 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14660/14660 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14660/14661 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14661/14661 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14661/14662 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14662/14662 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14662/14663 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14663/14663 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14663/14664 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14664/14664 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14664/14665 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14665/14665 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14665/14666 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14666/14666 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14666/14667 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14667/14667 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14667/14668 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14668/14668 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14668/14669 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14669/14669 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14669/14670 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14670/14670 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 14671/14671 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14671/14672 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14672/14672 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14672/14673 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14673/14673 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14673/14674 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14674/14674 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14674/14675 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14675/14675 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14675/14676 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14676/14676 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14676/14677 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14677/14677 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14677/14678 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14678/14678 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 14679/14679 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 14680/14680 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14680/14681 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14681/14681 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 14682/14682 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14682/14683 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14683/14683 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14683/14684 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14684/14684 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 14685/14685 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14685/14686 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14686/14686 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14686/14687 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14687/14687 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14687/14688 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14688/14688 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 14689/14689 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 14690/14690 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14690/14691 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14691/14691 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 14692/14692 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14692/14693 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14693/14693 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 14694/14694 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14694/14695 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14695/14695 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14695/14696 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14696/14696 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14696/14697 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14697/14697 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 14698/14698 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14698/14699 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14699/14699 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14699/14700 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14700/14700 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 14701/14701 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 14702/14702 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 14703/14703 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14703/14704 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14704/14704 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14704/14705 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14705/14705 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14705/14706 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14706/14706 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 14707/14707 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 14708/14708 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14708/14709 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14709/14709 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14709/14710 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14710/14710 [00:02<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14710/14711 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14711/14711 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14711/14712 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14712/14712 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14712/14713 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14713/14713 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 14714/14714 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 14715/14715 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14715/14716 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14716/14716 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 14717/14717 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14717/14718 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14718/14718 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14718/14719 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14719/14719 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14719/14720 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14720/14720 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14720/14721 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14721/14721 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14721/14722 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14722/14722 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 14723/14723 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14723/14724 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14724/14724 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14724/14725 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14725/14725 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14725/14726 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14726/14726 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 14727/14727 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14727/14728 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14728/14728 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14728/14729 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14729/14729 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14729/14730 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14730/14730 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14730/14731 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14731/14731 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14731/14732 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14732/14732 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14732/14733 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14733/14733 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14733/14734 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14734/14734 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14734/14735 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14735/14735 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14735/14736 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14736/14736 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14736/14737 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14737/14737 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14737/14738 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14738/14738 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14738/14739 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14739/14739 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14739/14740 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14740/14740 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 14741/14741 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14741/14742 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14742/14742 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14742/14743 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14743/14743 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14743/14744 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14744/14744 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14744/14745 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14745/14745 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14745/14746 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14746/14746 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14746/14747 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14747/14747 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14747/14748 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14748/14748 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 14749/14749 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14749/14750 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14750/14750 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 14751/14751 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14751/14752 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14752/14752 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14752/14753 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14753/14753 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14753/14754 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14754/14754 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14754/14755 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14755/14755 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14755/14756 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14756/14756 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14756/14757 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14757/14757 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 14758/14758 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14758/14759 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14759/14759 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14759/14760 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14760/14760 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 14761/14761 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14761/14762 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14762/14762 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 14763/14763 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14763/14764 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14764/14764 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 14765/14765 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 14766/14766 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14766/14767 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14767/14767 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 14768/14768 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 14769/14769 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14769/14770 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14770/14770 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 14771/14771 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 14772/14772 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 14773/14773 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 14774/14774 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 14775/14775 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 14776/14776 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 14777/14777 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 14778/14778 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14778/14779 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14779/14779 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 14780/14780 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 14781/14781 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 14782/14782 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 14783/14783 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 14784/14784 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 14785/14785 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 14786/14786 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 14787/14787 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 14788/14788 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 14789/14789 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 14790/14790 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 14791/14791 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 14792/14792 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 14793/14793 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 14794/14794 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 14795/14795 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 14796/14796 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 14797/14797 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 14798/14798 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 14799/14799 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 14800/14800 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 14801/14801 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 14802/14802 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 14803/14803 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 14804/14804 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 14805/14805 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 14806/14806 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 14807/14807 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 14808/14808 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 14809/14809 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 14810/14810 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 14811/14811 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 14812/14812 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 14813/14813 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 14814/14814 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 14815/14815 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 14816/14816 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 14817/14817 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 14818/14818 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 14819/14819 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 14820/14820 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 14821/14821 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 14822/14822 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 14823/14823 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 14824/14824 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14825/14825 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14826/14826 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 14827/14827 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 14828/14828 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 14829/14829 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14830/14830 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 14831/14831 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 14832/14832 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 14833/14833 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 14834/14834 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14835/14835 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14836/14836 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14837/14837 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14838/14838 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14839/14839 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 14840/14840 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14841/14841 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14842/14842 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14843/14843 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14844/14844 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 14845/14845 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14846/14846 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 14847/14847 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14848/14848 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 14849/14849 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14850/14850 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 14851/14851 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14852/14852 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14853/14853 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14854/14854 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14855/14855 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 14856/14856 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 14857/14857 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14858/14858 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 14859/14859 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14860/14860 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 14861/14861 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14862/14862 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14863/14863 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14864/14864 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 14865/14865 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14866/14866 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 14867/14867 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14868/14868 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 14869/14869 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 14870/14870 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 14871/14871 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 14872/14872 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 14873/14873 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14874/14874 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 14875/14875 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14876/14876 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 14877/14877 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 14878/14878 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14879/14879 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14880/14880 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14881/14881 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14882/14882 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 14883/14883 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 14884/14884 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 14885/14885 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 14886/14886 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 14887/14887 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 14888/14888 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14889/14889 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14890/14890 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14891/14891 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 14892/14892 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 14893/14893 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14894/14894 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14895/14895 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 14896/14896 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14897/14897 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 14898/14898 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 14899/14899 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14900/14900 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14901/14901 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 14902/14902 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14903/14903 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 14904/14904 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14905/14905 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14906/14906 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 14907/14907 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 14908/14908 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 14909/14909 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 14910/14910 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14911/14911 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 14912/14912 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14913/14913 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 14914/14914 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 14915/14915 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 14916/14916 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 14917/14917 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 14918/14918 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 14919/14919 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 14920/14920 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 14921/14921 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 14922/14922 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 14923/14923 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 14924/14924 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 14925/14925 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 14926/14926 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 14927/14927 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 14928/14928 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 14929/14929 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 14930/14930 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 14931/14931 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 14932/14932 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 14933/14933 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 14934/14934 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 14935/14935 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 14936/14936 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 14937/14937 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 14938/14938 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 14939/14939 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 14940/14940 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 14941/14941 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 14942/14942 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 14943/14943 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 14944/14944 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 14945/14945 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 14946/14946 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 14947/14947 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 14948/14948 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 14949/14949 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 14950/14950 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 14951/14951 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 14952/14952 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 14953/14953 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 14954/14954 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 14955/14955 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 14956/14956 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 14957/14957 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 14958/14958 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 14959/14959 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 14960/14960 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 14961/14961 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 14962/14962 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 14963/14963 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 14964/14964 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 14965/14965 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 14966/14966 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 14967/14967 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 14968/14968 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 14969/14969 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 14970/14970 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 14971/14971 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 14972/14972 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 14973/14973 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 14974/14974 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 14975/14975 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 14976/14976 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 14977/14977 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 14978/14978 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 14979/14979 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 14980/14980 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 14981/14981 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 14982/14982 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 14983/14983 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 14984/14984 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 14985/14985 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 14986/14986 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 14987/14987 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 14988/14988 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 14989/14989 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14989/14990 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14990/14990 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14990/14991 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14991/14991 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14991/14992 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14992/14992 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14992/14993 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14993/14993 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14993/14994 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14994/14994 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14994/14995 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14995/14995 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14995/14996 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14996/14996 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14996/14997 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14997/14997 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14997/14998 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14998/14998 [00:04<00:00,  4.71s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14998/14999 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14999/14999 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 14999/15000 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15000/15001 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15001/15001 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15001/15002 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15002/15002 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15002/15003 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15003/15003 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15003/15004 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15004/15004 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15004/15005 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15005/15005 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15005/15006 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15006/15006 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15006/15007 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15007/15007 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15007/15008 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15008/15008 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15008/15009 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15009/15009 [00:04<00:00,  4.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15009/15010 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15010/15010 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15010/15011 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15011/15011 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15011/15012 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15012/15012 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15012/15013 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15013/15013 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15013/15014 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15014/15014 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15014/15015 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15015/15015 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15015/15016 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15016/15016 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15016/15017 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15017/15017 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15017/15018 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15018/15018 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15018/15019 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15019/15019 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15019/15020 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15020/15020 [00:04<00:00,  4.71s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15020/15021 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15021/15021 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15021/15022 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15022/15022 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15022/15023 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15023/15023 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15023/15024 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15024/15024 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15024/15025 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15025/15025 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 15026/15026 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15026/15027 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15027/15027 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15027/15028 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15028/15028 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15028/15029 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15029/15029 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15029/15030 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15030/15030 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15030/15031 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15031/15031 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15031/15032 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15032/15032 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15032/15033 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15033/15033 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15033/15034 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15034/15034 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15034/15035 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15035/15035 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15035/15036 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15036/15036 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15036/15037 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15037/15037 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15037/15038 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15038/15038 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15038/15039 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15039/15039 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15039/15040 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15040/15040 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15040/15041 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15041/15041 [00:04<00:00,  4.71s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15041/15042 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15042/15042 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15042/15043 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15043/15043 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15043/15044 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15044/15044 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15044/15045 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15045/15045 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15045/15046 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15046/15046 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15046/15047 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15047/15047 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15047/15048 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15048/15048 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15048/15049 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15049/15049 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15049/15050 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15050/15050 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15050/15051 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15051/15051 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15051/15052 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15052/15052 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 15053/15053 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15053/15054 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15054/15054 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15054/15055 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15055/15055 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15055/15056 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15056/15056 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15056/15057 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15057/15057 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15057/15058 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15058/15058 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15058/15059 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15059/15059 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15059/15060 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15060/15060 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15060/15061 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15061/15061 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15061/15062 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15062/15062 [00:04<00:00,  4.56s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15062/15063 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15063/15063 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15063/15064 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15064/15064 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15064/15065 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15065/15065 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15065/15066 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15066/15066 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15066/15067 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15067/15067 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15067/15068 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15068/15068 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15068/15069 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15069/15069 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15069/15070 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15070/15070 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 15071/15071 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15071/15072 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15072/15072 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15072/15073 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15073/15073 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15073/15074 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15074/15074 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15074/15075 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15075/15075 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15075/15076 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15076/15076 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15076/15077 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15077/15077 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15077/15078 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15078/15078 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15078/15079 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15079/15079 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15079/15080 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15080/15080 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15080/15081 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15081/15081 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 15082/15082 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15082/15083 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15083/15083 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 15084/15084 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15084/15085 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15085/15085 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15085/15086 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15086/15086 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 15087/15087 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15087/15088 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15088/15088 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15088/15089 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15089/15089 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15089/15090 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15090/15090 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15090/15091 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15091/15091 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 15092/15092 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 15093/15093 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15093/15094 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15094/15094 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15094/15095 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15095/15095 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 15096/15096 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 15097/15097 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 15098/15098 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15098/15099 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15099/15099 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 15100/15100 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 15101/15101 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15101/15102 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15102/15102 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 15103/15103 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15103/15104 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15104/15104 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 15105/15105 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15105/15106 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15106/15106 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15106/15107 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15107/15107 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15107/15108 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15108/15108 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15108/15109 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15109/15109 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15109/15110 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15110/15110 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15110/15111 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15111/15111 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 15112/15112 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 15113/15113 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 15114/15114 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15114/15115 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15115/15115 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 15116/15116 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 15117/15117 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15117/15118 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15118/15118 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 15119/15119 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15119/15120 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15120/15120 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 15121/15121 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15121/15122 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15122/15122 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15122/15123 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15123/15123 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 15124/15124 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15124/15125 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15125/15125 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15125/15126 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15126/15126 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15126/15127 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15127/15127 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 15128/15128 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15128/15129 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15129/15129 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15129/15130 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15130/15130 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 15131/15131 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15132/15132 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15132/15133 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15133/15133 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 15134/15134 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 15135/15135 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 15136/15136 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 15137/15137 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 15138/15138 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15138/15139 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15139/15139 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15139/15140 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15140/15140 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15140/15141 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15141/15141 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15141/15142 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15142/15142 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15142/15143 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15143/15143 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15143/15144 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15144/15144 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 15145/15145 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15145/15146 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15146/15146 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15146/15147 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15147/15147 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15147/15148 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15148/15148 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15148/15149 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15149/15149 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15149/15150 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15150/15150 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15150/15151 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15151/15151 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15151/15152 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15152/15152 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15152/15153 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15153/15153 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15153/15154 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15154/15154 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 15155/15155 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 15156/15156 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 15157/15157 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15157/15158 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15158/15158 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15158/15159 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15159/15159 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 15160/15160 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 15161/15161 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15161/15162 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15162/15162 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15162/15163 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15163/15163 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15163/15164 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15164/15164 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15164/15165 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15165/15165 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15165/15166 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15166/15166 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15166/15167 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15167/15167 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15167/15168 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15168/15168 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 15169/15169 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 15170/15170 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15170/15171 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15171/15171 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15171/15172 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15172/15172 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15172/15173 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15173/15173 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15173/15174 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15174/15174 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15174/15175 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15175/15175 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15175/15176 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15176/15176 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15176/15177 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15177/15177 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15177/15178 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15178/15178 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15178/15179 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15179/15179 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15179/15180 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15180/15180 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15180/15181 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15181/15181 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 15182/15182 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15182/15183 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15183/15183 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15183/15184 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15184/15184 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15184/15185 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15185/15185 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15185/15186 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15186/15186 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15186/15187 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15187/15187 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15187/15188 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15188/15188 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15188/15189 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15189/15189 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15189/15190 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15190/15190 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15190/15191 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15191/15191 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15191/15192 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15192/15192 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15192/15193 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15193/15193 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15193/15194 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15194/15194 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15194/15195 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15195/15195 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15195/15196 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15196/15196 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15196/15197 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15197/15197 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15197/15198 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15198/15198 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15198/15199 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15199/15199 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15199/15200 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15200/15200 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15200/15201 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15201/15201 [00:04<00:00,  4.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15201/15202 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15202/15202 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15202/15203 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15203/15203 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15203/15204 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15204/15204 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15204/15205 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15205/15205 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15205/15206 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15206/15206 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15206/15207 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15207/15207 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15207/15208 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15208/15208 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15208/15209 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15209/15209 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15209/15210 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15210/15210 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15210/15211 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15211/15211 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15211/15212 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15212/15212 [00:04<00:00,  4.07s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15212/15213 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15213/15213 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15213/15214 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15214/15214 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15214/15215 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15215/15215 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15215/15216 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15216/15216 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15216/15217 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15217/15217 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15217/15218 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15218/15218 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15218/15219 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15219/15219 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15219/15220 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15220/15220 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15220/15221 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15221/15221 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15221/15222 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15222/15222 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15222/15223 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15223/15223 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15223/15224 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15224/15224 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15224/15225 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15225/15225 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15225/15226 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15226/15226 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15226/15227 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15227/15227 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15227/15228 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15228/15228 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15228/15229 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15229/15229 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15229/15230 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15230/15230 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15230/15231 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15231/15231 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15231/15232 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15232/15232 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15232/15233 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15233/15233 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15233/15234 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15234/15234 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 15234/15235 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15235/15235 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 15236/15236 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 15237/15237 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 15238/15238 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15239/15239 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15240/15240 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 15241/15241 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 15242/15242 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 15243/15243 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 15244/15244 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15245/15245 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 15246/15246 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 15247/15247 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 15248/15248 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 15249/15249 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 15250/15250 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 15251/15251 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 15252/15252 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 15253/15253 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 15254/15254 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 15255/15255 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 15256/15256 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 15257/15257 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 15258/15258 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 15259/15259 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 15260/15260 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 15261/15261 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15262/15262 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15263/15263 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15264/15264 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 15265/15265 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 15266/15266 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15267/15267 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15268/15268 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 15269/15269 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 15270/15270 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 15271/15271 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15272/15272 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15273/15273 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 15274/15274 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15275/15275 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 15276/15276 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 15277/15277 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15278/15278 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 15279/15279 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 15280/15280 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 15281/15281 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 15282/15282 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15283/15283 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15284/15284 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 15285/15285 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15286/15286 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 15287/15287 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 15288/15288 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15289/15289 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 15290/15290 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 15291/15291 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 15292/15292 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 15293/15293 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 15294/15294 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 15295/15295 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 15296/15296 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 15297/15297 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 15298/15298 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 15299/15299 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 15300/15300 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 15301/15301 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 15302/15302 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 15303/15303 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 15304/15304 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 15305/15305 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 15306/15306 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 15307/15307 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 15308/15308 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 15309/15309 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 15310/15310 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 15311/15311 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 15312/15312 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 15313/15313 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 15314/15314 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 15315/15315 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 15316/15316 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 15317/15317 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 15318/15318 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 15319/15319 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 15320/15320 [00:04<00:00,  4.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 15321/15321 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 15322/15322 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 15323/15323 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 15324/15324 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 15325/15325 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 15326/15326 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 15327/15327 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 15328/15328 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 15329/15329 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 15330/15330 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 15331/15331 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 15332/15332 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 15333/15333 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15334/15334 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 15335/15335 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 15336/15336 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 15337/15337 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 15338/15338 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15339/15339 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 15340/15340 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15341/15341 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 15342/15342 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 15343/15343 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 15344/15344 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 15345/15345 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 15346/15346 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 15347/15347 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 15348/15348 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 15349/15349 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 15350/15350 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 15351/15351 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 15352/15352 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 15353/15353 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 15354/15354 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 15355/15355 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 15356/15356 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 15357/15357 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 15358/15358 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 15359/15359 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 15360/15360 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 15361/15361 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 15362/15362 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 15363/15363 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 15364/15364 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 15365/15365 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 15366/15366 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 15367/15367 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 15368/15368 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 15369/15369 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 15370/15370 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 15371/15371 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 15372/15372 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 15373/15373 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 15374/15374 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 15375/15375 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 15376/15376 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 15377/15377 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 15378/15378 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15379/15379 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 15380/15380 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 15381/15381 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 15382/15382 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 15383/15383 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 15384/15384 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 15385/15385 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15386/15386 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15387/15387 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15388/15388 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15389/15389 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15390/15390 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 15391/15391 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 15392/15392 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15393/15393 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 15394/15394 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 15395/15395 [00:04<00:00,  4.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15396/15396 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 15397/15397 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 15398/15398 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 15399/15399 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 15400/15400 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 15401/15401 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 15402/15402 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 15403/15403 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 15404/15404 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 15405/15405 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 15406/15406 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 15407/15407 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 15408/15408 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 15409/15409 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 15410/15410 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 15411/15411 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 15412/15412 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 15413/15413 [00:03<00:00,  3.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 15414/15414 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 15415/15415 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 15416/15416 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 15417/15417 [00:04<00:00,  4.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 15418/15418 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 15419/15419 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 15420/15420 [00:04<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 15421/15421 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 15422/15422 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 15423/15423 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 15424/15424 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 15425/15425 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 15426/15426 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 15427/15427 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 15428/15428 [00:05<00:00,  5.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 15429/15429 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 15430/15430 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 15431/15431 [00:04<00:00,  4.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 15432/15432 [00:04<00:00,  4.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 15433/15433 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 15434/15434 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 15435/15435 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 15436/15436 [00:04<00:00,  4.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 15437/15437 [00:04<00:00,  4.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 15438/15438 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 15439/15439 [00:05<00:00,  5.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 15440/15440 [00:04<00:00,  4.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 15441/15441 [00:04<00:00,  4.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 15442/15442 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15443/15443 [00:04<00:00,  4.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15444/15444 [00:04<00:00,  4.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 15445/15445 [00:04<00:00,  4.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15446/15446 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 15447/15447 [00:04<00:00,  4.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 15448/15448 [00:04<00:00,  4.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 15449/15449 [00:04<00:00,  4.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 15450/15450 [00:05<00:00,  5.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 15451/15451 [00:04<00:00,  4.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 15452/15452 [00:04<00:00,  4.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 15453/15453 [00:04<00:00,  4.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 15454/15454 [00:05<00:00,  5.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 15455/15455 [00:04<00:00,  4.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 15456/15456 [00:04<00:00,  4.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 15457/15457 [00:04<00:00,  4.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 15458/15458 [00:04<00:00,  4.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 15459/15459 [00:04<00:00,  4.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 15460/15460 [00:04<00:00,  4.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 15461/15461 [00:05<00:00,  5.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 15462/15462 [00:04<00:00,  4.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 15463/15463 [00:04<00:00,  4.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 15464/15464 [00:04<00:00,  5.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 15465/15465 [00:04<00:00,  4.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 15466/15466 [00:04<00:00,  4.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 15467/15467 [00:05<00:00,  5.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 15468/15468 [00:05<00:00,  5.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 15469/15469 [00:04<00:00,  4.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 15470/15470 [00:05<00:00,  5.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 15471/15471 [00:05<00:00,  5.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 15472/15472 [00:06<00:00,  6.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 15473/15473 [00:05<00:00,  5.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 15474/15474 [00:05<00:00,  5.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 15475/15475 [00:05<00:00,  5.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 15476/15476 [00:05<00:00,  5.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 15477/15477 [00:05<00:00,  5.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 15478/15478 [00:05<00:00,  5.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 15479/15479 [00:05<00:00,  5.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 15480/15480 [00:05<00:00,  5.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 15481/15481 [00:05<00:00,  5.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 15482/15482 [00:05<00:00,  5.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 15483/15483 [00:06<00:00,  6.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 15484/15484 [00:05<00:00,  5.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 15485/15485 [00:05<00:00,  5.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 15486/15486 [00:05<00:00,  5.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15487/15487 [00:05<00:00,  5.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 15488/15488 [00:05<00:00,  5.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 15489/15489 [00:05<00:00,  5.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 15490/15490 [00:05<00:00,  5.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 15491/15491 [00:05<00:00,  5.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 15492/15492 [00:05<00:00,  5.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 15493/15493 [00:06<00:00,  6.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 15494/15494 [00:05<00:00,  5.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 15495/15495 [00:05<00:00,  5.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 15496/15496 [00:05<00:00,  5.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 15497/15497 [00:06<00:00,  6.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 15498/15498 [00:06<00:00,  6.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 15499/15499 [00:06<00:00,  6.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 15500/15500 [00:06<00:00,  6.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 15501/15501 [00:06<00:00,  6.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 15502/15502 [00:06<00:00,  6.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 15503/15503 [00:06<00:00,  6.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 15504/15504 [00:07<00:00,  7.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 15505/15505 [00:06<00:00,  6.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 15506/15506 [00:06<00:00,  6.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 15507/15507 [00:06<00:00,  6.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 15508/15508 [00:06<00:00,  6.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 15509/15509 [00:06<00:00,  6.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 15510/15510 [00:06<00:00,  6.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 15511/15511 [00:06<00:00,  6.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15512/15512 [00:06<00:00,  6.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 15513/15513 [00:06<00:00,  6.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 15514/15514 [00:06<00:00,  6.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 15515/15515 [00:07<00:00,  7.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15516/15516 [00:06<00:00,  6.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 15517/15517 [00:06<00:00,  6.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 15518/15518 [00:06<00:00,  6.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 15519/15519 [00:06<00:00,  6.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 15520/15520 [00:06<00:00,  6.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 15521/15521 [00:06<00:00,  6.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 15522/15522 [00:06<00:00,  6.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 15523/15523 [00:06<00:00,  6.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 15524/15524 [00:06<00:00,  6.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 15525/15525 [00:06<00:00,  6.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 15526/15526 [00:07<00:00,  7.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15527/15527 [00:06<00:00,  6.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15528/15528 [00:06<00:00,  6.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 15529/15529 [00:06<00:00,  6.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 15530/15530 [00:07<00:00,  7.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 15531/15531 [00:06<00:00,  6.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 15532/15532 [00:07<00:00,  7.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 15533/15533 [00:07<00:00,  7.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 15534/15534 [00:06<00:00,  6.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 15535/15535 [00:07<00:00,  7.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 15536/15536 [00:06<00:00,  6.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 15537/15537 [00:08<00:00,  8.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 15538/15538 [00:07<00:00,  7.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 15539/15539 [00:07<00:00,  7.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 15540/15540 [00:07<00:00,  7.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 15541/15541 [00:07<00:00,  7.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 15542/15542 [00:06<00:00,  6.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 15543/15543 [00:07<00:00,  7.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 15544/15544 [00:06<00:00,  6.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 15545/15545 [00:07<00:00,  7.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 15546/15546 [00:07<00:00,  7.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 15547/15547 [00:07<00:00,  7.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 15548/15548 [00:08<00:00,  8.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 15549/15549 [00:07<00:00,  7.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 15550/15550 [00:07<00:00,  7.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 15551/15551 [00:06<00:00,  6.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 15552/15552 [00:07<00:00,  7.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 15553/15553 [00:07<00:00,  7.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 15554/15554 [00:07<00:00,  7.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 15555/15555 [00:07<00:00,  7.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 15556/15556 [00:07<00:00,  7.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 15557/15557 [00:07<00:00,  7.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 15558/15558 [00:07<00:00,  7.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 15559/15559 [00:08<00:00,  8.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 15560/15560 [00:07<00:00,  7.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15561/15561 [00:07<00:00,  7.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 15562/15562 [00:07<00:00,  7.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 15563/15563 [00:07<00:00,  7.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 15564/15564 [00:07<00:00,  7.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 15565/15565 [00:07<00:00,  7.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 15566/15566 [00:07<00:00,  7.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 15567/15567 [00:06<00:00,  7.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 15568/15568 [00:07<00:00,  7.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 15569/15569 [00:07<00:00,  7.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 15570/15570 [00:07<00:00,  7.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 15571/15571 [00:07<00:00,  7.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 15572/15572 [00:07<00:00,  7.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 15573/15573 [00:07<00:00,  7.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15574/15574 [00:07<00:00,  7.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 15575/15575 [00:07<00:00,  7.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15576/15576 [00:07<00:00,  7.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 15577/15577 [00:06<00:00,  6.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 15578/15578 [00:07<00:00,  7.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 15579/15579 [00:06<00:00,  6.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 15580/15580 [00:06<00:00,  6.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 15581/15581 [00:07<00:00,  7.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15582/15582 [00:06<00:00,  6.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 15583/15583 [00:07<00:00,  7.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 15584/15584 [00:07<00:00,  7.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15585/15585 [00:06<00:00,  6.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 15586/15586 [00:06<00:00,  6.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 15587/15587 [00:06<00:00,  6.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 15588/15588 [00:06<00:00,  6.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 15589/15589 [00:06<00:00,  6.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 15590/15590 [00:06<00:00,  6.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 15591/15591 [00:06<00:00,  6.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 15592/15592 [00:07<00:00,  7.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 15593/15593 [00:06<00:00,  6.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 15594/15594 [00:07<00:00,  7.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 15595/15595 [00:06<00:00,  6.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 15596/15596 [00:07<00:00,  7.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 15597/15597 [00:06<00:00,  6.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15598/15598 [00:06<00:00,  6.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 15599/15599 [00:06<00:00,  6.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 15600/15600 [00:06<00:00,  6.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 15601/15601 [00:06<00:00,  6.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 15602/15602 [00:06<00:00,  6.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 15603/15603 [00:07<00:00,  7.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 15604/15604 [00:06<00:00,  6.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 15605/15605 [00:06<00:00,  6.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 15606/15606 [00:06<00:00,  6.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 15607/15607 [00:07<00:00,  7.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 15608/15608 [00:06<00:00,  6.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 15609/15609 [00:06<00:00,  6.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 15610/15610 [00:06<00:00,  6.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 15611/15611 [00:06<00:00,  6.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 15612/15612 [00:06<00:00,  6.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 15613/15613 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15614/15614 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15615/15615 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 15616/15616 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 15617/15617 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 15618/15618 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15619/15619 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 15620/15620 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 15621/15621 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15622/15622 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15623/15623 [00:01<00:00,  1.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 15624/15624 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15625/15625 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 15626/15626 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15627/15627 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15628/15628 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15629/15629 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15630/15630 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15631/15631 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15632/15632 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15633/15633 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15634/15634 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15635/15635 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15636/15636 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15637/15637 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15638/15638 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15639/15639 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15640/15640 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15641/15641 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15642/15642 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15643/15643 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15644/15644 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15645/15645 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15646/15646 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 15647/15647 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15648/15648 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15649/15649 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15650/15650 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15651/15651 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15652/15652 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15653/15653 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15654/15654 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15655/15655 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15656/15656 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15657/15657 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 15658/15658 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15659/15659 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15660/15660 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15661/15661 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15662/15662 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15663/15663 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15664/15664 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 15665/15665 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15666/15666 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15667/15667 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15668/15668 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 15669/15669 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15670/15670 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15671/15671 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15672/15672 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15673/15673 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15674/15674 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15675/15675 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15676/15676 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15677/15677 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15678/15678 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15679/15679 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15680/15680 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15681/15681 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15682/15682 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15683/15683 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15684/15684 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15685/15685 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15686/15686 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15687/15687 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15688/15688 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15689/15689 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 15690/15690 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15691/15691 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15692/15692 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15693/15693 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 15694/15694 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15695/15695 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15696/15696 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15697/15697 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15698/15698 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15699/15699 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15700/15700 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15701/15701 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15702/15702 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15703/15703 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15704/15704 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15705/15705 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15706/15706 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15707/15707 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15708/15708 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15709/15709 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15710/15710 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15711/15711 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15712/15712 [00:01<00:00,  1.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15713/15713 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15714/15714 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15715/15715 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15716/15716 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15717/15717 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15718/15718 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15719/15719 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15720/15720 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15721/15721 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 15722/15722 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15723/15723 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 15724/15724 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15725/15725 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15726/15726 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15727/15727 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15728/15728 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15729/15729 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15730/15730 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15731/15731 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15732/15732 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 15733/15733 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15734/15734 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15735/15735 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15736/15736 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15737/15737 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15738/15738 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15739/15739 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15740/15740 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15741/15741 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15742/15742 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15743/15743 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 15744/15744 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15745/15745 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15746/15746 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15747/15747 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15748/15748 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15749/15749 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15750/15750 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15751/15751 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15752/15752 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15753/15753 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15754/15754 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 15755/15755 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15756/15756 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15757/15757 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15758/15758 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15759/15759 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15760/15760 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15761/15761 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15762/15762 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15763/15763 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15764/15764 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15765/15765 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 15766/15766 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15767/15767 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15768/15768 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15769/15769 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15770/15770 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15771/15771 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15772/15772 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15773/15773 [00:01<00:00,  1.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 15774/15774 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15775/15775 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15776/15776 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 15777/15777 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15778/15778 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15779/15779 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15780/15780 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15781/15781 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15782/15782 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15783/15783 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15784/15784 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15785/15785 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15786/15786 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15787/15787 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 15788/15788 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15789/15789 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15790/15790 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15791/15791 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15792/15792 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15793/15793 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15794/15794 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15795/15795 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15796/15796 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15797/15797 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15798/15798 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 15799/15799 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15800/15800 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15801/15801 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15802/15802 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15803/15803 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15804/15804 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15805/15805 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15806/15806 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15807/15807 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15808/15808 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15809/15809 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 15810/15810 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15811/15811 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15812/15812 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15813/15813 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15814/15814 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15815/15815 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15816/15816 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15817/15817 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15818/15818 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15819/15819 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15820/15820 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 15821/15821 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 15822/15822 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15823/15823 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15824/15824 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15825/15825 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15826/15826 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15827/15827 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15828/15828 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15829/15829 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15830/15830 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15831/15831 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 15832/15832 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15833/15833 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 15834/15834 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15835/15835 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15836/15836 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15837/15837 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15838/15838 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15839/15839 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15840/15840 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15841/15841 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15842/15842 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 15843/15843 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15844/15844 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15845/15845 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15846/15846 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15847/15847 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15848/15848 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15849/15849 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15850/15850 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15851/15851 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15852/15852 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15853/15853 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15854/15854 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 15855/15855 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15856/15856 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15857/15857 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15858/15858 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15859/15859 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15860/15860 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15861/15861 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15862/15862 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15863/15863 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 15864/15864 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15865/15865 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15866/15866 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15867/15867 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15868/15868 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15869/15869 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15870/15870 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15871/15871 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15872/15872 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15873/15873 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15874/15874 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15875/15875 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15876/15876 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15877/15877 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15878/15878 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15879/15879 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15880/15880 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15881/15881 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15882/15882 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15883/15883 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15884/15884 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15885/15885 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15886/15886 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15887/15887 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15888/15888 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15889/15889 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15890/15890 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15891/15891 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15892/15892 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15893/15893 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15894/15894 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15895/15895 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 15896/15896 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15897/15897 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15898/15898 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15899/15899 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15900/15900 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15901/15901 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 15902/15902 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15903/15903 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15904/15904 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15905/15905 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15906/15906 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 15907/15907 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15908/15908 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15909/15909 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15910/15910 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15911/15911 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15912/15912 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15913/15913 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15914/15914 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15915/15915 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15916/15916 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15917/15917 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 15918/15918 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15919/15919 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15920/15920 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15921/15921 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15922/15922 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15923/15923 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15924/15924 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15925/15925 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15926/15926 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15927/15927 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15928/15928 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15929/15929 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15930/15930 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 15931/15931 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15932/15932 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15933/15933 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15934/15934 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15935/15935 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15936/15936 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15937/15937 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15938/15938 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15939/15939 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15940/15940 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15941/15941 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15942/15942 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15943/15943 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15944/15944 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15945/15945 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15946/15946 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15947/15947 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15948/15948 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15949/15949 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15950/15950 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15951/15951 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15952/15952 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15953/15953 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15954/15954 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15955/15955 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15956/15956 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15957/15957 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15958/15958 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15959/15959 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15960/15960 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 15961/15961 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15962/15962 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15963/15963 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15964/15964 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15965/15965 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15966/15966 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15967/15967 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15968/15968 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15969/15969 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15970/15970 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15971/15971 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 15972/15972 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15973/15973 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15974/15974 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15975/15975 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15976/15976 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15977/15977 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15978/15978 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15979/15979 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15980/15980 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15981/15981 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15982/15982 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 15983/15983 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 15984/15984 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15985/15985 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15986/15986 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15987/15987 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 15988/15988 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 15989/15989 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15990/15990 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15991/15991 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15992/15992 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 15993/15993 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15994/15994 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15995/15995 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 15996/15996 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 15997/15997 [00:01<00:00,  1.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 15998/15998 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 15999/15999 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16000/16000 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 16001/16001 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16002/16002 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16003/16003 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 16004/16004 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16005/16005 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16006/16006 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16007/16007 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16008/16008 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16009/16009 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16010/16010 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16011/16011 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16012/16012 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16013/16013 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16014/16014 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 16015/16015 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16016/16016 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16017/16017 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16018/16018 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16019/16019 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16020/16020 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16021/16021 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16022/16022 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16023/16023 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16024/16024 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16025/16025 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 16026/16026 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16027/16027 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16028/16028 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16029/16029 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16030/16030 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16031/16031 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16032/16032 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16033/16033 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16034/16034 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16035/16035 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16036/16036 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 16037/16037 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16038/16038 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16039/16039 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16040/16040 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16041/16041 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 16042/16042 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16043/16043 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16044/16044 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16045/16045 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16046/16046 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16047/16047 [00:01<00:00,  1.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16048/16048 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16049/16049 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16050/16050 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16051/16051 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16052/16052 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16053/16053 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16054/16054 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16055/16055 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16056/16056 [00:01<00:00,  1.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 16057/16057 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 16058/16058 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16059/16059 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16060/16060 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16061/16061 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16062/16062 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16063/16063 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16064/16064 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16065/16065 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16066/16066 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16067/16067 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16068/16068 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 16069/16069 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16070/16070 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16071/16071 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16072/16072 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16073/16073 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16074/16074 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16075/16075 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16076/16076 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16077/16077 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16078/16078 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16079/16079 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16080/16080 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 16081/16081 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16082/16082 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 16083/16083 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16084/16084 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16085/16085 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 16086/16086 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16087/16087 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16088/16088 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 16089/16089 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16090/16090 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16091/16091 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16092/16092 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16093/16093 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16094/16094 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16095/16095 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16096/16096 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16097/16097 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16098/16098 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16099/16099 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16100/16100 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16101/16101 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16102/16102 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 16103/16103 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 16104/16104 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16105/16105 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16106/16106 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16107/16107 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16108/16108 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16109/16109 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16110/16110 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 16111/16111 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16112/16112 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 16113/16113 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16114/16114 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16115/16115 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16116/16116 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 16117/16117 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16118/16118 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16119/16119 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16120/16120 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16121/16121 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16122/16122 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16123/16123 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16124/16124 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16125/16125 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 16126/16126 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16127/16127 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16128/16128 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16129/16129 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16130/16130 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16131/16131 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 16132/16132 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16133/16133 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16134/16134 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16135/16135 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16136/16136 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16137/16137 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16138/16138 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 16139/16139 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16140/16140 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16141/16141 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16142/16142 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16143/16143 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16144/16144 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16145/16145 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16146/16146 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16147/16147 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 16148/16148 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16149/16149 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16150/16150 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 16151/16151 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16152/16152 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16153/16153 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16154/16154 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16155/16155 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16156/16156 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16157/16157 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16158/16158 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16159/16159 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16160/16160 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16161/16161 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16162/16162 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16163/16163 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16164/16164 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16165/16165 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16166/16166 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16167/16167 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16168/16168 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16169/16169 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16170/16170 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16171/16171 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16172/16172 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16173/16173 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16174/16174 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16175/16175 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 16176/16176 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16177/16177 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16178/16178 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16179/16179 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16180/16180 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16181/16181 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16182/16182 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16183/16183 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16184/16184 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16185/16185 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16186/16186 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 16187/16187 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16188/16188 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16189/16189 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16190/16190 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16191/16191 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16192/16192 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16193/16193 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16194/16194 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16195/16195 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16196/16196 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16197/16197 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 16198/16198 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16199/16199 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16200/16200 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16201/16201 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16202/16202 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16203/16203 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16204/16204 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16205/16205 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16206/16206 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16207/16207 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16208/16208 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16209/16209 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16210/16210 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16211/16211 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16212/16212 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16213/16213 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16214/16214 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16215/16215 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16216/16216 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16217/16217 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16218/16218 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16219/16219 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16220/16220 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16221/16221 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16222/16222 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16223/16223 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16224/16224 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16225/16225 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16226/16226 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16227/16227 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16228/16228 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16229/16229 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 16230/16230 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16231/16231 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16232/16232 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16233/16233 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16234/16234 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16235/16235 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16236/16236 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16237/16237 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16238/16238 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16239/16239 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16240/16240 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16241/16241 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16242/16242 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16243/16243 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 16244/16244 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16245/16245 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16246/16246 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16247/16247 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16248/16248 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16249/16249 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16250/16250 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16251/16251 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16252/16252 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16253/16253 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16254/16254 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16255/16255 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16256/16256 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16257/16257 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16258/16258 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16259/16259 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16260/16260 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16261/16261 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 16262/16262 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16263/16263 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16264/16264 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16265/16265 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16266/16266 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16267/16267 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16268/16268 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16269/16269 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16270/16270 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16271/16271 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16272/16272 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 16273/16273 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16274/16274 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16275/16275 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16276/16276 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16277/16277 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16278/16278 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16279/16279 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16280/16280 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16281/16281 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16282/16282 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16283/16283 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16284/16284 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16285/16285 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16286/16286 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16287/16287 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16288/16288 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16289/16289 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16290/16290 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16291/16291 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16292/16292 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16293/16293 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16294/16294 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16295/16295 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16296/16296 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16297/16297 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16298/16298 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16299/16299 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16300/16300 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16301/16301 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16302/16302 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16303/16303 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16304/16304 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16305/16305 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16306/16306 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16307/16307 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16308/16308 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16309/16309 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16310/16310 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16311/16311 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16312/16312 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16313/16313 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16314/16314 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16315/16315 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16316/16316 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16317/16317 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16318/16318 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16319/16319 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16320/16320 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16321/16321 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16322/16322 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16323/16323 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16324/16324 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16325/16325 [00:01<00:00,  1.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16326/16326 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 16327/16327 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16328/16328 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16329/16329 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16330/16330 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16331/16331 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16332/16332 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16333/16333 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16334/16334 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16335/16335 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16336/16336 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16337/16337 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16338/16338 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16339/16339 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16340/16340 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16341/16341 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16342/16342 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16343/16343 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16344/16344 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16345/16345 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16346/16346 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16347/16347 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16348/16348 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16349/16349 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16350/16350 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16351/16351 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16352/16352 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16353/16353 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16354/16354 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16355/16355 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16356/16356 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16357/16357 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16358/16358 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16359/16359 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 16360/16360 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16361/16361 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16362/16362 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16363/16363 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16364/16364 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16365/16365 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16366/16366 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16367/16367 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16368/16368 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16369/16369 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16370/16370 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16371/16371 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16372/16372 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16373/16373 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16374/16374 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16375/16375 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16376/16376 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16377/16377 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16378/16378 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16379/16379 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16380/16380 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16381/16381 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16382/16382 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16383/16383 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16384/16384 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16385/16385 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16386/16386 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16387/16387 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16388/16388 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16389/16389 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16390/16390 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16391/16391 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16392/16392 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16393/16393 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16394/16394 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16395/16395 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16396/16396 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16397/16397 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16398/16398 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16399/16399 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16400/16400 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16401/16401 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16402/16402 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16403/16403 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16404/16404 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16405/16405 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16406/16406 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16407/16407 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16408/16408 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16409/16409 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16410/16410 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16411/16411 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16412/16412 [00:01<00:00,  1.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16413/16413 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 16414/16414 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16415/16415 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16416/16416 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16417/16417 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16418/16418 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16419/16419 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16420/16420 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16421/16421 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16422/16422 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16423/16423 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16424/16424 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16425/16425 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16426/16426 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16427/16427 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16428/16428 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16429/16429 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16430/16430 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16431/16431 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16432/16432 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16433/16433 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16434/16434 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16435/16435 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16436/16436 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16437/16437 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16438/16438 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16439/16439 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16440/16440 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16441/16441 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16442/16442 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16443/16443 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16444/16444 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16445/16445 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16446/16446 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16447/16447 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16448/16448 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16449/16449 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16450/16450 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16451/16451 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16452/16452 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16453/16453 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16454/16454 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16455/16455 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16456/16456 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16457/16457 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16458/16458 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16459/16459 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16460/16460 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16461/16461 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16462/16462 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16463/16463 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16464/16464 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16465/16465 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16466/16466 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16467/16467 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16468/16468 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16469/16469 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16470/16470 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16471/16471 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16472/16472 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16473/16473 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16474/16474 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16475/16475 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16476/16476 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16477/16477 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16478/16478 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16479/16479 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16480/16480 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16481/16481 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16482/16482 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16483/16483 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16484/16484 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16485/16485 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16486/16486 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16487/16487 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16488/16488 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16489/16489 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16490/16490 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16491/16491 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16492/16492 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16493/16493 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16494/16494 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16495/16495 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16496/16496 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16497/16497 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16498/16498 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16499/16499 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 16500/16500 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16501/16501 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16502/16502 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16503/16503 [00:01<00:00,  1.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 16504/16504 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16505/16505 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16506/16506 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16507/16507 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16508/16508 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16509/16509 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16510/16510 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16511/16511 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16512/16512 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16513/16513 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16514/16514 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16515/16515 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16516/16516 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16517/16517 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16518/16518 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16519/16519 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16520/16520 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16521/16521 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16522/16522 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16523/16523 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16524/16524 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16525/16525 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16526/16526 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16527/16527 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16528/16528 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16529/16529 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16530/16530 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16531/16531 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16532/16532 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16533/16533 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16534/16534 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16535/16535 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16536/16536 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16537/16537 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16538/16538 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16539/16539 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16540/16540 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16541/16541 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16542/16542 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16543/16543 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16544/16544 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16545/16545 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16546/16546 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16547/16547 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16548/16548 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16549/16549 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16550/16550 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16551/16551 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16552/16552 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16553/16553 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 16554/16554 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16555/16555 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16556/16556 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16557/16557 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16558/16558 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16559/16559 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16560/16560 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16561/16561 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16562/16562 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16563/16563 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16564/16564 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 16565/16565 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16566/16566 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16567/16567 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16568/16568 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16569/16569 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16570/16570 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16571/16571 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16572/16572 [00:01<00:00,  1.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 16573/16573 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16574/16574 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 16575/16575 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16576/16576 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16577/16577 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16578/16578 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16579/16579 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16580/16580 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16581/16581 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16582/16582 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16583/16583 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16584/16584 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16585/16585 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16586/16586 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16587/16587 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16588/16588 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16589/16589 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16590/16590 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16591/16591 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16592/16592 [00:01<00:00,  1.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 16593/16593 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16594/16594 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16595/16595 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16596/16596 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 16597/16597 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16598/16598 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16599/16599 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16600/16600 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16601/16601 [00:01<00:00,  1.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16602/16602 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16603/16603 [00:01<00:00,  1.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 16604/16604 [00:01<00:00,  1.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 16605/16605 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16606/16606 [00:01<00:00,  1.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 16607/16607 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 16608/16608 [00:01<00:00,  1.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 16609/16609 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16610/16610 [00:01<00:00,  1.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 16611/16611 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 16612/16612 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 16613/16613 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 16614/16614 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 16615/16615 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 16616/16616 [00:01<00:00,  1.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 16617/16617 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 16618/16618 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 16619/16619 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 16620/16620 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 16621/16621 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 16622/16622 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 16623/16623 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 16624/16624 [00:01<00:00,  1.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 16625/16625 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 16626/16626 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 16627/16627 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 16628/16628 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 16629/16629 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 16630/16630 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 16631/16631 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 16632/16632 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 16633/16633 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 16634/16634 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 16635/16635 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 16636/16636 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 16637/16637 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 16638/16638 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 16639/16639 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 16640/16640 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 16641/16641 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 16642/16642 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 16643/16643 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 16644/16644 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 16645/16645 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 16646/16646 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 16647/16647 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 16648/16648 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 16649/16649 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 16650/16650 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 16651/16651 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 16652/16652 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 16653/16653 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 16654/16654 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 16655/16655 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 16656/16656 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 16657/16657 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 16658/16658 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 16659/16659 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 16660/16660 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 16661/16661 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 16662/16662 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 16663/16663 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 16664/16664 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 16665/16665 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 16666/16666 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 16667/16667 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 16668/16668 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 16669/16669 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 16670/16670 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16671/16671 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 16672/16672 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16673/16673 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 16674/16674 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16675/16675 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16676/16676 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 16677/16677 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16678/16678 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16679/16679 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16680/16680 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 16681/16681 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16682/16682 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 16683/16683 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16684/16684 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 16685/16685 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 16686/16686 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 16687/16687 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 16688/16688 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16689/16689 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 16690/16690 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16691/16691 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16692/16692 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 16693/16693 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16694/16694 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 16695/16695 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 16696/16696 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16697/16697 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 16698/16698 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16699/16699 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 16700/16700 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16701/16701 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 16702/16702 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16703/16703 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16704/16704 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16705/16705 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 16706/16706 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16707/16707 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 16708/16708 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16709/16709 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16710/16710 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16711/16711 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16712/16712 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 16713/16713 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 16714/16714 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16715/16715 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 16716/16716 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 16717/16717 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 16718/16718 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16719/16719 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 16720/16720 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 16721/16721 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16722/16722 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 16723/16723 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16724/16724 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16725/16725 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 16726/16726 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 16727/16727 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 16728/16728 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16729/16729 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16730/16730 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16731/16731 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 16732/16732 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 16733/16733 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16734/16734 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16735/16735 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16736/16736 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16737/16737 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 16738/16738 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16739/16739 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16740/16740 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 16741/16741 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 16742/16742 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 16743/16743 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16744/16744 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16745/16745 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 16746/16746 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 16747/16747 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16748/16748 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 16749/16749 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 16750/16750 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16751/16751 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 16752/16752 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16753/16753 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 16754/16754 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16755/16755 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 16756/16756 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16757/16757 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 16758/16758 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 16759/16759 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 16760/16760 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16761/16761 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16762/16762 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16763/16763 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16764/16764 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16765/16765 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16766/16766 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 16767/16767 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16768/16768 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 16769/16769 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16770/16770 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 16771/16771 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16772/16772 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 16773/16773 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16774/16774 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 16775/16775 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16776/16776 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 16777/16777 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 16778/16778 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 16779/16779 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 16780/16780 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 16781/16781 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16782/16782 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16783/16783 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 16784/16784 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16785/16785 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 16786/16786 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16787/16787 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 16788/16788 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 16789/16789 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16790/16790 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16791/16791 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16792/16792 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 16793/16793 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 16794/16794 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 16795/16795 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16796/16796 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 16797/16797 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 16798/16798 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 16799/16799 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 16800/16800 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 16801/16801 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 16802/16802 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 16803/16803 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 16804/16804 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16805/16805 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 16806/16806 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 16807/16807 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16808/16808 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 16809/16809 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 16810/16810 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 16811/16811 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 16812/16812 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 16813/16813 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 16814/16814 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 16815/16815 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 16816/16816 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 16817/16817 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 16818/16818 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 16819/16819 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 16820/16820 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 16821/16821 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 16822/16822 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 16823/16823 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 16824/16824 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 16825/16825 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 16826/16826 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 16827/16827 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 16828/16828 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 16829/16829 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 16830/16830 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 16831/16831 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 16832/16832 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 16833/16833 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 16834/16834 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 16835/16835 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 16836/16836 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 16837/16837 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 16838/16838 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 16839/16839 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 16840/16840 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 16841/16841 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 16842/16842 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 16843/16843 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 16844/16844 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 16845/16845 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 16846/16846 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 16847/16847 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 16848/16848 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 16849/16849 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 16850/16850 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 16851/16851 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 16852/16852 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 16853/16853 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 16854/16854 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 16855/16855 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 16856/16856 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 16857/16857 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 16858/16858 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 16859/16859 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 16860/16860 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 16861/16861 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 16862/16862 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 16863/16863 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 16864/16864 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 16865/16865 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 16866/16866 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 16867/16867 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 16868/16868 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 16869/16869 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 16870/16870 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 16871/16871 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 16872/16872 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 16873/16873 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 16874/16874 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 16875/16875 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 16876/16876 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 16877/16877 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 16878/16878 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 16879/16879 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 16880/16880 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 16881/16881 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 16882/16882 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 16883/16883 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 16884/16884 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 16885/16885 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 16886/16886 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 16887/16887 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 16888/16888 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 16889/16889 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 16890/16890 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 16891/16891 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 16892/16892 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 16893/16893 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 16894/16894 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 16895/16895 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 16896/16896 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 16897/16897 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 16898/16898 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 16899/16899 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 16900/16900 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 16901/16901 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 16902/16902 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 16903/16903 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 16904/16904 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 16905/16905 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 16906/16906 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 16907/16907 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 16908/16908 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 16909/16909 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 16910/16910 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 16911/16911 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 16912/16912 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 16913/16913 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 16914/16914 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 16915/16915 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 16916/16916 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 16917/16917 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 16918/16918 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 16919/16919 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 16920/16920 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 16921/16921 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 16922/16922 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 16923/16923 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 16924/16924 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 16925/16925 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 16926/16926 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 16927/16927 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 16928/16928 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 16929/16929 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 16930/16930 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 16931/16931 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 16932/16932 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 16933/16933 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 16934/16934 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 16935/16935 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 16936/16936 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 16937/16937 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 16938/16938 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 16939/16939 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 16940/16940 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 16941/16941 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 16942/16942 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 16943/16943 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 16944/16944 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 16945/16945 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 16946/16946 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 16947/16947 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 16948/16948 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 16949/16949 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 16950/16950 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 16951/16951 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 16952/16952 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 16953/16953 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 16954/16954 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 16955/16955 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 16956/16956 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 16957/16957 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 16958/16958 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 16959/16959 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 16960/16960 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 16961/16961 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 16962/16962 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 16963/16963 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 16964/16964 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 16965/16965 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 16966/16966 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 16967/16967 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 16968/16968 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 16969/16969 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 16970/16970 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 16971/16971 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 16972/16972 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 16973/16973 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 16974/16974 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 16975/16975 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 16976/16976 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 16977/16977 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 16978/16978 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 16979/16979 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 16980/16980 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 16981/16981 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 16982/16982 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 16983/16983 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 16984/16984 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 16985/16985 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 16986/16986 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 16987/16987 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 16988/16988 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 16989/16989 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 16990/16990 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 16991/16991 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 16992/16992 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 16993/16993 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 16994/16994 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 16995/16995 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 16996/16996 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 16997/16997 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 16998/16998 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 16999/16999 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 17000/17000 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 17001/17001 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 17002/17002 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 17003/17003 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 17004/17004 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 17005/17005 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 17006/17006 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 17007/17007 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 17008/17008 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 17009/17009 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 17010/17010 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 17011/17011 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 17012/17012 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 17013/17013 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 17014/17014 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 17015/17015 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 17016/17016 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 17017/17017 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 17018/17018 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 17019/17019 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 17020/17020 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 17021/17021 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 17022/17022 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 17023/17023 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 17024/17024 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 17025/17025 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 17026/17026 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 17027/17027 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 17028/17028 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 17029/17029 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 17030/17030 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 17031/17031 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 17032/17032 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 17033/17033 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 17034/17034 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 17035/17035 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 17036/17036 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 17037/17037 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 17038/17038 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 17039/17039 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 17040/17040 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 17041/17041 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 17042/17042 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 17043/17043 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 17044/17044 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 17045/17045 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 17046/17046 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 17047/17047 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 17048/17048 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 17049/17049 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 17050/17050 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 17051/17051 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 17052/17052 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 17053/17053 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 17054/17054 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 17055/17055 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 17056/17056 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 17057/17057 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 17058/17058 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 17059/17059 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 17060/17060 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 17061/17061 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 17062/17062 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 17063/17063 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 17064/17064 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 17065/17065 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 17066/17066 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 17067/17067 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 17068/17068 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 17069/17069 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 17070/17070 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 17071/17071 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 17072/17072 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 17073/17073 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 17074/17074 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 17075/17075 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 17076/17076 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 17077/17077 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 17078/17078 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 17079/17079 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 17080/17080 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 17081/17081 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 17082/17082 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 17083/17083 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 17084/17084 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 17085/17085 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 17086/17086 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 17087/17087 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 17088/17088 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 17089/17089 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 17090/17090 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 17091/17091 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 17092/17092 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 17093/17093 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 17094/17094 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 17095/17095 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 17096/17096 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 17097/17097 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 17098/17098 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 17099/17099 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 17100/17100 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 17101/17101 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 17102/17102 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 17103/17103 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 17104/17104 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 17105/17105 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 17106/17106 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 17107/17107 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 17108/17108 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 17109/17109 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 17110/17110 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 17111/17111 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 17112/17112 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 17113/17113 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 17114/17114 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 17115/17115 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 17116/17116 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 17117/17117 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 17118/17118 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 17119/17119 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 17120/17120 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 17121/17121 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 17122/17122 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 17123/17123 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 17124/17124 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 17125/17125 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 17126/17126 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 17127/17127 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 17128/17128 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 17129/17129 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 17130/17130 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 17131/17131 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 17132/17132 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 17133/17133 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 17134/17134 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 17135/17135 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 17136/17136 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 17137/17137 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 17138/17138 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 17139/17139 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 17140/17140 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 17141/17141 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 17142/17142 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 17143/17143 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 17144/17144 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 17145/17145 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 17146/17146 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 17147/17147 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 17148/17148 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 17149/17149 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 17150/17150 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 17151/17151 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 17152/17152 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 17153/17153 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 17154/17154 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 17155/17155 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 17156/17156 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 17157/17157 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 17158/17158 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 17159/17159 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 17160/17160 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 17161/17161 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 17162/17162 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 17163/17163 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 17164/17164 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 17165/17165 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 17166/17166 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 17167/17167 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 17168/17168 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 17169/17169 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 17170/17170 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 17171/17171 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 17172/17172 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 17173/17173 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 17174/17174 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 17175/17175 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 17176/17176 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 17177/17177 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 17178/17178 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 17179/17179 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 17180/17180 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 17181/17181 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 17182/17182 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 17183/17183 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 17184/17184 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 17185/17185 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17186/17186 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 17187/17187 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 17188/17188 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 17189/17189 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17190/17190 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 17191/17191 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 17192/17192 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 17193/17193 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17194/17194 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 17195/17195 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 17196/17196 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17197/17197 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 17198/17198 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 17199/17199 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 17200/17200 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 17201/17201 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 17202/17202 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 17203/17203 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17204/17204 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17205/17205 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17206/17206 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17207/17207 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 17208/17208 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17209/17209 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17210/17210 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17211/17211 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17212/17212 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 17213/17213 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 17214/17214 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17215/17215 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17216/17216 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 17217/17217 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17218/17218 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17219/17219 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17220/17220 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 17221/17221 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17222/17222 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17223/17223 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17224/17224 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17225/17225 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17226/17226 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17227/17227 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17228/17228 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17229/17229 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17230/17230 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17231/17231 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17232/17232 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17233/17233 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17234/17234 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17235/17235 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17236/17236 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17237/17237 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17238/17238 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17239/17239 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 17240/17240 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17241/17241 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17242/17242 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17243/17243 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17244/17244 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 17245/17245 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 17246/17246 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 17247/17247 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 17248/17248 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 17249/17249 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 17250/17250 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 17251/17251 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 17252/17252 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17253/17253 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17254/17254 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17255/17255 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17256/17256 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17257/17257 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17258/17258 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 17259/17259 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17260/17260 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17261/17261 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17262/17262 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17263/17263 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17264/17264 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17265/17265 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17266/17266 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17267/17267 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17268/17268 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17269/17269 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17270/17270 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17271/17271 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17272/17272 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17273/17273 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17274/17274 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 17275/17275 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 17276/17276 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17277/17277 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17278/17278 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17279/17279 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17280/17280 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17281/17281 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17282/17282 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17283/17283 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17284/17284 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17285/17285 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 17286/17286 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17287/17287 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17288/17288 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17289/17289 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17290/17290 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 17291/17291 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17292/17292 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 17293/17293 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 17294/17294 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 17295/17295 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 17296/17296 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 17297/17297 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 17298/17298 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 17299/17299 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 17300/17300 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 17301/17301 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 17302/17302 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 17303/17303 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 17304/17304 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 17305/17305 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 17306/17306 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 17307/17307 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 17308/17308 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 17309/17309 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 17310/17310 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 17311/17311 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 17312/17312 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 17313/17313 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 17314/17314 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 17315/17315 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 17316/17316 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 17317/17317 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 17318/17318 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 17319/17319 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 17320/17320 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 17321/17321 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 17322/17322 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 17323/17323 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 17324/17324 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 17325/17325 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 17326/17326 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 17327/17327 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 17328/17328 [00:01<00:00,  1.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 17329/17329 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 17330/17330 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17331/17331 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 17332/17332 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 17333/17333 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 17334/17334 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17335/17335 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 17336/17336 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 17337/17337 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 17338/17338 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 17339/17339 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17340/17340 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 17341/17341 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 17342/17342 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 17343/17343 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 17344/17344 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 17345/17345 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 17346/17346 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17347/17347 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 17348/17348 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17349/17349 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 17350/17350 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 17351/17351 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 17352/17352 [00:01<00:00,  1.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 17353/17353 [00:01<00:00,  1.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 17354/17354 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 17355/17355 [00:01<00:00,  1.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 17356/17356 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 17357/17357 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17358/17358 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17359/17359 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17360/17360 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 17361/17361 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17362/17362 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17363/17363 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17364/17364 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17365/17365 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17366/17366 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17367/17367 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 17368/17368 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17369/17369 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17370/17370 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17371/17371 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17372/17372 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17373/17373 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17374/17374 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17375/17375 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17376/17376 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 17377/17377 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17378/17378 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17379/17379 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17380/17380 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17381/17381 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 17382/17382 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17383/17383 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17384/17384 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17385/17385 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 17386/17386 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17387/17387 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 17388/17388 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17389/17389 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 17390/17390 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17391/17391 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17392/17392 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17393/17393 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17394/17394 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17395/17395 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17396/17396 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17397/17397 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 17398/17398 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17399/17399 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17400/17400 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17401/17401 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17402/17402 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17403/17403 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17404/17404 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17405/17405 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17406/17406 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17407/17407 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17408/17408 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17409/17409 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17410/17410 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17411/17411 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17412/17412 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17413/17413 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17414/17414 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17415/17415 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17416/17416 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17417/17417 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17418/17418 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17419/17419 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 17420/17420 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17421/17421 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17422/17422 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17423/17423 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17424/17424 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17425/17425 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17426/17426 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17427/17427 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17428/17428 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17429/17429 [00:01<00:00,  1.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 17430/17430 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17431/17431 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17432/17432 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 17433/17433 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17434/17434 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17435/17435 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17436/17436 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17437/17437 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17438/17438 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17439/17439 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17440/17440 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17441/17441 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 17442/17442 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17443/17443 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17444/17444 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17445/17445 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17446/17446 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17447/17447 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17448/17448 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17449/17449 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17450/17450 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17451/17451 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 17452/17452 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17453/17453 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17454/17454 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17455/17455 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17456/17456 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17457/17457 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17458/17458 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17459/17459 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17460/17460 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17461/17461 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 17462/17462 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17463/17463 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17464/17464 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17465/17465 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17466/17466 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17467/17467 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17468/17468 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17469/17469 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17470/17470 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17471/17471 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17472/17472 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17473/17473 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17474/17474 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17475/17475 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17476/17476 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17477/17477 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17478/17478 [00:01<00:00,  1.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 17479/17479 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17480/17480 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17481/17481 [00:01<00:00,  1.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 17482/17482 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17483/17483 [00:01<00:00,  1.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 17484/17484 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17485/17485 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17486/17486 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 17487/17487 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17488/17488 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17489/17489 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17490/17490 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17491/17491 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17492/17492 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17493/17493 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17494/17494 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17495/17495 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17496/17496 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17497/17497 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17498/17498 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17499/17499 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17500/17500 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17501/17501 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17502/17502 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17503/17503 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17504/17504 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17505/17505 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17506/17506 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17507/17507 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17508/17508 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17509/17509 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17510/17510 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17511/17511 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17512/17512 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17513/17513 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 17514/17514 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17515/17515 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17516/17516 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17517/17517 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17518/17518 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17519/17519 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17520/17520 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17521/17521 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17522/17522 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17523/17523 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17524/17524 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17525/17525 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17526/17526 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17527/17527 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17528/17528 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17529/17529 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17530/17530 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17531/17531 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17532/17532 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17533/17533 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 17534/17534 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17535/17535 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17536/17536 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17537/17537 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17538/17538 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17539/17539 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17540/17540 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17541/17541 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17542/17542 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17543/17543 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17544/17544 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17545/17545 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17546/17546 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17547/17547 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17548/17548 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17549/17549 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17550/17550 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17551/17551 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17552/17552 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17553/17553 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17554/17554 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17555/17555 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17556/17556 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17557/17557 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17558/17558 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17559/17559 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17560/17560 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17561/17561 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17562/17562 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17563/17563 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17564/17564 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17565/17565 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17566/17566 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17567/17567 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17568/17568 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17569/17569 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17570/17570 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17571/17571 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17572/17572 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17573/17573 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17574/17574 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17575/17575 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17576/17576 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17577/17577 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17578/17578 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17579/17579 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17580/17580 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17581/17581 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17582/17582 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17583/17583 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17584/17584 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17585/17585 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17586/17586 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17587/17587 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17588/17588 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17589/17589 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17590/17590 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17591/17591 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17592/17592 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17593/17593 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17594/17594 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17595/17595 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17596/17596 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17597/17597 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17598/17598 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17599/17599 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17600/17600 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17601/17601 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17602/17602 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17603/17603 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17604/17604 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17605/17605 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17606/17606 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17607/17607 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17608/17608 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17609/17609 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17610/17610 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17611/17611 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17612/17612 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17613/17613 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17614/17614 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17615/17615 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17616/17616 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17617/17617 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17618/17618 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17619/17619 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17620/17620 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17621/17621 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17622/17622 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17623/17623 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17624/17624 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17625/17625 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17626/17626 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17627/17627 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17628/17628 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17629/17629 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17630/17630 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17631/17631 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17632/17632 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17633/17633 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17634/17634 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17635/17635 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17636/17636 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17637/17637 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17638/17638 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17639/17639 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17640/17640 [00:01<00:00,  1.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 17641/17641 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17642/17642 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17643/17643 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17644/17644 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17645/17645 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17646/17646 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17647/17647 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17648/17648 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17649/17649 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17650/17650 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17651/17651 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17652/17652 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17653/17653 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17654/17654 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17655/17655 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17656/17656 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17657/17657 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17658/17658 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17659/17659 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17660/17660 [00:01<00:00,  1.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 17661/17661 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17662/17662 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17663/17663 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17664/17664 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17665/17665 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17666/17666 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17667/17667 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17668/17668 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17669/17669 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17670/17670 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17671/17671 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17672/17672 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17673/17673 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17674/17674 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17675/17675 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17676/17676 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17677/17677 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17678/17678 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17679/17679 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17680/17680 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17681/17681 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17682/17682 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17683/17683 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17684/17684 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17685/17685 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17686/17686 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17687/17687 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17688/17688 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17689/17689 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17690/17690 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17691/17691 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17692/17692 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17693/17693 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17694/17694 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17695/17695 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17696/17696 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17697/17697 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17698/17698 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17699/17699 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17700/17700 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17701/17701 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17702/17702 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17703/17703 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17704/17704 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17705/17705 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17706/17706 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17707/17707 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17708/17708 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17709/17709 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17710/17710 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17711/17711 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17712/17712 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17713/17713 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17714/17714 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17715/17715 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17716/17716 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17717/17717 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17718/17718 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17719/17719 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17720/17720 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17721/17721 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17722/17722 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17723/17723 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17724/17724 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17725/17725 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17726/17726 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 17727/17727 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 17728/17728 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17729/17729 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17730/17730 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17731/17731 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17732/17732 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17733/17733 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17734/17734 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17735/17735 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17736/17736 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17737/17737 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17738/17738 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17739/17739 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17740/17740 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17741/17741 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 17742/17742 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17743/17743 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17744/17744 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17745/17745 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 17746/17746 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17747/17747 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17748/17748 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17749/17749 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17750/17750 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17751/17751 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17752/17752 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17753/17753 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17754/17754 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17755/17755 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17756/17756 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17757/17757 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17758/17758 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17759/17759 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17760/17760 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17761/17761 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17762/17762 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17763/17763 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17764/17764 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17765/17765 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17766/17766 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17767/17767 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17768/17768 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17769/17769 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17770/17770 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17771/17771 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17772/17772 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17773/17773 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17774/17774 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17775/17775 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17776/17776 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17777/17777 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17778/17778 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17779/17779 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17780/17780 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 17781/17781 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17782/17782 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17783/17783 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17784/17784 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17785/17785 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17786/17786 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17787/17787 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 17788/17788 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17789/17789 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17790/17790 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17791/17791 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17792/17792 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17793/17793 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17794/17794 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17795/17795 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17796/17796 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17797/17797 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17798/17798 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17799/17799 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17800/17800 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 17801/17801 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17802/17802 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17803/17803 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17804/17804 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17805/17805 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17806/17806 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17807/17807 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 17808/17808 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17809/17809 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17810/17810 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17811/17811 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17812/17812 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 17813/17813 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17814/17814 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17815/17815 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17816/17816 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17817/17817 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17818/17818 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17819/17819 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17820/17820 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17821/17821 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17822/17822 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17823/17823 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17824/17824 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17825/17825 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17826/17826 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17827/17827 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17828/17828 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17829/17829 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17830/17830 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17831/17831 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17832/17832 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17833/17833 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17834/17834 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17835/17835 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17836/17836 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 17837/17837 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 17838/17838 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 17839/17839 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 17840/17840 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 17841/17841 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 17842/17842 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17843/17843 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17844/17844 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17845/17845 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17846/17846 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17847/17847 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17848/17848 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17849/17849 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17850/17850 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17851/17851 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17852/17852 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17853/17853 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17854/17854 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17855/17855 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17856/17856 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17857/17857 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 17858/17858 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17859/17859 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17860/17860 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17861/17861 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17862/17862 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17863/17863 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17864/17864 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17865/17865 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17866/17866 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17867/17867 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17868/17868 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17869/17869 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17870/17870 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17871/17871 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17872/17872 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17873/17873 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17874/17874 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17875/17875 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 17876/17876 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17877/17877 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17878/17878 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17879/17879 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17880/17880 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17881/17881 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17882/17882 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17883/17883 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17884/17884 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17885/17885 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17886/17886 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17887/17887 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17888/17888 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17889/17889 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17890/17890 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17891/17891 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17892/17892 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17893/17893 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17894/17894 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17895/17895 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17896/17896 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17897/17897 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17898/17898 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17899/17899 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17900/17900 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17901/17901 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17902/17902 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17903/17903 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17904/17904 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17905/17905 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17906/17906 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17907/17907 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17908/17908 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17909/17909 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17910/17910 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17911/17911 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17912/17912 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17913/17913 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17914/17914 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17915/17915 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17916/17916 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17917/17917 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17918/17918 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 17919/17919 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17920/17920 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17921/17921 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17922/17922 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17923/17923 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17924/17924 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17925/17925 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17926/17926 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17927/17927 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17928/17928 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17929/17929 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17930/17930 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17931/17931 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17932/17932 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17933/17933 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17934/17934 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17935/17935 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17936/17936 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17937/17937 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17938/17938 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17939/17939 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17940/17940 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17941/17941 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17942/17942 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17943/17943 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17944/17944 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17945/17945 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 17946/17946 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17947/17947 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 17948/17948 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17949/17949 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17950/17950 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17951/17951 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17952/17952 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17953/17953 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17954/17954 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17955/17955 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17956/17956 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17957/17957 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17958/17958 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17959/17959 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17960/17960 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17961/17961 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17962/17962 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17963/17963 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 17964/17964 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17965/17965 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17966/17966 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 17967/17967 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17968/17968 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 17969/17969 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 17970/17970 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17971/17971 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 17972/17972 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17973/17973 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 17974/17974 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 17975/17975 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17976/17976 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 17977/17977 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 17978/17978 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 17979/17979 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 17980/17980 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 17981/17981 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 17982/17982 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 17983/17983 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 17984/17984 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 17985/17985 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 17986/17986 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 17987/17987 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 17988/17988 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 17989/17989 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 17990/17990 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 17991/17991 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 17992/17992 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 17993/17993 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 17994/17994 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 17995/17995 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 17996/17996 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 17997/17997 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 17998/17998 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 17999/17999 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18000/18000 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18001/18001 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18002/18002 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18003/18003 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18004/18004 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18005/18005 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18006/18006 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18007/18007 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18008/18008 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18009/18009 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18010/18010 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18011/18011 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18012/18012 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18013/18013 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18014/18014 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18015/18015 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18016/18016 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18017/18017 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18018/18018 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18019/18019 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18020/18020 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18021/18021 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18022/18022 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18023/18023 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18024/18024 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18025/18025 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18026/18026 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 18027/18027 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18028/18028 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18029/18029 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18030/18030 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18031/18031 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18032/18032 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18033/18033 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18034/18034 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18035/18035 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18036/18036 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18037/18037 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18038/18038 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18039/18039 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18040/18040 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18041/18041 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18042/18042 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18043/18043 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18044/18044 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18045/18045 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18046/18046 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18047/18047 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18048/18048 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18049/18049 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18050/18050 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 18051/18051 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 18052/18052 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18053/18053 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18054/18054 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18055/18055 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18056/18056 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18057/18057 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18058/18058 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18059/18059 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 18060/18060 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18061/18061 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18062/18062 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18063/18063 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18064/18064 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18065/18065 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18066/18066 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18067/18067 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18068/18068 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18069/18069 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18070/18070 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18071/18071 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18072/18072 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18073/18073 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18074/18074 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18075/18075 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 18076/18076 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18077/18077 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18078/18078 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18079/18079 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18080/18080 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18081/18081 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18082/18082 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 18083/18083 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18084/18084 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18085/18085 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18086/18086 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18087/18087 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18088/18088 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18089/18089 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18090/18090 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18091/18091 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18092/18092 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18093/18093 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18094/18094 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18095/18095 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18096/18096 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18097/18097 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18098/18098 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18099/18099 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18100/18100 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18101/18101 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18102/18102 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18103/18103 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18104/18104 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18105/18105 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18106/18106 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18107/18107 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 18108/18108 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18109/18109 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18110/18110 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18111/18111 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18112/18112 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18113/18113 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18114/18114 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18115/18115 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18116/18116 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18117/18117 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18118/18118 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18119/18119 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18120/18120 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18121/18121 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18122/18122 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18123/18123 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18124/18124 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18125/18125 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18126/18126 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18127/18127 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18128/18128 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18129/18129 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18130/18130 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18131/18131 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18132/18132 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18133/18133 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18134/18134 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18135/18135 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18136/18136 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18137/18137 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18138/18138 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18139/18139 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 18140/18140 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18141/18141 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18142/18142 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18143/18143 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18144/18144 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18145/18145 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18146/18146 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18147/18147 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18148/18148 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18149/18149 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18150/18150 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18151/18151 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18152/18152 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 18153/18153 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18154/18154 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18155/18155 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18156/18156 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 18157/18157 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18158/18158 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18159/18159 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18160/18160 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18161/18161 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18162/18162 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18163/18163 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 18164/18164 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18165/18165 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18166/18166 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18167/18167 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18168/18168 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18169/18169 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18170/18170 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18171/18171 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18172/18172 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18173/18173 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18174/18174 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18175/18175 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18176/18176 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18177/18177 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18178/18178 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18179/18179 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18180/18180 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18181/18181 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18182/18182 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18183/18183 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18184/18184 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18185/18185 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18186/18186 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18187/18187 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18188/18188 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 18189/18189 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18190/18190 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18191/18191 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18192/18192 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18193/18193 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18194/18194 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18195/18195 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18196/18196 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18197/18197 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18198/18198 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18199/18199 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18200/18200 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18201/18201 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18202/18202 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 18203/18203 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 18204/18204 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18205/18205 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18206/18206 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18207/18207 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18208/18208 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18209/18209 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18210/18210 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18211/18211 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18212/18212 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18213/18213 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18214/18214 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18215/18215 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18216/18216 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 18217/18217 [00:06<00:00,  6.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 18218/18218 [00:06<00:00,  6.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 18219/18219 [00:06<00:00,  6.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 18220/18220 [00:06<00:00,  6.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 18221/18221 [00:06<00:00,  6.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 18222/18222 [00:06<00:00,  6.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 18223/18223 [00:06<00:00,  6.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 18224/18224 [00:06<00:00,  6.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 18225/18225 [00:06<00:00,  6.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 18226/18226 [00:06<00:00,  6.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 18227/18227 [00:07<00:00,  7.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 18228/18228 [00:06<00:00,  6.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 18229/18229 [00:06<00:00,  6.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 18230/18230 [00:06<00:00,  6.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 18231/18231 [00:06<00:00,  6.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 18232/18232 [00:06<00:00,  6.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 18233/18233 [00:06<00:00,  6.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18234/18234 [00:06<00:00,  6.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 18235/18235 [00:06<00:00,  6.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 18236/18236 [00:06<00:00,  6.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 18237/18237 [00:05<00:00,  5.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18238/18238 [00:07<00:00,  7.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 18239/18239 [00:06<00:00,  6.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 18240/18240 [00:06<00:00,  6.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 18241/18241 [00:06<00:00,  6.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 18242/18242 [00:05<00:00,  5.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 18243/18243 [00:05<00:00,  5.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 18244/18244 [00:05<00:00,  5.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 18245/18245 [00:05<00:00,  5.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18246/18246 [00:05<00:00,  5.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18247/18247 [00:05<00:00,  6.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18248/18248 [00:07<00:00,  7.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 18249/18249 [00:06<00:00,  6.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18250/18250 [00:05<00:00,  5.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18251/18251 [00:05<00:00,  5.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 18252/18252 [00:05<00:00,  5.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 18253/18253 [00:05<00:00,  5.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18254/18254 [00:05<00:00,  5.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 18255/18255 [00:05<00:00,  5.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 18256/18256 [00:05<00:00,  5.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 18257/18257 [00:05<00:00,  5.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 18258/18258 [00:05<00:00,  5.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 18259/18259 [00:06<00:00,  6.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 18260/18260 [00:05<00:00,  5.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 18261/18261 [00:05<00:00,  5.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 18262/18262 [00:05<00:00,  5.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 18263/18263 [00:05<00:00,  5.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 18264/18264 [00:05<00:00,  5.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 18265/18265 [00:05<00:00,  5.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 18266/18266 [00:07<00:00,  7.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18267/18267 [00:06<00:00,  6.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 18268/18268 [00:05<00:00,  5.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 18269/18269 [00:05<00:00,  5.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 18270/18270 [00:06<00:00,  6.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 18271/18271 [00:05<00:00,  5.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 18272/18272 [00:05<00:00,  5.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 18273/18273 [00:05<00:00,  5.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 18274/18274 [00:05<00:00,  5.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 18275/18275 [00:05<00:00,  5.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 18276/18276 [00:05<00:00,  5.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 18277/18277 [00:05<00:00,  5.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 18278/18278 [00:05<00:00,  5.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 18279/18279 [00:05<00:00,  5.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 18280/18280 [00:05<00:00,  5.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 18281/18281 [00:06<00:00,  6.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 18282/18282 [00:05<00:00,  5.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 18283/18283 [00:05<00:00,  5.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18284/18284 [00:05<00:00,  5.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 18285/18285 [00:05<00:00,  5.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18286/18286 [00:05<00:00,  5.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 18287/18287 [00:05<00:00,  5.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 18288/18288 [00:05<00:00,  5.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 18289/18289 [00:05<00:00,  5.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 18290/18290 [00:04<00:00,  4.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18291/18291 [00:05<00:00,  5.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 18292/18292 [00:06<00:00,  6.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 18293/18293 [00:04<00:00,  4.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 18294/18294 [00:04<00:00,  4.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 18295/18295 [00:04<00:00,  4.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 18296/18296 [00:04<00:00,  4.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 18297/18297 [00:05<00:00,  5.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 18298/18298 [00:04<00:00,  4.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 18299/18299 [00:04<00:00,  4.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 18300/18300 [00:05<00:00,  5.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 18301/18301 [00:04<00:00,  4.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 18302/18302 [00:04<00:00,  4.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 18303/18303 [00:05<00:00,  5.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 18304/18304 [00:04<00:00,  4.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 18305/18305 [00:04<00:00,  4.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18306/18306 [00:04<00:00,  4.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 18307/18307 [00:04<00:00,  4.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18308/18308 [00:04<00:00,  4.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18309/18309 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18310/18310 [00:04<00:00,  4.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18311/18311 [00:04<00:00,  4.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18312/18312 [00:04<00:00,  4.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18313/18313 [00:04<00:00,  4.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18314/18314 [00:05<00:00,  5.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18315/18315 [00:04<00:00,  4.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 18316/18316 [00:04<00:00,  4.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18317/18317 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18318/18318 [00:04<00:00,  4.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18319/18319 [00:04<00:00,  4.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18320/18320 [00:04<00:00,  4.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 18321/18321 [00:04<00:00,  4.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18322/18322 [00:04<00:00,  4.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18323/18323 [00:04<00:00,  4.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 18324/18324 [00:04<00:00,  4.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 18325/18325 [00:05<00:00,  5.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18326/18326 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 18327/18327 [00:04<00:00,  4.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18328/18328 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 18329/18329 [00:04<00:00,  4.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18330/18330 [00:04<00:00,  4.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 18331/18331 [00:04<00:00,  4.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 18332/18332 [00:04<00:00,  4.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 18333/18333 [00:04<00:00,  4.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 18334/18334 [00:04<00:00,  4.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 18335/18335 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 18336/18336 [00:05<00:00,  5.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18337/18337 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 18338/18338 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 18339/18339 [00:04<00:00,  4.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 18340/18340 [00:04<00:00,  4.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 18341/18341 [00:04<00:00,  4.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 18342/18342 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 18343/18343 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 18344/18344 [00:04<00:00,  4.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 18345/18345 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 18346/18346 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 18347/18347 [00:04<00:00,  4.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 18348/18348 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 18349/18349 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 18350/18350 [00:03<00:00,  3.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 18351/18351 [00:03<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 18352/18352 [00:04<00:00,  4.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 18353/18353 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18354/18354 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18355/18355 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18356/18356 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18357/18357 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18358/18358 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18359/18359 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18360/18360 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18361/18361 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18362/18362 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18363/18363 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18364/18364 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18365/18365 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18366/18366 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18367/18367 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18368/18368 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18369/18369 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18370/18370 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18371/18371 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18372/18372 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18373/18373 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18374/18374 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18375/18375 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18376/18376 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18377/18377 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18378/18378 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18379/18379 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18380/18380 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18381/18381 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18382/18382 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18383/18383 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18384/18384 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18385/18385 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18386/18386 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18387/18387 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 18388/18388 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18389/18389 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 18390/18390 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18391/18391 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18392/18392 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18393/18393 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18394/18394 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 18395/18395 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18396/18396 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18397/18397 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18398/18398 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18399/18399 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18400/18400 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18401/18401 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18402/18402 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18403/18403 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18404/18404 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18405/18405 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18406/18406 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18407/18407 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18408/18408 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18409/18409 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18410/18410 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18411/18411 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18412/18412 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18413/18413 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18414/18414 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18415/18415 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18416/18416 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18417/18417 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18418/18418 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18419/18419 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18420/18420 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18421/18421 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18422/18422 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18423/18423 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 18424/18424 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18425/18425 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18426/18426 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18427/18427 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18428/18428 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18429/18429 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18430/18430 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18431/18431 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18432/18432 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18433/18433 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18434/18434 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18435/18435 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18436/18436 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18437/18437 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18438/18438 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18439/18439 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18440/18440 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18441/18441 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18442/18442 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18443/18443 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18444/18444 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18445/18445 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18446/18446 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18447/18447 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18448/18448 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18449/18449 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18450/18450 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18451/18451 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18452/18452 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18453/18453 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18454/18454 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18455/18455 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18456/18456 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18457/18457 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18458/18458 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18459/18459 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18460/18460 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18461/18461 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18462/18462 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18463/18463 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18464/18464 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18465/18465 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18466/18466 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18467/18467 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18468/18468 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18469/18469 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18470/18470 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18471/18471 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18472/18472 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18473/18473 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18474/18474 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18475/18475 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18476/18476 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18477/18477 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18478/18478 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18479/18479 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18480/18480 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18481/18481 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18482/18482 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18483/18483 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18484/18484 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18485/18485 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18486/18486 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 18487/18487 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18488/18488 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18489/18489 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18490/18490 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18491/18491 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18492/18492 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18493/18493 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18494/18494 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18495/18495 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18496/18496 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18497/18497 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18498/18498 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18499/18499 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 18500/18500 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 18501/18501 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 18502/18502 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18503/18503 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18504/18504 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 18505/18505 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18506/18506 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18507/18507 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18508/18508 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18509/18509 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18510/18510 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18511/18511 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18512/18512 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18513/18513 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18514/18514 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18515/18515 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18516/18516 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18517/18517 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18518/18518 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18519/18519 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18520/18520 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18521/18521 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 18522/18522 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18523/18523 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18524/18524 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18525/18525 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18526/18526 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18527/18527 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18528/18528 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18529/18529 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18530/18530 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18531/18531 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18532/18532 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18533/18533 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18534/18534 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18535/18535 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18536/18536 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18537/18537 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18538/18538 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18539/18539 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18540/18540 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18541/18541 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18542/18542 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18543/18543 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18544/18544 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18545/18545 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18546/18546 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 18547/18547 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18548/18548 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18549/18549 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18550/18550 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18551/18551 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18552/18552 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18553/18553 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18554/18554 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18555/18555 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18556/18556 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18557/18557 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18558/18558 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18559/18559 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18560/18560 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18561/18561 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18562/18562 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18563/18563 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18564/18564 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 18565/18565 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 18566/18566 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 18567/18567 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 18568/18568 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 18569/18569 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18570/18570 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18571/18571 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18572/18572 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18573/18573 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18574/18574 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18575/18575 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18576/18576 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18577/18577 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18578/18578 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18579/18579 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18580/18580 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18581/18581 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18582/18582 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18583/18583 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18584/18584 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18585/18585 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18586/18586 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18587/18587 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18588/18588 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18589/18589 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18590/18590 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 18591/18591 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 18592/18592 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18593/18593 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18594/18594 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18595/18595 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18596/18596 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18597/18597 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18598/18598 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18599/18599 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18600/18600 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18601/18601 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18602/18602 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18603/18603 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18604/18604 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18605/18605 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18606/18606 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18607/18607 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18608/18608 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18609/18609 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18610/18610 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18611/18611 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18612/18612 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18613/18613 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18614/18614 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18615/18615 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18616/18616 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18617/18617 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18618/18618 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18619/18619 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18620/18620 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18621/18621 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18622/18622 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18623/18623 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18624/18624 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18625/18625 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18626/18626 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18627/18627 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18628/18628 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18629/18629 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18630/18630 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18631/18631 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18632/18632 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18633/18633 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18634/18634 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18635/18635 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18636/18636 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18637/18637 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18638/18638 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18639/18639 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18640/18640 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18641/18641 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 18642/18642 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18643/18643 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18644/18644 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18645/18645 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18646/18646 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18647/18647 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18648/18648 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18649/18649 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18650/18650 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18651/18651 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18652/18652 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18653/18653 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18654/18654 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18655/18655 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18656/18656 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18657/18657 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18658/18658 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18659/18659 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18660/18660 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18661/18661 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18662/18662 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18663/18663 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18664/18664 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18665/18665 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18666/18666 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18667/18667 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18668/18668 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18669/18669 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18670/18670 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18671/18671 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18672/18672 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18673/18673 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18674/18674 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18675/18675 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18676/18676 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18677/18677 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18678/18678 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18679/18679 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18680/18680 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18681/18681 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18682/18682 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18683/18683 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18684/18684 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18685/18685 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18686/18686 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18687/18687 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18688/18688 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18689/18689 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18690/18690 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18691/18691 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18692/18692 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18693/18693 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18694/18694 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18695/18695 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18696/18696 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18697/18697 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18698/18698 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 18699/18699 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18700/18700 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18701/18701 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18702/18702 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18703/18703 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18704/18704 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18705/18705 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18706/18706 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18707/18707 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18708/18708 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18709/18709 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18710/18710 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18711/18711 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18712/18712 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18713/18713 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18714/18714 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18715/18715 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18716/18716 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18717/18717 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18718/18718 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18719/18719 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18720/18720 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18721/18721 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18722/18722 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18723/18723 [00:01<00:00,  1.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 18724/18724 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18725/18725 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18726/18726 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18727/18727 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18728/18728 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18729/18729 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18730/18730 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18731/18731 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18732/18732 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18733/18733 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18734/18734 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18735/18735 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 18736/18736 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18737/18737 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18738/18738 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18739/18739 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18740/18740 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18741/18741 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18742/18742 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18743/18743 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18744/18744 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18745/18745 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18746/18746 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18747/18747 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18748/18748 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18749/18749 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18750/18750 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18751/18751 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18752/18752 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18753/18753 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18754/18754 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18755/18755 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18756/18756 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 18757/18757 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18758/18758 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18759/18759 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18760/18760 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18761/18761 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18762/18762 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18763/18763 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 18764/18764 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18765/18765 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18766/18766 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18767/18767 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18768/18768 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18769/18769 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18770/18770 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18771/18771 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18772/18772 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18773/18773 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18774/18774 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18775/18775 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18776/18776 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18777/18777 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18778/18778 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18779/18779 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18780/18780 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18781/18781 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18782/18782 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18783/18783 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18784/18784 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18785/18785 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 18786/18786 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18787/18787 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18788/18788 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18789/18789 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18790/18790 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18791/18791 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18792/18792 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18793/18793 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18794/18794 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18795/18795 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18796/18796 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18797/18797 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18798/18798 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18799/18799 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18800/18800 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18801/18801 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18802/18802 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18803/18803 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18804/18804 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18805/18805 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18806/18806 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18807/18807 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18808/18808 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18809/18809 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18810/18810 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18811/18811 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18812/18812 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18813/18813 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18814/18814 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18815/18815 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18816/18816 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18817/18817 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 18818/18818 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18819/18819 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18820/18820 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18821/18821 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18822/18822 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18823/18823 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 18824/18824 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18825/18825 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18826/18826 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18827/18827 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18828/18828 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18829/18829 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18830/18830 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18831/18831 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18832/18832 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18833/18833 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18834/18834 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18835/18835 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18836/18836 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18837/18837 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18838/18838 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18839/18839 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 18840/18840 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 18841/18841 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18842/18842 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18843/18843 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18844/18844 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18845/18845 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18846/18846 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18847/18847 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18848/18848 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18849/18849 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18850/18850 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18851/18851 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18852/18852 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18853/18853 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18854/18854 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18855/18855 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18856/18856 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18857/18857 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18858/18858 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18859/18859 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18860/18860 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18861/18861 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18862/18862 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18863/18863 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18864/18864 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18865/18865 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18866/18866 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18867/18867 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18868/18868 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18869/18869 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18870/18870 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18871/18871 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18872/18872 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18873/18873 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18874/18874 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18875/18875 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18876/18876 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18877/18877 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18878/18878 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18879/18879 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18880/18880 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18881/18881 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18882/18882 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18883/18883 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 18884/18884 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18885/18885 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18886/18886 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18887/18887 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18888/18888 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18889/18889 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18890/18890 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18891/18891 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18892/18892 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18893/18893 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18894/18894 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 18895/18895 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18896/18896 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18897/18897 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18898/18898 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18899/18899 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18900/18900 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18901/18901 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18902/18902 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18903/18903 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18904/18904 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18905/18905 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 18906/18906 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18907/18907 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18908/18908 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18909/18909 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18910/18910 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18911/18911 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18912/18912 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18913/18913 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18914/18914 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18915/18915 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18916/18916 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18917/18917 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18918/18918 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18919/18919 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18920/18920 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18921/18921 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18922/18922 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18923/18923 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18924/18924 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18925/18925 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18926/18926 [00:01<00:00,  1.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 18927/18927 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18928/18928 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18929/18929 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18930/18930 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18931/18931 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18932/18932 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18933/18933 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18934/18934 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18935/18935 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18936/18936 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18937/18937 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18938/18938 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18939/18939 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18940/18940 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18941/18941 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18942/18942 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18943/18943 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18944/18944 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18945/18945 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18946/18946 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18947/18947 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18948/18948 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18949/18949 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18950/18950 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18951/18951 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18952/18952 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18953/18953 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18954/18954 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18955/18955 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18956/18956 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18957/18957 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18958/18958 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18959/18959 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18960/18960 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 18961/18961 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18962/18962 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18963/18963 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 18964/18964 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 18965/18965 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18966/18966 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18967/18967 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18968/18968 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18969/18969 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18970/18970 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18971/18971 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18972/18972 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18973/18973 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18974/18974 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 18975/18975 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18976/18976 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18977/18977 [00:01<00:00,  1.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 18978/18978 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18979/18979 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 18980/18980 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18981/18981 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18982/18982 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18983/18983 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18984/18984 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 18985/18985 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18986/18986 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18987/18987 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18988/18988 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18989/18989 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18990/18990 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18991/18991 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18992/18992 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 18993/18993 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 18994/18994 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18995/18995 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18996/18996 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18997/18997 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 18998/18998 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 18999/18999 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19000/19000 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19001/19001 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19002/19002 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19003/19003 [00:01<00:00,  1.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 19004/19004 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19005/19005 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19006/19006 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19007/19007 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19008/19008 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19009/19009 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19010/19010 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19011/19011 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19012/19012 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19013/19013 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19014/19014 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19015/19015 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19016/19016 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19017/19017 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19018/19018 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19019/19019 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19020/19020 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19021/19021 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19022/19022 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19023/19023 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19024/19024 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19025/19025 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19026/19026 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19027/19027 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19028/19028 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19029/19029 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19030/19030 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19031/19031 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19032/19032 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 19033/19033 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19034/19034 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19035/19035 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19036/19036 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19037/19037 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19038/19038 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19039/19039 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19040/19040 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19041/19041 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19042/19042 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19043/19043 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19044/19044 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19045/19045 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19046/19046 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19047/19047 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19048/19048 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19049/19049 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19050/19050 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19051/19051 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19052/19052 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19053/19053 [00:01<00:00,  1.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 19054/19054 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19055/19055 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19056/19056 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19057/19057 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19058/19058 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19059/19059 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19060/19060 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 19061/19061 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19062/19062 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19063/19063 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19064/19064 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19065/19065 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 19066/19066 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19067/19067 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 19068/19068 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19069/19069 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19070/19070 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19071/19071 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19072/19072 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19073/19073 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19074/19074 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 19075/19075 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19076/19076 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19077/19077 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19078/19078 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 19079/19079 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19080/19080 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19081/19081 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19082/19082 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19083/19083 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19084/19084 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19085/19085 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19086/19086 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19087/19087 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19088/19088 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19089/19089 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19090/19090 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19091/19091 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19092/19092 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19093/19093 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19094/19094 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19095/19095 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19096/19096 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 19097/19097 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19098/19098 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19099/19099 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19100/19100 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19101/19101 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19102/19102 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19103/19103 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19104/19104 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19105/19105 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19106/19106 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19107/19107 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19108/19108 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19109/19109 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19110/19110 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19111/19111 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19112/19112 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19113/19113 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 19114/19114 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 19115/19115 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19116/19116 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19117/19117 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19118/19118 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19119/19119 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19120/19120 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19121/19121 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19122/19122 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 19123/19123 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19124/19124 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19125/19125 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19126/19126 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19127/19127 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19128/19128 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19129/19129 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19130/19130 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19131/19131 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19132/19132 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19133/19133 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19134/19134 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19135/19135 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19136/19136 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 19137/19137 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19138/19138 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19139/19139 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19140/19140 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19141/19141 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19142/19142 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19143/19143 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19144/19144 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19145/19145 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19146/19146 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19147/19147 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19148/19148 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19149/19149 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19150/19150 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 19151/19151 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19152/19152 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19153/19153 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19154/19154 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19155/19155 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19156/19156 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 19157/19157 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19158/19158 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19159/19159 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19160/19160 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19161/19161 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19162/19162 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 19163/19163 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19164/19164 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19165/19165 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19166/19166 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19167/19167 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19168/19168 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19169/19169 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19170/19170 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 19171/19171 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19172/19172 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19173/19173 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19174/19174 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19175/19175 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19176/19176 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19177/19177 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19178/19178 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19179/19179 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19180/19180 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 19181/19181 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19182/19182 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19183/19183 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19184/19184 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19185/19185 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19186/19186 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19187/19187 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19188/19188 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19189/19189 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19190/19190 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 19191/19191 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19192/19192 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19193/19193 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19194/19194 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19195/19195 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19196/19196 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19197/19197 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19198/19198 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19199/19199 [00:01<00:00,  1.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 19200/19200 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19201/19201 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 19202/19202 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 19203/19203 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19204/19204 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19205/19205 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19206/19206 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19207/19207 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19208/19208 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19209/19209 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19210/19210 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19211/19211 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19212/19212 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19213/19213 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19214/19214 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19215/19215 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19216/19216 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19217/19217 [00:01<00:00,  1.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 19218/19218 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19219/19219 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19220/19220 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19221/19221 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19222/19222 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19223/19223 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19224/19224 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 19225/19225 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19226/19226 [00:01<00:00,  1.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 19227/19227 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19228/19228 [00:01<00:00,  1.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 19229/19229 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19230/19230 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19231/19231 [00:01<00:00,  1.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19232/19232 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19233/19233 [00:01<00:00,  1.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 19234/19234 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19235/19235 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 19236/19236 [00:01<00:00,  1.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19237/19237 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 19238/19238 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 19239/19239 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 19240/19240 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 19241/19241 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 19242/19242 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 19243/19243 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 19244/19244 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 19245/19245 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 19246/19246 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 19247/19247 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 19248/19248 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 19249/19249 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 19250/19250 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 19251/19251 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 19252/19252 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 19253/19253 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 19254/19254 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 19255/19255 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 19256/19256 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 19257/19257 [00:04<00:00,  4.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 19258/19258 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 19259/19259 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 19260/19260 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 19261/19261 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 19262/19262 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 19263/19263 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 19264/19264 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 19265/19265 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 19266/19266 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 19267/19267 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 19268/19268 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 19269/19269 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 19270/19270 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 19271/19271 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 19272/19272 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 19273/19273 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 19274/19274 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 19275/19275 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 19276/19276 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 19277/19277 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 19278/19278 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 19279/19279 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 19280/19280 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 19281/19281 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 19282/19282 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 19283/19283 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 19284/19284 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 19285/19285 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 19286/19286 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 19287/19287 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 19288/19288 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 19289/19289 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 19290/19290 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 19291/19291 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 19292/19292 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 19293/19293 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 19294/19294 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 19295/19295 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 19296/19296 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 19297/19297 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 19298/19298 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 19299/19299 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 19300/19300 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 19301/19301 [00:04<00:00,  4.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 19302/19302 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 19303/19303 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 19304/19304 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 19305/19305 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 19306/19306 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 19307/19307 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 19308/19308 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 19309/19309 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 19310/19310 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 19311/19311 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 19312/19312 [00:04<00:00,  4.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 19313/19313 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 19314/19314 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 19315/19315 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 19316/19316 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 19317/19317 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 19318/19318 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 19319/19319 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 19320/19320 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 19321/19321 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 19322/19322 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 19323/19323 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 19324/19324 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 19325/19325 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 19326/19326 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 19327/19327 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 19328/19328 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 19329/19329 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 19330/19330 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 19331/19331 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 19332/19332 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 19333/19333 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 19334/19334 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 19335/19335 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 19336/19336 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 19337/19337 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 19338/19338 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 19339/19339 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 19340/19340 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 19341/19341 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 19342/19342 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 19343/19343 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 19344/19344 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 19345/19345 [00:04<00:00,  4.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 19346/19346 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 19347/19347 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 19348/19348 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 19349/19349 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 19350/19350 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 19351/19351 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 19352/19352 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 19353/19353 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 19354/19354 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 19355/19355 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 19356/19356 [00:04<00:00,  4.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 19357/19357 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 19358/19358 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 19359/19359 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 19360/19360 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 19361/19361 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 19362/19362 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 19363/19363 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 19364/19364 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 19365/19365 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 19366/19366 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 19367/19367 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 19368/19368 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 19369/19369 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 19370/19370 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 19371/19371 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 19372/19372 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 19373/19373 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 19374/19374 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 19375/19375 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 19376/19376 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 19377/19377 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 19378/19378 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 19379/19379 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 19380/19380 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19381/19381 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 19382/19382 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 19383/19383 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 19384/19384 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19385/19385 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19386/19386 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 19387/19387 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19388/19388 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19389/19389 [00:04<00:00,  4.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19390/19390 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 19391/19391 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19392/19392 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 19393/19393 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19394/19394 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 19395/19395 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19396/19396 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19397/19397 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19398/19398 [00:03<00:00,  3.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19399/19399 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19400/19400 [00:04<00:00,  4.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19401/19401 [00:04<00:00,  4.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19402/19402 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19403/19403 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19404/19404 [00:04<00:00,  4.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 19405/19405 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 19406/19406 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 19407/19407 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19408/19408 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 19409/19409 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 19410/19410 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 19411/19411 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 19412/19412 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 19413/19413 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 19414/19414 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 19415/19415 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 19416/19416 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 19417/19417 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 19418/19418 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 19419/19419 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 19420/19420 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 19421/19421 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 19422/19422 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 19423/19423 [00:04<00:00,  4.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 19424/19424 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 19425/19425 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 19426/19426 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 19427/19427 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 19428/19428 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 19429/19429 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 19430/19430 [00:04<00:00,  4.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19431/19431 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 19432/19432 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 19433/19433 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 19434/19434 [00:04<00:00,  4.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 19435/19435 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 19436/19436 [00:04<00:00,  4.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 19437/19437 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 19438/19438 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 19439/19439 [00:04<00:00,  4.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 19440/19440 [00:04<00:00,  4.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 19441/19441 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 19442/19442 [00:04<00:00,  4.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 19443/19443 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 19444/19444 [00:05<00:00,  5.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 19445/19445 [00:04<00:00,  4.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 19446/19446 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 19447/19447 [00:04<00:00,  4.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 19448/19448 [00:04<00:00,  4.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 19449/19449 [00:04<00:00,  4.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 19450/19450 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 19451/19451 [00:04<00:00,  4.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 19452/19452 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 19453/19453 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 19454/19454 [00:04<00:00,  4.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 19455/19455 [00:04<00:00,  4.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 19456/19456 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 19457/19457 [00:04<00:00,  4.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 19458/19458 [00:04<00:00,  4.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 19459/19459 [00:04<00:00,  4.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19460/19460 [00:04<00:00,  4.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19461/19461 [00:05<00:00,  5.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19462/19462 [00:04<00:00,  4.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 19463/19463 [00:04<00:00,  4.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 19464/19464 [00:04<00:00,  4.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 19465/19465 [00:04<00:00,  4.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 19466/19466 [00:05<00:00,  5.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19467/19467 [00:04<00:00,  4.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 19468/19468 [00:04<00:00,  4.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 19469/19469 [00:04<00:00,  4.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19470/19470 [00:04<00:00,  4.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 19471/19471 [00:04<00:00,  4.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19472/19472 [00:04<00:00,  4.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19473/19473 [00:04<00:00,  5.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19474/19474 [00:04<00:00,  4.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19475/19475 [00:04<00:00,  4.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19476/19476 [00:04<00:00,  4.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 19477/19477 [00:05<00:00,  5.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 19478/19478 [00:04<00:00,  4.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19479/19479 [00:04<00:00,  4.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19480/19480 [00:04<00:00,  4.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19481/19481 [00:05<00:00,  5.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 19482/19482 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 19483/19483 [00:05<00:00,  5.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 19484/19484 [00:05<00:00,  5.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19485/19485 [00:05<00:00,  5.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 19486/19486 [00:05<00:00,  5.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 19487/19487 [00:05<00:00,  5.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 19488/19488 [00:05<00:00,  5.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 19489/19489 [00:05<00:00,  5.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 19490/19490 [00:05<00:00,  5.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 19491/19491 [00:05<00:00,  5.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 19492/19492 [00:05<00:00,  5.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 19493/19493 [00:05<00:00,  5.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 19494/19494 [00:05<00:00,  5.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 19495/19495 [00:05<00:00,  5.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 19496/19496 [00:05<00:00,  5.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 19497/19497 [00:01<00:00,  1.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 19498/19498 [00:01<00:00,  1.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 19499/19499 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19500/19500 [00:01<00:00,  1.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 19501/19501 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19502/19502 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19503/19503 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19504/19504 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19505/19505 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19506/19506 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19507/19507 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19508/19508 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19509/19509 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19510/19510 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 19511/19511 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19512/19512 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19513/19513 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19514/19514 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19515/19515 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19516/19516 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19517/19517 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19518/19518 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19519/19519 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19520/19520 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 19521/19521 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19522/19522 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19523/19523 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19524/19524 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19525/19525 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19526/19526 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19527/19527 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19528/19528 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19529/19529 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19530/19530 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19531/19531 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19532/19532 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19533/19533 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19534/19534 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19535/19535 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19536/19536 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19537/19537 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19538/19538 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19539/19539 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19540/19540 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19541/19541 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19542/19542 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19543/19543 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 19544/19544 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19545/19545 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19546/19546 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19547/19547 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19548/19548 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19549/19549 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19550/19550 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19551/19551 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 19552/19552 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19553/19553 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19554/19554 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19555/19555 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19556/19556 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 19557/19557 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19558/19558 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19559/19559 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 19560/19560 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19561/19561 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19562/19562 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19563/19563 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19564/19564 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19565/19565 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 19566/19566 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19567/19567 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19568/19568 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19569/19569 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19570/19570 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 19571/19571 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19572/19572 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19573/19573 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 19574/19574 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19575/19575 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19576/19576 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 19577/19577 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 19578/19578 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19579/19579 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19580/19580 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19581/19581 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19582/19582 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19583/19583 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19584/19584 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19585/19585 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19586/19586 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19587/19587 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19588/19588 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19589/19589 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19590/19590 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19591/19591 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19592/19592 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19593/19593 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19594/19594 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19595/19595 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19596/19596 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19597/19597 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19598/19598 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19599/19599 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19600/19600 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19601/19601 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19602/19602 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19603/19603 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19604/19604 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19605/19605 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19606/19606 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19607/19607 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19608/19608 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19609/19609 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19610/19610 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19611/19611 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19612/19612 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19613/19613 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19614/19614 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19615/19615 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19616/19616 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19617/19617 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19618/19618 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19619/19619 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19620/19620 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19621/19621 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19622/19622 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19623/19623 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19624/19624 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19625/19625 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19626/19626 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19627/19627 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19628/19628 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19629/19629 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19630/19630 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19631/19631 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19632/19632 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19633/19633 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19634/19634 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19635/19635 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19636/19636 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19637/19637 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19638/19638 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19639/19639 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19640/19640 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19641/19641 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19642/19642 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 19643/19643 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19644/19644 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 19645/19645 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19646/19646 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 19647/19647 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19648/19648 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19649/19649 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19650/19650 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19651/19651 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19652/19652 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19653/19653 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 19654/19654 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19655/19655 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19656/19656 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19657/19657 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19658/19658 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19659/19659 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19660/19660 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19661/19661 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19662/19662 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19663/19663 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19664/19664 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 19665/19665 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19666/19666 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19667/19667 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19668/19668 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19669/19669 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19670/19670 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19671/19671 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19672/19672 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19673/19673 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19674/19674 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19675/19675 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19676/19676 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19677/19677 [00:01<00:00,  1.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 19678/19678 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19679/19679 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19680/19680 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19681/19681 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19682/19682 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19683/19683 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19684/19684 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19685/19685 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19686/19686 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19687/19687 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19688/19688 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19689/19689 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19690/19690 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19691/19691 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19692/19692 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19693/19693 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19694/19694 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19695/19695 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19696/19696 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19697/19697 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19698/19698 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19699/19699 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19700/19700 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19701/19701 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19702/19702 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19703/19703 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19704/19704 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19705/19705 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19706/19706 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19707/19707 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19708/19708 [00:02<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19709/19709 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19710/19710 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19711/19711 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19712/19712 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19713/19713 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19714/19714 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19715/19715 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19716/19716 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19717/19717 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19718/19718 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19719/19719 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19720/19720 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19721/19721 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19722/19722 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19723/19723 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19724/19724 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19725/19725 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19726/19726 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19727/19727 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19728/19728 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19729/19729 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19730/19730 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19731/19731 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19732/19732 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19733/19733 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19734/19734 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19735/19735 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 19736/19736 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19737/19737 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19738/19738 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 19739/19739 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19740/19740 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19741/19741 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19742/19742 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19743/19743 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19744/19744 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19745/19745 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19746/19746 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19747/19747 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19748/19748 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19749/19749 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19750/19750 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19751/19751 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19752/19752 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19753/19753 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19754/19754 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19755/19755 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19756/19756 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19757/19757 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19758/19758 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19759/19759 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19760/19760 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19761/19761 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19762/19762 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19763/19763 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 19764/19764 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19765/19765 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19766/19766 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19767/19767 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19768/19768 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19769/19769 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19770/19770 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19771/19771 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19772/19772 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19773/19773 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19774/19774 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 19775/19775 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19776/19776 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19777/19777 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19778/19778 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19779/19779 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19780/19780 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19781/19781 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19782/19782 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19783/19783 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19784/19784 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19785/19785 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 19786/19786 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19787/19787 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19788/19788 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19789/19789 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19790/19790 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19791/19791 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19792/19792 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19793/19793 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19794/19794 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19795/19795 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19796/19796 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 19797/19797 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19798/19798 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19799/19799 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19800/19800 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19801/19801 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19802/19802 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19803/19803 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19804/19804 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19805/19805 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19806/19806 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19807/19807 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 19808/19808 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19809/19809 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19810/19810 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19811/19811 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19812/19812 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19813/19813 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19814/19814 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19815/19815 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19816/19816 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19817/19817 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19818/19818 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19819/19819 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19820/19820 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19821/19821 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19822/19822 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19823/19823 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19824/19824 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19825/19825 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19826/19826 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19827/19827 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19828/19828 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19829/19829 [00:02<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19830/19830 [00:01<00:00,  1.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 19831/19831 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19832/19832 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19833/19833 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19834/19834 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19835/19835 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19836/19836 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19837/19837 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19838/19838 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19839/19839 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19840/19840 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19841/19841 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19842/19842 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19843/19843 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19844/19844 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19845/19845 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19846/19846 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19847/19847 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19848/19848 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19849/19849 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19850/19850 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19851/19851 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19852/19852 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19853/19853 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19854/19854 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19855/19855 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19856/19856 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19857/19857 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19858/19858 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19859/19859 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19860/19860 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19861/19861 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19862/19862 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19863/19863 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19864/19864 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19865/19865 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19866/19866 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19867/19867 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19868/19868 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19869/19869 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19870/19870 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19871/19871 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19872/19872 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19873/19873 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19874/19874 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19875/19875 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19876/19876 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19877/19877 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19878/19878 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19879/19879 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19880/19880 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19881/19881 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19882/19882 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19883/19883 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19884/19884 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19885/19885 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19886/19886 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19887/19887 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19888/19888 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19889/19889 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19890/19890 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19891/19891 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19892/19892 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19893/19893 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19894/19894 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19895/19895 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19896/19896 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19897/19897 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19898/19898 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19899/19899 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19900/19900 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19901/19901 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19902/19902 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19903/19903 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19904/19904 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19905/19905 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19906/19906 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 19907/19907 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19908/19908 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19909/19909 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19910/19910 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19911/19911 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19912/19912 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19913/19913 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19914/19914 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19915/19915 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19916/19916 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19917/19917 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19918/19918 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19919/19919 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19920/19920 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19921/19921 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19922/19922 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19923/19923 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19924/19924 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19925/19925 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19926/19926 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19927/19927 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19928/19928 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19929/19929 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19930/19930 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19931/19931 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19932/19932 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19933/19933 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19934/19934 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19935/19935 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19936/19936 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19937/19937 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19938/19938 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 19939/19939 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19940/19940 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19941/19941 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19942/19942 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19943/19943 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19944/19944 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19945/19945 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19946/19946 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19947/19947 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19948/19948 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19949/19949 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19950/19950 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19951/19951 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19952/19952 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 19953/19953 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19954/19954 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19955/19955 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19956/19956 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 19957/19957 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19958/19958 [00:01<00:00,  1.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 19959/19959 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19960/19960 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19961/19961 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 19962/19962 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19963/19963 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19964/19964 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19965/19965 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19966/19966 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19967/19967 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19968/19968 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19969/19969 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19970/19970 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19971/19971 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19972/19972 [00:02<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 19973/19973 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19974/19974 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 19975/19975 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19976/19976 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 19977/19977 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19978/19978 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19979/19979 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19980/19980 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19981/19981 [00:01<00:00,  1.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 19982/19982 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19983/19983 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 19984/19984 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19985/19985 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 19986/19986 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 19987/19987 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 19988/19988 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 19989/19989 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 19990/19990 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 19991/19991 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 19992/19992 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 19993/19993 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 19994/19994 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 19995/19995 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 19996/19996 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 19997/19997 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 19998/19998 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 19999/19999 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20000/20000 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20001/20001 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20002/20002 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20003/20003 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20004/20004 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20005/20005 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20006/20006 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20007/20007 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20008/20008 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20009/20009 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20010/20010 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20011/20011 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20012/20012 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20013/20013 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20014/20014 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20015/20015 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20016/20016 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20017/20017 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20018/20018 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20019/20019 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20020/20020 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20021/20021 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 20022/20022 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20023/20023 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20024/20024 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20025/20025 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 20026/20026 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 20027/20027 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20028/20028 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20029/20029 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20030/20030 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20031/20031 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 20032/20032 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20033/20033 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 20034/20034 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20035/20035 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20036/20036 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20037/20037 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 20038/20038 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20039/20039 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20040/20040 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20041/20041 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 20042/20042 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20043/20043 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20044/20044 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20045/20045 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 20046/20046 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20047/20047 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20048/20048 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 20049/20049 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20050/20050 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20051/20051 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 20052/20052 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20053/20053 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20054/20054 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20055/20055 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20056/20056 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 20057/20057 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 20058/20058 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 20059/20059 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 20060/20060 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20061/20061 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 20062/20062 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 20063/20063 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 20064/20064 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20065/20065 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20066/20066 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20067/20067 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20068/20068 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20069/20069 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20070/20070 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 20071/20071 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 20072/20072 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20073/20073 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 20074/20074 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20075/20075 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 20076/20076 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20077/20077 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20078/20078 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 20079/20079 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20080/20080 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 20081/20081 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 20082/20082 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 20083/20083 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 20084/20084 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 20085/20085 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 20086/20086 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 20087/20087 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 20088/20088 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 20089/20089 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 20090/20090 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 20091/20091 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 20092/20092 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 20093/20093 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20094/20094 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20095/20095 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20096/20096 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20097/20097 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20098/20098 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20099/20099 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20100/20100 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20101/20101 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20102/20102 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20103/20103 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 20104/20104 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20105/20105 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 20106/20106 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20107/20107 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20108/20108 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20109/20109 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 20110/20110 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 20111/20111 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20112/20112 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20113/20113 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20114/20114 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 20115/20115 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20116/20116 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20117/20117 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20118/20118 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20119/20119 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20120/20120 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20121/20121 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20122/20122 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20123/20123 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 20124/20124 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20125/20125 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 20126/20126 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20127/20127 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20128/20128 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20129/20129 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 20130/20130 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20131/20131 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20132/20132 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20133/20133 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20134/20134 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20135/20135 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 20136/20136 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 20137/20137 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20138/20138 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20139/20139 [00:04<00:00,  4.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 20140/20140 [00:04<00:00,  4.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 20141/20141 [00:04<00:00,  4.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 20142/20142 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 20143/20143 [00:04<00:00,  4.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 20144/20144 [00:04<00:00,  4.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 20145/20145 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 20146/20146 [00:04<00:00,  4.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 20147/20147 [00:05<00:00,  5.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 20148/20148 [00:04<00:00,  4.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 20149/20149 [00:04<00:00,  4.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 20150/20150 [00:04<00:00,  4.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 20151/20151 [00:04<00:00,  4.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 20152/20152 [00:04<00:00,  4.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 20153/20153 [00:04<00:00,  4.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 20154/20154 [00:04<00:00,  4.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 20155/20155 [00:04<00:00,  4.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 20156/20156 [00:04<00:00,  4.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 20157/20157 [00:04<00:00,  4.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 20158/20158 [00:05<00:00,  5.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 20159/20159 [00:04<00:00,  4.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 20160/20160 [00:04<00:00,  4.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20161/20161 [00:04<00:00,  4.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 20162/20162 [00:04<00:00,  4.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 20163/20163 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 20164/20164 [00:04<00:00,  4.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 20165/20165 [00:04<00:00,  4.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 20166/20166 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 20167/20167 [00:04<00:00,  4.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 20168/20168 [00:04<00:00,  4.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 20169/20169 [00:05<00:00,  5.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 20170/20170 [00:04<00:00,  4.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 20171/20171 [00:04<00:00,  4.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 20172/20172 [00:04<00:00,  4.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 20173/20173 [00:04<00:00,  4.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 20174/20174 [00:04<00:00,  4.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 20175/20175 [00:04<00:00,  4.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 20176/20176 [00:04<00:00,  4.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 20177/20177 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 20178/20178 [00:04<00:00,  4.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 20179/20179 [00:04<00:00,  4.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 20180/20180 [00:05<00:00,  5.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 20181/20181 [00:04<00:00,  4.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 20182/20182 [00:04<00:00,  4.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 20183/20183 [00:04<00:00,  4.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 20184/20184 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 20185/20185 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 20186/20186 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 20187/20187 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 20188/20188 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 20189/20189 [00:04<00:00,  4.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 20190/20190 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 20191/20191 [00:05<00:00,  5.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 20192/20192 [00:04<00:00,  4.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 20193/20193 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 20194/20194 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 20195/20195 [00:04<00:00,  4.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 20196/20196 [00:04<00:00,  4.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 20197/20197 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 20198/20198 [00:04<00:00,  4.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 20199/20199 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 20200/20200 [00:04<00:00,  4.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 20201/20201 [00:04<00:00,  4.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 20202/20202 [00:05<00:00,  5.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 20203/20203 [00:04<00:00,  4.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 20204/20204 [00:04<00:00,  4.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 20205/20205 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 20206/20206 [00:04<00:00,  4.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 20207/20207 [00:04<00:00,  4.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 20208/20208 [00:04<00:00,  4.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 20209/20209 [00:04<00:00,  4.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 20210/20210 [00:04<00:00,  4.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 20211/20211 [00:04<00:00,  4.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 20212/20212 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 20213/20213 [00:05<00:00,  5.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 20214/20214 [00:04<00:00,  4.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 20215/20215 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 20216/20216 [00:04<00:00,  4.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 20217/20217 [00:04<00:00,  4.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 20218/20218 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 20219/20219 [00:04<00:00,  4.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 20220/20220 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 20221/20221 [00:04<00:00,  4.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 20222/20222 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 20223/20223 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 20224/20224 [00:05<00:00,  5.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 20225/20225 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 20226/20226 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 20227/20227 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 20228/20228 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 20229/20229 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 20230/20230 [00:04<00:00,  4.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 20231/20231 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 20232/20232 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 20233/20233 [00:04<00:00,  4.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 20234/20234 [00:04<00:00,  4.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 20235/20235 [00:05<00:00,  5.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 20236/20236 [00:04<00:00,  4.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 20237/20237 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 20238/20238 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 20239/20239 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 20240/20240 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 20241/20241 [00:04<00:00,  4.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20242/20242 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20243/20243 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20244/20244 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20245/20245 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20246/20246 [00:05<00:00,  5.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 20247/20247 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20248/20248 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20249/20249 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20250/20250 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 20251/20251 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20252/20252 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20253/20253 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20254/20254 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20255/20255 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20256/20256 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20257/20257 [00:05<00:00,  5.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 20258/20258 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20259/20259 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20260/20260 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20261/20261 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20262/20262 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20263/20263 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20264/20264 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 20265/20265 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 20266/20266 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 20267/20267 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20268/20268 [00:05<00:00,  5.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 20269/20269 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20270/20270 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 20271/20271 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 20272/20272 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20273/20273 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 20274/20274 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20275/20275 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20276/20276 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20277/20277 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 20278/20278 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 20279/20279 [00:05<00:00,  5.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 20280/20280 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 20281/20281 [00:04<00:00,  4.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 20282/20282 [00:04<00:00,  4.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 20283/20283 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 20284/20284 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 20285/20285 [00:04<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20286/20286 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20287/20287 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 20288/20288 [00:04<00:00,  4.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20289/20289 [00:03<00:00,  3.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20290/20290 [00:05<00:00,  5.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 20291/20291 [00:04<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20292/20292 [00:04<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20293/20293 [00:03<00:00,  3.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20294/20294 [00:04<00:00,  4.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 20295/20295 [00:04<00:00,  4.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20296/20296 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 20297/20297 [00:04<00:00,  4.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20298/20298 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 20299/20299 [00:03<00:00,  3.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20300/20300 [00:03<00:00,  3.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20301/20301 [00:05<00:00,  5.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 20302/20302 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 20303/20303 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 20304/20304 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 20305/20305 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 20306/20306 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 20307/20307 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 20308/20308 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 20309/20309 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 20310/20310 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 20311/20311 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 20312/20312 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 20313/20313 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 20314/20314 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 20315/20315 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 20316/20316 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 20317/20317 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 20318/20318 [00:03<00:00,  3.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20319/20319 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 20320/20320 [00:03<00:00,  3.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20321/20321 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 20322/20322 [00:03<00:00,  3.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 20323/20323 [00:04<00:00,  4.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20324/20324 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 20325/20325 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 20326/20326 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 20327/20327 [00:03<00:00,  3.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20328/20328 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 20329/20329 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 20330/20330 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 20331/20331 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 20332/20332 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 20333/20333 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 20334/20334 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 20335/20335 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 20336/20336 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 20337/20337 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 20338/20338 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 20339/20339 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 20340/20340 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 20341/20341 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 20342/20342 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 20343/20343 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 20344/20344 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 20345/20345 [00:04<00:00,  4.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 20346/20346 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 20347/20347 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 20348/20348 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 20349/20349 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 20350/20350 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 20351/20351 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 20352/20352 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 20353/20353 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 20354/20354 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 20355/20355 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 20356/20356 [00:04<00:00,  4.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 20357/20357 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 20358/20358 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 20359/20359 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 20360/20360 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 20361/20361 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 20362/20362 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 20363/20363 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 20364/20364 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 20365/20365 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 20366/20366 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 20367/20367 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 20368/20368 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 20369/20369 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 20370/20370 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 20371/20371 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 20372/20372 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 20373/20373 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 20374/20374 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 20375/20375 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 20376/20376 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 20377/20377 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 20378/20378 [00:04<00:00,  4.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 20379/20379 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 20380/20380 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 20381/20381 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 20382/20382 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 20383/20383 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 20384/20384 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 20385/20385 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 20386/20386 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 20387/20387 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 20388/20388 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 20389/20389 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 20390/20390 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 20391/20391 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 20392/20392 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 20393/20393 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 20394/20394 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 20395/20395 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 20396/20396 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 20397/20397 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 20398/20398 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 20399/20399 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 20400/20400 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 20401/20401 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 20402/20402 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 20403/20403 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 20404/20404 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 20405/20405 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 20406/20406 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 20407/20407 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 20408/20408 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 20409/20409 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 20410/20410 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 20411/20411 [00:04<00:00,  4.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 20412/20412 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 20413/20413 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 20414/20414 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 20415/20415 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 20416/20416 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 20417/20417 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 20418/20418 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 20419/20419 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 20420/20420 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 20421/20421 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 20422/20422 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 20423/20423 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 20424/20424 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 20425/20425 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 20426/20426 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 20427/20427 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 20428/20428 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 20429/20429 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 20430/20430 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 20431/20431 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 20432/20432 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 20433/20433 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 20434/20434 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 20435/20435 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 20436/20436 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 20437/20437 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 20438/20438 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 20439/20439 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 20440/20440 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 20441/20441 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 20442/20442 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 20443/20443 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 20444/20444 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 20445/20445 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 20446/20446 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 20447/20447 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 20448/20448 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 20449/20449 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 20450/20450 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 20451/20451 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 20452/20452 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 20453/20453 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 20454/20454 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 20455/20455 [00:04<00:00,  4.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 20456/20456 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 20457/20457 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 20458/20458 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 20459/20459 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 20460/20460 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 20461/20461 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 20462/20462 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 20463/20463 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 20464/20464 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 20465/20465 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 20466/20466 [00:04<00:00,  4.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 20467/20467 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 20468/20468 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 20469/20469 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 20470/20470 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 20471/20471 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 20472/20472 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 20473/20473 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 20474/20474 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 20475/20475 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 20476/20476 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 20477/20477 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 20478/20478 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 20479/20479 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 20480/20480 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 20481/20481 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 20482/20482 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 20483/20483 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 20484/20484 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 20485/20485 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 20486/20486 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 20487/20487 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 20488/20488 [00:05<00:00,  5.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 20489/20489 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 20490/20490 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 20491/20491 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 20492/20492 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 20493/20493 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 20494/20494 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 20495/20495 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 20496/20496 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20497/20497 [00:03<00:00,  3.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20498/20498 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 20499/20499 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 20500/20500 [00:03<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20501/20501 [00:03<00:00,  3.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20502/20502 [00:03<00:00,  3.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20503/20503 [00:04<00:00,  4.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 20504/20504 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 20505/20505 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 20506/20506 [00:03<00:00,  3.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20507/20507 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 20508/20508 [00:04<00:00,  4.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 20509/20509 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20510/20510 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 20511/20511 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 20512/20512 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 20513/20513 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 20514/20514 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20515/20515 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20516/20516 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20517/20517 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20518/20518 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20519/20519 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20520/20520 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20521/20521 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20522/20522 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20523/20523 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20524/20524 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 20525/20525 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20526/20526 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20527/20527 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20528/20528 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20529/20529 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20530/20530 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20531/20531 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20532/20532 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20533/20533 [00:01<00:00,  1.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 20534/20534 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20535/20535 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20536/20536 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20537/20537 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20538/20538 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20539/20539 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20540/20540 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20541/20541 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20542/20542 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20543/20543 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20544/20544 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 20545/20545 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20546/20546 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20547/20547 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20548/20548 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20549/20549 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20550/20550 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20551/20551 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20552/20552 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20553/20553 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20554/20554 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20555/20555 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20556/20556 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20557/20557 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20558/20558 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20559/20559 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20560/20560 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20561/20561 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 20562/20562 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20563/20563 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20564/20564 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20565/20565 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 20566/20566 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20567/20567 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20568/20568 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20569/20569 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20570/20570 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20571/20571 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20572/20572 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20573/20573 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20574/20574 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20575/20575 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20576/20576 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 20577/20577 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20578/20578 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20579/20579 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20580/20580 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20581/20581 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20582/20582 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20583/20583 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20584/20584 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20585/20585 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20586/20586 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20587/20587 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20588/20588 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20589/20589 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20590/20590 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20591/20591 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20592/20592 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20593/20593 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20594/20594 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20595/20595 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20596/20596 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20597/20597 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20598/20598 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20599/20599 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20600/20600 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20601/20601 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20602/20602 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20603/20603 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20604/20604 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20605/20605 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20606/20606 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20607/20607 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20608/20608 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20609/20609 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20610/20610 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20611/20611 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20612/20612 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20613/20613 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20614/20614 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20615/20615 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20616/20616 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20617/20617 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20618/20618 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20619/20619 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 20620/20620 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 20621/20621 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20622/20622 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20623/20623 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20624/20624 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20625/20625 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20626/20626 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20627/20627 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20628/20628 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20629/20629 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20630/20630 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20631/20631 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 20632/20632 [00:01<00:00,  1.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 20633/20633 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20634/20634 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20635/20635 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20636/20636 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20637/20637 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20638/20638 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 20639/20639 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 20640/20640 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20641/20641 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20642/20642 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 20643/20643 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20644/20644 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20645/20645 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20646/20646 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20647/20647 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20648/20648 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20649/20649 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20650/20650 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20651/20651 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20652/20652 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20653/20653 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20654/20654 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20655/20655 [00:01<00:00,  1.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 20656/20656 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20657/20657 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20658/20658 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20659/20659 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20660/20660 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20661/20661 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20662/20662 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20663/20663 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20664/20664 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 20665/20665 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20666/20666 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20667/20667 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20668/20668 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20669/20669 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20670/20670 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20671/20671 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20672/20672 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20673/20673 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20674/20674 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20675/20675 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20676/20676 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20677/20677 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20678/20678 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20679/20679 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20680/20680 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20681/20681 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20682/20682 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20683/20683 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20684/20684 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20685/20685 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 20686/20686 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 20687/20687 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 20688/20688 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 20689/20689 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 20690/20690 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 20691/20691 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 20692/20692 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 20693/20693 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 20694/20694 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 20695/20695 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 20696/20696 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 20697/20697 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20698/20698 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 20699/20699 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 20700/20700 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 20701/20701 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 20702/20702 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 20703/20703 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 20704/20704 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 20705/20705 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 20706/20706 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 20707/20707 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20708/20708 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 20709/20709 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20710/20710 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20711/20711 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 20712/20712 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20713/20713 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20714/20714 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20715/20715 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20716/20716 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20717/20717 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20718/20718 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20719/20719 [00:01<00:00,  1.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 20720/20720 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20721/20721 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20722/20722 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20723/20723 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20724/20724 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20725/20725 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20726/20726 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20727/20727 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20728/20728 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20729/20729 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20730/20730 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20731/20731 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20732/20732 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20733/20733 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20734/20734 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20735/20735 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20736/20736 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20737/20737 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20738/20738 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20739/20739 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20740/20740 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20741/20741 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20742/20742 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20743/20743 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 20744/20744 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20745/20745 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20746/20746 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20747/20747 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20748/20748 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20749/20749 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20750/20750 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20751/20751 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20752/20752 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20753/20753 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20754/20754 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 20755/20755 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20756/20756 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 20757/20757 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20758/20758 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 20759/20759 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20760/20760 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20761/20761 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 20762/20762 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20763/20763 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 20764/20764 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20765/20765 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20766/20766 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20767/20767 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 20768/20768 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20769/20769 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 20770/20770 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20771/20771 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20772/20772 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 20773/20773 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20774/20774 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20775/20775 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 20776/20776 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20777/20777 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20778/20778 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 20779/20779 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20780/20780 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 20781/20781 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20782/20782 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20783/20783 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20784/20784 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20785/20785 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20786/20786 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20787/20787 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20788/20788 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20789/20789 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20790/20790 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20791/20791 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20792/20792 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20793/20793 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20794/20794 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20795/20795 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20796/20796 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20797/20797 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20798/20798 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20799/20799 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20800/20800 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20801/20801 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20802/20802 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20803/20803 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20804/20804 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20805/20805 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20806/20806 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20807/20807 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20808/20808 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 20809/20809 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 20810/20810 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 20811/20811 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 20812/20812 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 20813/20813 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 20814/20814 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 20815/20815 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 20816/20816 [00:02<00:00,  2.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 20817/20817 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 20818/20818 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 20819/20819 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 20820/20820 [00:02<00:00,  2.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 20821/20821 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 20822/20822 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 20823/20823 [00:02<00:00,  2.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 20824/20824 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 20825/20825 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20826/20826 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20827/20827 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20828/20828 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20829/20829 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 20830/20830 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20831/20831 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20832/20832 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20833/20833 [00:02<00:00,  2.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 20834/20834 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20835/20835 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20836/20836 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20837/20837 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20838/20838 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20839/20839 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20840/20840 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20841/20841 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20842/20842 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20843/20843 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20844/20844 [00:02<00:00,  2.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 20845/20845 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20846/20846 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20847/20847 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20848/20848 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20849/20849 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20850/20850 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20851/20851 [00:02<00:00,  2.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 20852/20852 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20853/20853 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20854/20854 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20855/20855 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20856/20856 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20857/20857 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20858/20858 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20859/20859 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20860/20860 [00:02<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20861/20861 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20862/20862 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20863/20863 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20864/20864 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20865/20865 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20866/20866 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20867/20867 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 20868/20868 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20869/20869 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20870/20870 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20871/20871 [00:02<00:00,  2.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 20872/20872 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20873/20873 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20874/20874 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20875/20875 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20876/20876 [00:01<00:00,  2.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 20877/20877 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20878/20878 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20879/20879 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20880/20880 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20881/20881 [00:01<00:00,  1.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 20882/20882 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20883/20883 [00:01<00:00,  1.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 20884/20884 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 20885/20885 [00:01<00:00,  1.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 20886/20886 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20887/20887 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 20888/20888 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20889/20889 [00:02<00:00,  2.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 20890/20890 [00:01<00:00,  1.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 20891/20891 [00:02<00:00,  2.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 20892/20892 [00:01<00:00,  1.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 20893/20893 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 20894/20894 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20895/20895 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20896/20896 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20897/20897 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20898/20898 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20899/20899 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20900/20900 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20901/20901 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20902/20902 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20903/20903 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20904/20904 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20905/20905 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 20906/20906 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20907/20907 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20908/20908 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20909/20909 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20910/20910 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 20911/20911 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20912/20912 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20913/20913 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20914/20914 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20915/20915 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20916/20916 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20917/20917 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20918/20918 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20919/20919 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20920/20920 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20921/20921 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20922/20922 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20923/20923 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20924/20924 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20925/20925 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20926/20926 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20927/20927 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20928/20928 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20929/20929 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 20930/20930 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20931/20931 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20932/20932 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20933/20933 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 20934/20934 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20935/20935 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20936/20936 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 20937/20937 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20938/20938 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20939/20939 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20940/20940 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20941/20941 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20942/20942 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20943/20943 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20944/20944 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20945/20945 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20946/20946 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20947/20947 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20948/20948 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20949/20949 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20950/20950 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 20951/20951 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20952/20952 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20953/20953 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20954/20954 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20955/20955 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 20956/20956 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 20957/20957 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 20958/20958 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 20959/20959 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20960/20960 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 20961/20961 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 20962/20962 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20963/20963 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20964/20964 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20965/20965 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20966/20966 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 20967/20967 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20968/20968 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20969/20969 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20970/20970 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20971/20971 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20972/20972 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 20973/20973 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 20974/20974 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20975/20975 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20976/20976 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20977/20977 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20978/20978 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20979/20979 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20980/20980 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20981/20981 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20982/20982 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20983/20983 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20984/20984 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20985/20985 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 20986/20986 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20987/20987 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20988/20988 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 20989/20989 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 20990/20990 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20991/20991 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20992/20992 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20993/20993 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 20994/20994 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 20995/20995 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 20996/20996 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 20997/20997 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 20998/20998 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 20999/20999 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21000/21000 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21001/21001 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21002/21002 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21003/21003 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21004/21004 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21005/21005 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21006/21006 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21007/21007 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 21008/21008 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21009/21009 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21010/21010 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21011/21011 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21012/21012 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21013/21013 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21014/21014 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21015/21015 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21016/21016 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21017/21017 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21018/21018 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21019/21019 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21020/21020 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21021/21021 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21022/21022 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21023/21023 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21024/21024 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21025/21025 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21026/21026 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 21027/21027 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21028/21028 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21029/21029 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21030/21030 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21031/21031 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21032/21032 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21033/21033 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21034/21034 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21035/21035 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21036/21036 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21037/21037 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 21038/21038 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21039/21039 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21040/21040 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21041/21041 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21042/21042 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21043/21043 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21044/21044 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21045/21045 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21046/21046 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21047/21047 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21048/21048 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 21049/21049 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21050/21050 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21051/21051 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21052/21052 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21053/21053 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21054/21054 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21055/21055 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21056/21056 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21057/21057 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21058/21058 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21059/21059 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21060/21060 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21061/21061 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21062/21062 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 21063/21063 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21064/21064 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21065/21065 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21066/21066 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21067/21067 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21068/21068 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21069/21069 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21070/21070 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21071/21071 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21072/21072 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21073/21073 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21074/21074 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21075/21075 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21076/21076 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21077/21077 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21078/21078 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 21079/21079 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21080/21080 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21081/21081 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21082/21082 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21083/21083 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 21084/21084 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21085/21085 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21086/21086 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21087/21087 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21088/21088 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21089/21089 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21090/21090 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21091/21091 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21092/21092 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21093/21093 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21094/21094 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21095/21095 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 21096/21096 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21097/21097 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21098/21098 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21099/21099 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21100/21100 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21101/21101 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21102/21102 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21103/21103 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21104/21104 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21105/21105 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21106/21106 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21107/21107 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21108/21108 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21109/21109 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21110/21110 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21111/21111 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21112/21112 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21113/21113 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 21114/21114 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21115/21115 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21116/21116 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21117/21117 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21118/21118 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 21119/21119 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21120/21120 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21121/21121 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21122/21122 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21123/21123 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21124/21124 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21125/21125 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21126/21126 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21127/21127 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 21128/21128 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 21129/21129 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21130/21130 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21131/21131 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21132/21132 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21133/21133 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21134/21134 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21135/21135 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21136/21136 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21137/21137 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 21138/21138 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21139/21139 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21140/21140 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21141/21141 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21142/21142 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21143/21143 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21144/21144 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21145/21145 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21146/21146 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21147/21147 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21148/21148 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21149/21149 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 21150/21150 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21151/21151 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21152/21152 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21153/21153 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21154/21154 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21155/21155 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21156/21156 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21157/21157 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21158/21158 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21159/21159 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21160/21160 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21161/21161 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21162/21162 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21163/21163 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21164/21164 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21165/21165 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21166/21166 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21167/21167 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21168/21168 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21169/21169 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21170/21170 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21171/21171 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 21172/21172 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21173/21173 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 21174/21174 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21175/21175 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21176/21176 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21177/21177 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21178/21178 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21179/21179 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21180/21180 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21181/21181 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21182/21182 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 21183/21183 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21184/21184 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 21185/21185 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21186/21186 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 21187/21187 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 21188/21188 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21189/21189 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21190/21190 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21191/21191 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21192/21192 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21193/21193 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 21194/21194 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21195/21195 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21196/21196 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21197/21197 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21198/21198 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21199/21199 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21200/21200 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21201/21201 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21202/21202 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21203/21203 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21204/21204 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21205/21205 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21206/21206 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21207/21207 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21208/21208 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21209/21209 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21210/21210 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21211/21211 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21212/21212 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21213/21213 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21214/21214 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21215/21215 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 21216/21216 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21217/21217 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21218/21218 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21219/21219 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21220/21220 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21221/21221 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21222/21222 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21223/21223 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21224/21224 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21225/21225 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21226/21226 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21227/21227 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21228/21228 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21229/21229 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21230/21230 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21231/21231 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21232/21232 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21233/21233 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21234/21234 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21235/21235 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21236/21236 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21237/21237 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21238/21238 [00:02<00:00,  2.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 21239/21239 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 21240/21240 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21241/21241 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21242/21242 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21243/21243 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21244/21244 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21245/21245 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21246/21246 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21247/21247 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21248/21248 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21249/21249 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21250/21250 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21251/21251 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21252/21252 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21253/21253 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21254/21254 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21255/21255 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21256/21256 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21257/21257 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21258/21258 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21259/21259 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21260/21260 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21261/21261 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21262/21262 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 21263/21263 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21264/21264 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21265/21265 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21266/21266 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21267/21267 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21268/21268 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21269/21269 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21270/21270 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21271/21271 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21272/21272 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21273/21273 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21274/21274 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21275/21275 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21276/21276 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21277/21277 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21278/21278 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21279/21279 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21280/21280 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21281/21281 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21282/21282 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21283/21283 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21284/21284 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21285/21285 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21286/21286 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21287/21287 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21288/21288 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21289/21289 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21290/21290 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21291/21291 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21292/21292 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21293/21293 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21294/21294 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 21295/21295 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21296/21296 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21297/21297 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 21298/21298 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 21299/21299 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21300/21300 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21301/21301 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21302/21302 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21303/21303 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21304/21304 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21305/21305 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21306/21306 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21307/21307 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21308/21308 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21309/21309 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21310/21310 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21311/21311 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21312/21312 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21313/21313 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21314/21314 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21315/21315 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21316/21316 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 21317/21317 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21318/21318 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 21319/21319 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 21320/21320 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21321/21321 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21322/21322 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21323/21323 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21324/21324 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21325/21325 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21326/21326 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21327/21327 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21328/21328 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21329/21329 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21330/21330 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21331/21331 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 21332/21332 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 21333/21333 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 21334/21334 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21335/21335 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21336/21336 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 21337/21337 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 21338/21338 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21339/21339 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21340/21340 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21341/21341 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21342/21342 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 21343/21343 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21344/21344 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21345/21345 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21346/21346 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21347/21347 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 21348/21348 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21349/21349 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21350/21350 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21351/21351 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21352/21352 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21353/21353 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 21354/21354 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21355/21355 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21356/21356 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21357/21357 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21358/21358 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21359/21359 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21360/21360 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21361/21361 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21362/21362 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21363/21363 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21364/21364 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 21365/21365 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21366/21366 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21367/21367 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21368/21368 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21369/21369 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21370/21370 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21371/21371 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21372/21372 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21373/21373 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21374/21374 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21375/21375 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21376/21376 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 21377/21377 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21378/21378 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21379/21379 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21380/21380 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21381/21381 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21382/21382 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21383/21383 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21384/21384 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21385/21385 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21386/21386 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21387/21387 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21388/21388 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21389/21389 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21390/21390 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21391/21391 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21392/21392 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21393/21393 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21394/21394 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21395/21395 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21396/21396 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21397/21397 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21398/21398 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21399/21399 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21400/21400 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21401/21401 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21402/21402 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21403/21403 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21404/21404 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21405/21405 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21406/21406 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21407/21407 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21408/21408 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21409/21409 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 21410/21410 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21411/21411 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21412/21412 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21413/21413 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21414/21414 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21415/21415 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21416/21416 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21417/21417 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21418/21418 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21419/21419 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21420/21420 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 21421/21421 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21422/21422 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21423/21423 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21424/21424 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21425/21425 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21426/21426 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21427/21427 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21428/21428 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 21429/21429 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21430/21430 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21431/21431 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21432/21432 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21433/21433 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21434/21434 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21435/21435 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21436/21436 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21437/21437 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21438/21438 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21439/21439 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21440/21440 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 21441/21441 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21442/21442 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21443/21443 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21444/21444 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21445/21445 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21446/21446 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21447/21447 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21448/21448 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21449/21449 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21450/21450 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21451/21451 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 21452/21452 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 21453/21453 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21454/21454 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21455/21455 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 21456/21456 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21457/21457 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21458/21458 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21459/21459 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21460/21460 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21461/21461 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21462/21462 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21463/21463 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21464/21464 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21465/21465 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21466/21466 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21467/21467 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21468/21468 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21469/21469 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21470/21470 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21471/21471 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21472/21472 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21473/21473 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21474/21474 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 21475/21475 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21476/21476 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21477/21477 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21478/21478 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21479/21479 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21480/21480 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21481/21481 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21482/21482 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21483/21483 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21484/21484 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21485/21485 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21486/21486 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21487/21487 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 21488/21488 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21489/21489 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21490/21490 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21491/21491 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21492/21492 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21493/21493 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21494/21494 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21495/21495 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21496/21496 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 21497/21497 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21498/21498 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21499/21499 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21500/21500 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21501/21501 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21502/21502 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21503/21503 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21504/21504 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21505/21505 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21506/21506 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21507/21507 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 21508/21508 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21509/21509 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21510/21510 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21511/21511 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21512/21512 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 21513/21513 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21514/21514 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21515/21515 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21516/21516 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21517/21517 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 21518/21518 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21519/21519 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21520/21520 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21521/21521 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21522/21522 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21523/21523 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 21524/21524 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21525/21525 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21526/21526 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21527/21527 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21528/21528 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21529/21529 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21530/21530 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21531/21531 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21531/21532 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (243) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21532/21532 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 21533/21533 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21534/21534 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21535/21535 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21535/21536 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (246) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21536/21536 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21536/21537 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (247) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21537/21537 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 21538/21538 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21539/21539 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21540/21540 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21541/21541 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 21542/21542 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21543/21543 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21544/21544 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21545/21545 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21546/21546 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21547/21547 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 21548/21548 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21549/21549 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21550/21550 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21551/21551 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 21552/21552 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 21553/21553 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21554/21554 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21555/21555 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 21556/21556 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 21557/21557 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21558/21558 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21559/21559 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21560/21560 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21561/21561 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21562/21562 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21563/21563 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 21564/21564 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 21565/21565 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21566/21566 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21567/21567 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21568/21568 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21569/21569 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 21570/21570 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21571/21571 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21572/21572 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21573/21573 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21574/21574 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 21575/21575 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21576/21576 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 21577/21577 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 21578/21578 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21579/21579 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21580/21580 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21581/21581 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21582/21582 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21583/21583 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 21584/21584 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21585/21585 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 21586/21586 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21587/21587 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21588/21588 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21589/21589 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 21590/21590 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21591/21591 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21592/21592 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21593/21593 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 21594/21594 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 21595/21595 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21596/21596 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 21597/21597 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21598/21598 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21599/21599 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21600/21600 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21601/21601 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21602/21602 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21603/21603 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21604/21604 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21605/21605 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21606/21606 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21607/21607 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21608/21608 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21609/21609 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21610/21610 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21611/21611 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21612/21612 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21613/21613 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21614/21614 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21615/21615 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21616/21616 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21617/21617 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21618/21618 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21619/21619 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21620/21620 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21621/21621 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21622/21622 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21623/21623 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21624/21624 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21625/21625 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21626/21626 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21627/21627 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21628/21628 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21629/21629 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21630/21630 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 21631/21631 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21632/21632 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21633/21633 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21634/21634 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21635/21635 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21636/21636 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21637/21637 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21638/21638 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21639/21639 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21640/21640 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21641/21641 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21642/21642 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21643/21643 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21644/21644 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21645/21645 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21646/21646 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21647/21647 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21648/21648 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21649/21649 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21650/21650 [00:02<00:00,  2.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 21651/21651 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21652/21652 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21653/21653 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21654/21654 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 21655/21655 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21656/21656 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21657/21657 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21658/21658 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21659/21659 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21660/21660 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21661/21661 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21662/21662 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21663/21663 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 21664/21664 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21665/21665 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21666/21666 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21667/21667 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21668/21668 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21669/21669 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21670/21670 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21671/21671 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21672/21672 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21673/21673 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21674/21674 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 21675/21675 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21676/21676 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21677/21677 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21678/21678 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21679/21679 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21680/21680 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21681/21681 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21682/21682 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21683/21683 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21684/21684 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21685/21685 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 21686/21686 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21687/21687 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21688/21688 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21689/21689 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21690/21690 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21691/21691 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21692/21692 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21693/21693 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21694/21694 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21695/21695 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21696/21696 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 21697/21697 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21698/21698 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21699/21699 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21700/21700 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21701/21701 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21702/21702 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21703/21703 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21704/21704 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 21705/21705 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21706/21706 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21707/21707 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21708/21708 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21709/21709 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21710/21710 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21711/21711 [00:02<00:00,  2.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 21712/21712 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21713/21713 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 21714/21714 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21715/21715 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21716/21716 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21717/21717 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21718/21718 [00:02<00:00,  2.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 21719/21719 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21720/21720 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21721/21721 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21722/21722 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21723/21723 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21724/21724 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21725/21725 [00:02<00:00,  2.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 21726/21726 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21727/21727 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21728/21728 [00:02<00:00,  2.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 21729/21729 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 21730/21730 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21731/21731 [00:02<00:00,  2.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 21732/21732 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21733/21733 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21734/21734 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 21735/21735 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 21736/21736 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21737/21737 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21738/21738 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21739/21739 [00:02<00:00,  2.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21740/21740 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21741/21741 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 21742/21742 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 21743/21743 [00:02<00:00,  2.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21744/21744 [00:02<00:00,  2.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21745/21745 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21746/21746 [00:02<00:00,  2.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 21747/21747 [00:02<00:00,  2.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 21748/21748 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 21749/21749 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 21750/21750 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 21751/21751 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 21752/21752 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 21753/21753 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 21754/21754 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 21755/21755 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21756/21756 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21757/21757 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 21758/21758 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 21759/21759 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 21760/21760 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21761/21761 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 21762/21762 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 21763/21763 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 21764/21764 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21765/21765 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 21766/21766 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 21767/21767 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21768/21768 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 21769/21769 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 21770/21770 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 21771/21771 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 21772/21772 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 21773/21773 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 21774/21774 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 21775/21775 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 21776/21776 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 21777/21777 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 21778/21778 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 21779/21779 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 21780/21780 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21781/21781 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 21782/21782 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 21783/21783 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 21784/21784 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 21785/21785 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 21786/21786 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 21787/21787 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21788/21788 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 21789/21789 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 21790/21790 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21791/21791 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21792/21792 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 21793/21793 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 21794/21794 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 21795/21795 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 21796/21796 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 21797/21797 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21798/21798 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21799/21799 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 21800/21800 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 21801/21801 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 21802/21802 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21803/21803 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 21804/21804 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 21805/21805 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21806/21806 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21807/21807 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 21808/21808 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21809/21809 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21810/21810 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 21811/21811 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 21812/21812 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 21813/21813 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21814/21814 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21815/21815 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21816/21816 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 21817/21817 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21818/21818 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 21819/21819 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21820/21820 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 21821/21821 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 21822/21822 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 21823/21823 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21824/21824 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 21825/21825 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 21826/21826 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21827/21827 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21828/21828 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 21829/21829 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 21830/21830 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 21831/21831 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 21832/21832 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 21833/21833 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21834/21834 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 21835/21835 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21836/21836 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 21837/21837 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 21838/21838 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 21839/21839 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 21840/21840 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21841/21841 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 21842/21842 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21843/21843 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 21844/21844 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21845/21845 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21846/21846 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 21847/21847 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 21848/21848 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 21849/21849 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21850/21850 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21851/21851 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 21852/21852 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 21853/21853 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 21854/21854 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21855/21855 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 21856/21856 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 21857/21857 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21858/21858 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 21859/21859 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21860/21860 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21861/21861 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21862/21862 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 21863/21863 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 21864/21864 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 21865/21865 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 21866/21866 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 21867/21867 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 21868/21868 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 21869/21869 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 21870/21870 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 21871/21871 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21872/21872 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21873/21873 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21874/21874 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 21875/21875 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 21876/21876 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 21877/21877 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 21878/21878 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 21879/21879 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 21880/21880 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 21881/21881 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 21882/21882 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 21883/21883 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21884/21884 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21885/21885 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 21886/21886 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 21887/21887 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21888/21888 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21889/21889 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21890/21890 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 21891/21891 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 21892/21892 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21893/21893 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 21894/21894 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 21895/21895 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21896/21896 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 21897/21897 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 21898/21898 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21899/21899 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 21900/21900 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 21901/21901 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 21902/21902 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 21903/21903 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 21904/21904 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 21905/21905 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 21906/21906 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 21907/21907 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21907/21908 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21908/21908 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21908/21909 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21909/21909 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21909/21910 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21910/21910 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21910/21911 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21911/21911 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21911/21912 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21912/21912 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21912/21913 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21913/21913 [00:04<00:00,  4.01s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21913/21914 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21914/21914 [00:04<00:00,  4.48s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21914/21915 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21915/21915 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21915/21916 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21916/21916 [00:04<00:00,  4.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21916/21917 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21917/21917 [00:04<00:00,  4.49s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21917/21918 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21918/21918 [00:05<00:00,  5.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21918/21919 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21919/21919 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21919/21920 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21920/21920 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21920/21921 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21921/21921 [00:04<00:00,  4.53s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21921/21922 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21922/21922 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21922/21923 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21923/21923 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21923/21924 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21924/21924 [00:04<00:00,  4.41s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21924/21925 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21925/21925 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21925/21926 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21926/21926 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 21927/21927 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21927/21928 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21928/21928 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21928/21929 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21929/21929 [00:05<00:00,  5.46s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21929/21930 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21930/21930 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21930/21931 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21931/21931 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21931/21932 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21932/21932 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21932/21933 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21933/21933 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21933/21934 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21934/21934 [00:04<00:00,  4.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21934/21935 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21935/21935 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 21936/21936 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21936/21937 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21937/21937 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21938/21938 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 21939/21939 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21939/21940 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21940/21940 [00:05<00:00,  5.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21940/21941 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21941/21941 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 21942/21942 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21942/21943 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21943/21943 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 21944/21944 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21944/21945 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21945/21945 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21945/21946 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21946/21946 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21946/21947 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21947/21947 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 21948/21948 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21948/21949 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21949/21949 [00:04<00:00,  4.50s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21949/21950 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21950/21950 [00:04<00:00,  4.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 21951/21951 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21951/21952 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21952/21952 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21952/21953 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21953/21953 [00:04<00:00,  4.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21953/21954 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21954/21954 [00:04<00:00,  4.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21954/21955 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21955/21955 [00:04<00:00,  4.41s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21955/21956 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21956/21956 [00:04<00:00,  4.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21956/21957 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21957/21957 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21957/21958 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21958/21958 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21958/21959 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21959/21959 [00:04<00:00,  4.51s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21959/21960 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21960/21960 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 21961/21961 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21961/21962 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21962/21962 [00:04<00:00,  4.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21962/21963 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21963/21963 [00:04<00:00,  4.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21963/21964 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21964/21964 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 21965/21965 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21965/21966 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21966/21966 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21966/21967 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21967/21967 [00:04<00:00,  4.55s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21967/21968 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21968/21968 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21968/21969 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21969/21969 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 21970/21970 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21970/21971 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21971/21971 [00:04<00:00,  4.38s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21971/21972 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21972/21972 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21972/21973 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21973/21973 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21973/21974 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21974/21974 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21974/21975 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21975/21975 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21975/21976 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21976/21976 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 21977/21977 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21977/21978 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21978/21978 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21978/21979 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21979/21979 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21979/21980 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21980/21980 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21980/21981 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21981/21981 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21981/21982 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21982/21982 [00:04<00:00,  4.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21982/21983 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21983/21983 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21983/21984 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21984/21984 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21984/21985 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21985/21985 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21985/21986 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21986/21986 [00:04<00:00,  4.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21986/21987 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21987/21987 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21987/21988 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21988/21988 [00:04<00:00,  4.50s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21988/21989 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21989/21989 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21989/21990 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21990/21990 [00:04<00:00,  4.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21990/21991 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21991/21991 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21991/21992 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21992/21992 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21992/21993 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21993/21993 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21993/21994 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21994/21994 [00:04<00:00,  4.46s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21994/21995 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21995/21995 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21995/21996 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21996/21996 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21996/21997 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21997/21997 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21997/21998 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21998/21998 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21998/21999 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21999/21999 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 21999/22000 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22000/22000 [00:04<00:00,  4.55s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22000/22001 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22001/22001 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22001/22002 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22002/22002 [00:04<00:00,  4.52s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22002/22003 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22003/22003 [00:04<00:00,  4.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22003/22004 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22004/22004 [00:04<00:00,  4.52s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22004/22005 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22005/22005 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22005/22006 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22006/22006 [00:05<00:00,  5.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22006/22007 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22007/22007 [00:04<00:00,  4.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22007/22008 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22008/22008 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22008/22009 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22009/22009 [00:04<00:00,  4.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22009/22010 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22010/22010 [00:04<00:00,  4.07s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22010/22011 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22011/22011 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22011/22012 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22012/22012 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22012/22013 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22013/22013 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22013/22014 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22014/22014 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22014/22015 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22015/22015 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 22016/22016 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22017/22017 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22017/22018 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22018/22018 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22019/22019 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22019/22020 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22020/22020 [00:04<00:00,  4.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22020/22021 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22021/22021 [00:04<00:00,  4.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 22022/22022 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22022/22023 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22023/22023 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 22024/22024 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22024/22025 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22025/22025 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22025/22026 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22026/22026 [00:03<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 22027/22027 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22027/22028 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22028/22028 [00:05<00:00,  5.02s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22028/22029 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22029/22029 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 22030/22030 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22030/22031 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22031/22031 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22031/22032 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22032/22032 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22032/22033 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22033/22033 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 22034/22034 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22034/22035 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22035/22035 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22035/22036 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22036/22036 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22036/22037 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22037/22037 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22037/22038 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22038/22038 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22038/22039 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22039/22039 [00:05<00:00,  5.03s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22039/22040 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22040/22040 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22040/22041 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22041/22041 [00:04<00:00,  4.01s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22041/22042 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22042/22042 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22042/22043 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22043/22043 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 22044/22044 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22044/22045 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22045/22045 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 22046/22046 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22046/22047 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22047/22047 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22047/22048 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22048/22048 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22048/22049 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22049/22049 [00:03<00:00,  3.99s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22049/22050 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22050/22050 [00:04<00:00,  4.98s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22050/22051 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22051/22051 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 22052/22052 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22053/22053 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22053/22054 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22054/22054 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22054/22055 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22055/22055 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 22056/22056 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22056/22057 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22057/22057 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22058/22058 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22058/22059 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22059/22059 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22059/22060 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22060/22060 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22060/22061 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22061/22061 [00:05<00:00,  5.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22062/22062 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22062/22063 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22063/22063 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22064/22064 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22064/22065 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22065/22065 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22065/22066 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22066/22066 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 22067/22067 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22068/22068 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22068/22069 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22069/22069 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22069/22070 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22070/22070 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22070/22071 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22071/22071 [00:03<00:00,  3.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 22072/22072 [00:04<00:00,  4.08s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22072/22073 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22073/22073 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 22074/22074 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 22075/22075 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22075/22076 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22076/22076 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 22077/22077 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 22078/22078 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 22079/22079 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 22080/22080 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22080/22081 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22081/22081 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22082/22082 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 22083/22083 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22083/22084 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22084/22084 [00:04<00:00,  4.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 22085/22085 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22085/22086 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22086/22086 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22086/22087 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22087/22087 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22087/22088 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22088/22088 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22088/22089 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22089/22089 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22089/22090 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22090/22090 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 22091/22091 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22091/22092 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22092/22092 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22092/22093 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22093/22093 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 22094/22094 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22094/22095 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22095/22095 [00:04<00:00,  4.86s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22095/22096 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22096/22096 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22096/22097 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22097/22097 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 22098/22098 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 22099/22099 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22099/22100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22100/22100 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22100/22101 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22101/22101 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 22102/22102 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22102/22103 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22103/22103 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22103/22104 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22104/22104 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22104/22105 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22105/22105 [00:05<00:00,  5.04s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22105/22106 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22106/22106 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 22107/22107 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 22108/22108 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22108/22109 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22109/22109 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22109/22110 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22110/22110 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22110/22111 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22111/22111 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22111/22112 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22112/22112 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22112/22113 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22113/22113 [00:03<00:00,  3.93s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22113/22114 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22114/22114 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22114/22115 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22115/22115 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22115/22116 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22116/22116 [00:03<00:00,  3.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 22117/22117 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22117/22118 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22118/22118 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22118/22119 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22119/22119 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22119/22120 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22120/22120 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 22121/22121 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22121/22122 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22122/22122 [00:03<00:00,  3.98s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22122/22123 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22123/22123 [00:03<00:00,  3.97s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22123/22124 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22124/22124 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22124/22125 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22125/22125 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 22126/22126 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22126/22127 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22127/22127 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 22128/22128 [00:03<00:00,  3.98s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22128/22129 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22129/22129 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22129/22130 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22130/22130 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 22131/22131 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22131/22132 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22132/22132 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 22133/22133 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22134/22134 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 22135/22135 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22135/22136 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22136/22136 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22136/22137 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22137/22137 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22137/22138 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22138/22138 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22138/22139 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22139/22139 [00:04<00:00,  4.92s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22139/22140 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22140/22140 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22140/22141 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22141/22141 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22141/22142 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22142/22142 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22142/22143 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22143/22143 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 22144/22144 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 22145/22145 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22145/22146 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22146/22146 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22147/22147 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 22148/22148 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22149/22149 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22150/22150 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 22151/22151 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 22152/22152 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22153/22153 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22154/22154 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22155/22155 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22156/22156 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22157/22157 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 22158/22158 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 22159/22159 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 22160/22160 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 22161/22161 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 22162/22162 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 22163/22163 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 22164/22164 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 22165/22165 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 22166/22166 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22167/22167 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22168/22168 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 22169/22169 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 22170/22170 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22171/22171 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 22172/22172 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 22173/22173 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 22174/22174 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 22175/22175 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22176/22176 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22177/22177 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 22178/22178 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22179/22179 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 22180/22180 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 22181/22181 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 22182/22182 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 22183/22183 [00:04<00:00,  4.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 22184/22184 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 22185/22185 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 22186/22186 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 22187/22187 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 22188/22188 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 22189/22189 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 22190/22190 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 22191/22191 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 22192/22192 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22193/22193 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 22194/22194 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 22195/22195 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 22196/22196 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 22197/22197 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22198/22198 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 22199/22199 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 22200/22200 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 22201/22201 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 22202/22202 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22203/22203 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 22204/22204 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 22205/22205 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 22206/22206 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 22207/22207 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 22208/22208 [00:02<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 22209/22209 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 22210/22210 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22211/22211 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 22212/22212 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 22213/22213 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 22214/22214 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 22215/22215 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22216/22216 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 22217/22217 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 22218/22218 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 22219/22219 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 22220/22220 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 22221/22221 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 22222/22222 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 22223/22223 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 22224/22224 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 22225/22225 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 22226/22226 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 22227/22227 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 22228/22228 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 22229/22229 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 22230/22230 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 22231/22231 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 22232/22232 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22233/22233 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 22234/22234 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22234/22235 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (197) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22235/22235 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 22236/22236 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22236/22237 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (196) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22237/22237 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22238/22238 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 22239/22239 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22240/22240 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22240/22241 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (196) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22241/22241 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22241/22242 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (195) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22242/22242 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22243/22243 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22243/22244 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (196) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22244/22244 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 22245/22245 [00:02<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 22246/22246 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22247/22247 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 22248/22248 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 22249/22249 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 22250/22250 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 22251/22251 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 22252/22252 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 22253/22253 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22254/22254 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 22255/22255 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 22256/22256 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 22257/22257 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 22258/22258 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22259/22259 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 22260/22260 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 22261/22261 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 22262/22262 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22263/22263 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 22264/22264 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22265/22265 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22266/22266 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 22267/22267 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22268/22268 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22268/22269 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (193) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22269/22269 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 22270/22270 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 22271/22271 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22272/22272 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22273/22273 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22274/22274 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 22275/22275 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22276/22276 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 22277/22277 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 22278/22278 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 22279/22279 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 22280/22280 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 22281/22281 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22282/22282 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22283/22283 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 22284/22284 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 22285/22285 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22286/22286 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22287/22287 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22288/22288 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22289/22289 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22290/22290 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 22291/22291 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22292/22292 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 22293/22293 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 22294/22294 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 22295/22295 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 22296/22296 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22297/22297 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 22298/22298 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 22299/22299 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22300/22300 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22301/22301 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 22302/22302 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 22303/22303 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 22304/22304 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22305/22305 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22306/22306 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 22307/22307 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 22308/22308 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22309/22309 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22310/22310 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22311/22311 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 22312/22312 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 22313/22313 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22314/22314 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22315/22315 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 22316/22316 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 22317/22317 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 22318/22318 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 22319/22319 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22320/22320 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22321/22321 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 22322/22322 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 22323/22323 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22324/22324 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 22325/22325 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22326/22326 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22327/22327 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 22328/22328 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22329/22329 [00:04<00:00,  4.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 22330/22330 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22331/22331 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22332/22332 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 22333/22333 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 22334/22334 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 22335/22335 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22336/22336 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 22337/22337 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22338/22338 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 22339/22339 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 22340/22340 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 22341/22341 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22342/22342 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22343/22343 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 22344/22344 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22345/22345 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22346/22346 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22347/22347 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22348/22348 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22349/22349 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22350/22350 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22351/22351 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 22352/22352 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 22353/22353 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22354/22354 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22355/22355 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22356/22356 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22357/22357 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 22358/22358 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22359/22359 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22360/22360 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 22361/22361 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22362/22362 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 22363/22363 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 22364/22364 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 22365/22365 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22366/22366 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 22367/22367 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22368/22368 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22369/22369 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22370/22370 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22371/22371 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 22372/22372 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 22373/22373 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 22374/22374 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 22375/22375 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 22376/22376 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22377/22377 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 22378/22378 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22379/22379 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22380/22380 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 22381/22381 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 22382/22382 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 22383/22383 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 22384/22384 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 22385/22385 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 22386/22386 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 22387/22387 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22388/22388 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22389/22389 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22390/22390 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 22391/22391 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 22392/22392 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 22393/22393 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22394/22394 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 22395/22395 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 22396/22396 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22397/22397 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22398/22398 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22399/22399 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 22400/22400 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 22401/22401 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 22402/22402 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22403/22403 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 22404/22404 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22405/22405 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 22406/22406 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 22407/22407 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 22408/22408 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22409/22409 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22410/22410 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22411/22411 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22412/22412 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 22413/22413 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22414/22414 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 22415/22415 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 22416/22416 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22417/22417 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22418/22418 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 22419/22419 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 22420/22420 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22421/22421 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 22422/22422 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 22423/22423 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 22424/22424 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 22425/22425 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 22426/22426 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 22427/22427 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 22428/22428 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 22429/22429 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 22430/22430 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 22431/22431 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22432/22432 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22433/22433 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22434/22434 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 22435/22435 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22436/22436 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22437/22437 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22438/22438 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22439/22439 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 22440/22440 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 22441/22441 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22442/22442 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22443/22443 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 22444/22444 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 22445/22445 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22446/22446 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 22447/22447 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22448/22448 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 22449/22449 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22450/22450 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 22451/22451 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 22452/22452 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22453/22453 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 22454/22454 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22455/22455 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 22456/22456 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 22457/22457 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22458/22458 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 22459/22459 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22460/22460 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 22461/22461 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 22462/22462 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22463/22463 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22464/22464 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 22465/22465 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 22466/22466 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 22467/22467 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22468/22468 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 22469/22469 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 22470/22470 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 22471/22471 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 22472/22472 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 22473/22473 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 22474/22474 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 22475/22475 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 22476/22476 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22477/22477 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22478/22478 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 22479/22479 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 22480/22480 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 22481/22481 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22482/22482 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 22483/22483 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22483/22484 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22484/22484 [00:04<00:00,  4.70s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22484/22485 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22485/22485 [00:04<00:00,  4.51s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22485/22486 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22486/22486 [00:04<00:00,  4.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22486/22487 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22487/22487 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22487/22488 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22488/22488 [00:04<00:00,  4.82s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22488/22489 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22489/22489 [00:04<00:00,  4.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22489/22490 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22490/22490 [00:04<00:00,  4.38s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22490/22491 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22491/22491 [00:04<00:00,  4.79s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22491/22492 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22492/22492 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22492/22493 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22493/22493 [00:04<00:00,  4.70s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22493/22494 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22494/22494 [00:05<00:00,  5.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22494/22495 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22495/22495 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22495/22496 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22496/22496 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22496/22497 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22497/22497 [00:04<00:00,  4.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22498/22498 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22498/22499 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22499/22499 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22499/22500 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22500/22500 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22500/22501 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22501/22501 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22501/22502 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22502/22502 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22502/22503 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22503/22503 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22503/22504 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22504/22504 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22504/22505 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22505/22505 [00:05<00:00,  5.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22505/22506 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22506/22506 [00:04<00:00,  4.55s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22506/22507 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22507/22507 [00:04<00:00,  4.70s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22507/22508 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22508/22508 [00:04<00:00,  4.70s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22508/22509 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22509/22509 [00:04<00:00,  4.70s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22509/22510 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22510/22510 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22510/22511 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22511/22511 [00:04<00:00,  4.51s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22511/22512 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22512/22512 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22512/22513 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22513/22513 [00:04<00:00,  4.70s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22513/22514 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22514/22514 [00:04<00:00,  4.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22514/22515 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22515/22515 [00:04<00:00,  4.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22515/22516 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22516/22516 [00:05<00:00,  5.94s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22516/22517 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22517/22517 [00:04<00:00,  4.88s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22517/22518 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22518/22518 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22518/22519 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22519/22519 [00:04<00:00,  4.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22519/22520 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22520/22520 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22520/22521 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22521/22521 [00:04<00:00,  4.99s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22521/22522 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22522/22522 [00:04<00:00,  4.64s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22522/22523 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22523/22523 [00:04<00:00,  4.64s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22523/22524 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22524/22524 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22524/22525 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22525/22525 [00:04<00:00,  4.48s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22525/22526 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22526/22526 [00:04<00:00,  4.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22526/22527 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22527/22527 [00:04<00:00,  4.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 22528/22528 [00:04<00:00,  4.07s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22528/22529 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22529/22529 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22529/22530 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22530/22530 [00:04<00:00,  4.53s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22530/22531 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22531/22531 [00:04<00:00,  4.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22531/22532 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22532/22532 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22532/22533 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22533/22533 [00:04<00:00,  4.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22533/22534 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22534/22534 [00:04<00:00,  4.88s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22534/22535 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22535/22535 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22535/22536 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22536/22536 [00:04<00:00,  4.46s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22536/22537 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22537/22537 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22537/22538 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22538/22538 [00:06<00:00,  6.09s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22538/22539 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22539/22539 [00:04<00:00,  4.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 22540/22540 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22540/22541 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22541/22541 [00:04<00:00,  4.81s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22541/22542 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22542/22542 [00:04<00:00,  4.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22542/22543 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22543/22543 [00:04<00:00,  4.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22543/22544 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22544/22544 [00:04<00:00,  4.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22544/22545 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22545/22545 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22545/22546 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22546/22546 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22546/22547 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22547/22547 [00:04<00:00,  4.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22547/22548 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22548/22548 [00:04<00:00,  4.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22548/22549 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22549/22549 [00:04<00:00,  4.41s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22549/22550 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22550/22550 [00:05<00:00,  5.81s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22550/22551 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22551/22551 [00:04<00:00,  4.56s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22551/22552 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22552/22552 [00:04<00:00,  4.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 22553/22553 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22553/22554 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22554/22554 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22554/22555 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22555/22555 [00:04<00:00,  4.62s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22555/22556 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22556/22556 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22556/22557 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22557/22557 [00:04<00:00,  4.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22557/22558 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22558/22558 [00:04<00:00,  4.68s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22558/22559 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22559/22559 [00:04<00:00,  4.96s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22559/22560 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22560/22560 [00:04<00:00,  4.52s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22560/22561 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22561/22561 [00:04<00:00,  4.99s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22561/22562 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22562/22562 [00:04<00:00,  4.50s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22562/22563 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22563/22563 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22563/22564 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22564/22564 [00:04<00:00,  4.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22564/22565 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22565/22565 [00:04<00:00,  4.55s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22565/22566 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22566/22566 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22566/22567 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22567/22567 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22567/22568 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22568/22568 [00:04<00:00,  4.74s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22568/22569 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22569/22569 [00:04<00:00,  4.66s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22569/22570 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22570/22570 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22570/22571 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22571/22571 [00:04<00:00,  4.38s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22571/22572 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22572/22572 [00:05<00:00,  5.58s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22572/22573 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22573/22573 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22573/22574 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22574/22574 [00:04<00:00,  4.81s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22574/22575 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22575/22575 [00:04<00:00,  4.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22575/22576 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22576/22576 [00:04<00:00,  4.49s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22576/22577 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22577/22577 [00:04<00:00,  4.56s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22577/22578 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22578/22578 [00:04<00:00,  4.61s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22578/22579 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22579/22579 [00:04<00:00,  4.59s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22579/22580 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22580/22580 [00:04<00:00,  4.70s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22580/22581 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22581/22581 [00:04<00:00,  4.61s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22581/22582 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22582/22582 [00:04<00:00,  4.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22582/22583 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22583/22583 [00:05<00:00,  5.90s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22583/22584 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22584/22584 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22584/22585 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22585/22585 [00:04<00:00,  4.46s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22585/22586 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22586/22586 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22586/22587 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22587/22587 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22587/22588 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22588/22588 [00:04<00:00,  4.63s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22588/22589 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22589/22589 [00:04<00:00,  4.53s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22589/22590 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22590/22590 [00:04<00:00,  4.68s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22590/22591 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22591/22591 [00:04<00:00,  4.49s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22591/22592 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22592/22592 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22592/22593 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22593/22593 [00:04<00:00,  4.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22593/22594 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22594/22594 [00:04<00:00,  4.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22594/22595 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22595/22595 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22595/22596 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22596/22596 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22596/22597 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22597/22597 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22597/22598 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22598/22598 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22598/22599 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22599/22599 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22599/22600 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22600/22600 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22600/22601 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22601/22601 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22601/22602 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22602/22602 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22602/22603 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22603/22603 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22603/22604 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22604/22604 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22604/22605 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22605/22605 [00:05<00:00,  5.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22605/22606 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22606/22606 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22606/22607 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22607/22607 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22607/22608 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22608/22608 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22608/22609 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22609/22609 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22609/22610 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22610/22610 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22610/22611 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22611/22611 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22611/22612 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22612/22612 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22612/22613 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22613/22613 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22613/22614 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22614/22614 [00:04<00:00,  4.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 22615/22615 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22615/22616 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22616/22616 [00:05<00:00,  5.47s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22616/22617 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22617/22617 [00:04<00:00,  4.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22617/22618 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22618/22618 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22618/22619 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22619/22619 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22619/22620 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22620/22620 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22620/22621 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22621/22621 [00:04<00:00,  4.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 22622/22622 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 22623/22623 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22623/22624 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22624/22624 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22624/22625 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22625/22625 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22625/22626 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22626/22626 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22626/22627 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22627/22627 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22627/22628 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22628/22628 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22628/22629 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22629/22629 [00:04<00:00,  4.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22629/22630 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22630/22630 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22630/22631 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22631/22631 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22631/22632 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22632/22632 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 22633/22633 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22633/22634 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22634/22634 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22634/22635 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22635/22635 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22635/22636 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22636/22636 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22636/22637 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22637/22637 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 22638/22638 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 22639/22639 [00:03<00:00,  3.99s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22639/22640 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22640/22640 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22640/22641 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22641/22641 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22641/22642 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22642/22642 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22642/22643 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22643/22643 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22643/22644 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22644/22644 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22644/22645 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22645/22645 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22645/22646 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22646/22646 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22646/22647 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22647/22647 [00:04<00:00,  4.25s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22647/22648 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22648/22648 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22648/22649 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22649/22649 [00:04<00:00,  4.54s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22649/22650 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22650/22650 [00:05<00:00,  5.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22650/22651 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22651/22651 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22651/22652 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22652/22652 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22652/22653 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22653/22653 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22653/22654 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22654/22654 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22654/22655 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22655/22655 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22655/22656 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22656/22656 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 22657/22657 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 22658/22658 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22658/22659 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22659/22659 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22659/22660 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22660/22660 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22660/22661 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22661/22661 [00:05<00:00,  5.57s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22661/22662 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22662/22662 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22662/22663 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22663/22663 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22663/22664 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22664/22664 [00:04<00:00,  4.53s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22664/22665 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22665/22665 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22665/22666 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22666/22666 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 22667/22667 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22667/22668 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22668/22668 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22668/22669 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22669/22669 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22669/22670 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22670/22670 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22670/22671 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22671/22671 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 22672/22672 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22672/22673 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22673/22673 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22673/22674 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22674/22674 [00:04<00:00,  4.38s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22674/22675 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22675/22675 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 22676/22676 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22676/22677 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22677/22677 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22677/22678 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22678/22678 [00:04<00:00,  4.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22678/22679 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22679/22679 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22679/22680 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22680/22680 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22680/22681 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22681/22681 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22681/22682 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22682/22682 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22682/22683 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22683/22683 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 22684/22684 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22684/22685 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22685/22685 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22685/22686 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22686/22686 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22686/22687 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22687/22687 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22687/22688 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22688/22688 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22688/22689 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22689/22689 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22689/22690 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22690/22690 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22690/22691 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22691/22691 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22691/22692 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22692/22692 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22692/22693 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22693/22693 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 22694/22694 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22694/22695 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22695/22695 [00:05<00:00,  5.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22695/22696 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22696/22696 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22696/22697 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22697/22697 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22697/22698 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22698/22698 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22698/22699 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22699/22699 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22699/22700 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22700/22700 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22700/22701 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22701/22701 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22701/22702 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22702/22702 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 22703/22703 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22703/22704 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22704/22704 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22704/22705 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22705/22705 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 22706/22706 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 22707/22707 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 22708/22708 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 22709/22709 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22709/22710 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22710/22710 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22710/22711 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22711/22711 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22711/22712 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22712/22712 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 22713/22713 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22713/22714 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22714/22714 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 22715/22715 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 22716/22716 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22716/22717 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22717/22717 [00:05<00:00,  5.43s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22717/22718 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22718/22718 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22718/22719 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22719/22719 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22719/22720 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22720/22720 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22720/22721 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22721/22721 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22721/22722 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22722/22722 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 22723/22723 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22723/22724 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22724/22724 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 22725/22725 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22725/22726 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22726/22726 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 22727/22727 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22727/22728 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22728/22728 [00:05<00:00,  5.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 22729/22729 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22729/22730 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22730/22730 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22730/22731 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22731/22731 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 22732/22732 [00:03<00:00,  3.97s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22732/22733 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22733/22733 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22733/22734 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22734/22734 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22734/22735 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22735/22735 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 22736/22736 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22736/22737 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22737/22737 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22737/22738 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22738/22738 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22738/22739 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22739/22739 [00:04<00:00,  4.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22739/22740 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22740/22740 [00:05<00:00,  5.55s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22740/22741 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22741/22741 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22741/22742 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22742/22742 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22742/22743 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22743/22743 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22743/22744 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22744/22744 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22744/22745 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22745/22745 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22745/22746 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22746/22746 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22746/22747 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22747/22747 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22747/22748 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22748/22748 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22748/22749 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22749/22749 [00:04<00:00,  4.03s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22749/22750 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22750/22750 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22750/22751 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22751/22751 [00:05<00:00,  5.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22751/22752 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22752/22752 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 22753/22753 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 22754/22754 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22754/22755 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22755/22755 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22755/22756 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22756/22756 [00:04<00:00,  4.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 22757/22757 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 22758/22758 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22758/22759 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22759/22759 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22759/22760 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22760/22760 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22760/22761 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22761/22761 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 22762/22762 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22762/22763 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22763/22763 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 22764/22764 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22764/22765 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22765/22765 [00:04<00:00,  4.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 22766/22766 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 22767/22767 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 22768/22768 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22768/22769 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22769/22769 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22769/22770 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22770/22770 [00:04<00:00,  4.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22770/22771 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22771/22771 [00:04<00:00,  4.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22771/22772 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22772/22772 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22772/22773 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22773/22773 [00:05<00:00,  5.56s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22773/22774 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22774/22774 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22774/22775 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22775/22775 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22775/22776 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22776/22776 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22776/22777 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22777/22777 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22777/22778 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22778/22778 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22778/22779 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22779/22779 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22779/22780 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22780/22780 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22780/22781 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22781/22781 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22781/22782 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22782/22782 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22782/22783 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22783/22783 [00:04<00:00,  4.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22783/22784 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22784/22784 [00:05<00:00,  5.39s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22784/22785 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22785/22785 [00:04<00:00,  4.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22785/22786 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22786/22786 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22786/22787 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22787/22787 [00:04<00:00,  4.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22787/22788 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22788/22788 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22788/22789 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22789/22789 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22789/22790 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22790/22790 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22790/22791 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22791/22791 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22791/22792 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22792/22792 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22792/22793 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22793/22793 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22793/22794 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22794/22794 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22794/22795 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22795/22795 [00:05<00:00,  5.40s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22795/22796 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22796/22796 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22796/22797 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22797/22797 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22797/22798 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22798/22798 [00:04<00:00,  4.08s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22798/22799 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22799/22799 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22799/22800 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22800/22800 [00:03<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22800/22801 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22801/22801 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22801/22802 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22802/22802 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22802/22803 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22803/22803 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22803/22804 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22804/22804 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22804/22805 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22805/22805 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22805/22806 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22806/22806 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22806/22807 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22807/22807 [00:04<00:00,  4.03s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22807/22808 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22808/22808 [00:04<00:00,  4.42s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22808/22809 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22809/22809 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22809/22810 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22810/22810 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22810/22811 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22811/22811 [00:04<00:00,  4.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 22812/22812 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22812/22813 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22813/22813 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22813/22814 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22814/22814 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22814/22815 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22815/22815 [00:04<00:00,  4.34s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22815/22816 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22816/22816 [00:04<00:00,  4.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22816/22817 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22817/22817 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22817/22818 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22818/22818 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22818/22819 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22819/22819 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22819/22820 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22820/22820 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22820/22821 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22821/22821 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22821/22822 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22822/22822 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22822/22823 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22823/22823 [00:04<00:00,  4.32s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22823/22824 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22824/22824 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22824/22825 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22825/22825 [00:04<00:00,  4.30s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22825/22826 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22826/22826 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22826/22827 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22827/22827 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22827/22828 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22828/22828 [00:04<00:00,  4.29s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22828/22829 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22829/22829 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22829/22830 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22830/22830 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22830/22831 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22831/22831 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22831/22832 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22832/22832 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22832/22833 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22833/22833 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22833/22834 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22834/22834 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22835/22835 [00:04<00:00,  4.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 22836/22836 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22836/22837 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22837/22837 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22837/22838 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22838/22838 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22838/22839 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22839/22839 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22839/22840 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22840/22840 [00:05<00:00,  5.35s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22840/22841 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22841/22841 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22841/22842 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22842/22842 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22842/22843 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22843/22843 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22843/22844 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22844/22844 [00:04<00:00,  4.33s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22844/22845 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22845/22845 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22845/22846 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22846/22846 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22846/22847 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22847/22847 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22847/22848 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22848/22848 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22848/22849 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22849/22849 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22849/22850 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22850/22850 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 22850/22851 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22851/22851 [00:05<00:00,  5.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 22852/22852 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 22853/22853 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 22854/22854 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 22855/22855 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 22856/22856 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 22857/22857 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 22858/22858 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 22859/22859 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 22860/22860 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 22861/22861 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 22862/22862 [00:04<00:00,  4.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 22863/22863 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 22864/22864 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 22865/22865 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 22866/22866 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 22867/22867 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 22868/22868 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 22869/22869 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 22870/22870 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 22871/22871 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 22872/22872 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 22873/22873 [00:04<00:00,  4.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 22874/22874 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 22875/22875 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22876/22876 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 22877/22877 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 22878/22878 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 22879/22879 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 22880/22880 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 22881/22881 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 22882/22882 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 22883/22883 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 22884/22884 [00:04<00:00,  4.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 22885/22885 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 22886/22886 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 22887/22887 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 22888/22888 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 22889/22889 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 22890/22890 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 22891/22891 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 22892/22892 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 22893/22893 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 22894/22894 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 22895/22895 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 22896/22896 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 22897/22897 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 22898/22898 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 22899/22899 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 22900/22900 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 22901/22901 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 22902/22902 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 22903/22903 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 22904/22904 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 22905/22905 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 22906/22906 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 22907/22907 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 22908/22908 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 22909/22909 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 22910/22910 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 22911/22911 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 22912/22912 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 22913/22913 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 22914/22914 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 22915/22915 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 22916/22916 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 22917/22917 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 22918/22918 [00:04<00:00,  5.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 22919/22919 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 22920/22920 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 22921/22921 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 22922/22922 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 22923/22923 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 22924/22924 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 22925/22925 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 22926/22926 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 22927/22927 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 22928/22928 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 22929/22929 [00:04<00:00,  4.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 22930/22930 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 22931/22931 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 22932/22932 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 22933/22933 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 22934/22934 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 22935/22935 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 22936/22936 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 22937/22937 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 22938/22938 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 22939/22939 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 22940/22940 [00:05<00:00,  5.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 22941/22941 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 22942/22942 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 22943/22943 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 22944/22944 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 22945/22945 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 22946/22946 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 22947/22947 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 22948/22948 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 22949/22949 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 22950/22950 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 22951/22951 [00:04<00:00,  4.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 22952/22952 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 22953/22953 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 22954/22954 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 22955/22955 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 22956/22956 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 22957/22957 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 22958/22958 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 22959/22959 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 22960/22960 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 22961/22961 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 22962/22962 [00:05<00:00,  5.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 22963/22963 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 22964/22964 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 22965/22965 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 22966/22966 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 22967/22967 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 22968/22968 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 22969/22969 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 22970/22970 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 22971/22971 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 22972/22972 [00:04<00:00,  4.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 22973/22973 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 22974/22974 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 22975/22975 [00:04<00:00,  4.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 22976/22976 [00:04<00:00,  4.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 22977/22977 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 22978/22978 [00:04<00:00,  4.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 22979/22979 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 22980/22980 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 22981/22981 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 22982/22982 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 22983/22983 [00:04<00:00,  4.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 22984/22984 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 22985/22985 [00:04<00:00,  4.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 22986/22986 [00:04<00:00,  4.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 22987/22987 [00:04<00:00,  4.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 22988/22988 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 22989/22989 [00:04<00:00,  4.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 22990/22990 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 22991/22991 [00:04<00:00,  4.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 22992/22992 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 22993/22993 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 22994/22994 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 22995/22995 [00:04<00:00,  4.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 22996/22996 [00:05<00:00,  5.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 22997/22997 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 22998/22998 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 22999/22999 [00:04<00:00,  4.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 23000/23000 [00:04<00:00,  4.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 23001/23001 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 23002/23002 [00:04<00:00,  4.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 23003/23003 [00:04<00:00,  4.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 23004/23004 [00:04<00:00,  4.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 23005/23005 [00:04<00:00,  4.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 23006/23006 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 23007/23007 [00:05<00:00,  5.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 23008/23008 [00:04<00:00,  4.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 23009/23009 [00:04<00:00,  4.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 23010/23010 [00:04<00:00,  4.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 23011/23011 [00:04<00:00,  4.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 23012/23012 [00:04<00:00,  4.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 23013/23013 [00:04<00:00,  4.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 23014/23014 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 23015/23015 [00:04<00:00,  4.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 23016/23016 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 23017/23017 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 23018/23018 [00:05<00:00,  5.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23019/23019 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 23020/23020 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 23021/23021 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 23022/23022 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 23023/23023 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 23024/23024 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 23025/23025 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 23026/23026 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 23027/23027 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 23028/23028 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 23029/23029 [00:05<00:00,  5.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 23030/23030 [00:04<00:00,  4.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 23031/23031 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 23032/23032 [00:04<00:00,  4.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 23033/23033 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23034/23034 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 23035/23035 [00:04<00:00,  4.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23036/23036 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23037/23037 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23038/23038 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23039/23039 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23040/23040 [00:04<00:00,  4.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23041/23041 [00:05<00:00,  5.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 23042/23042 [00:04<00:00,  4.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23043/23043 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 23044/23044 [00:04<00:00,  4.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 23045/23045 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 23046/23046 [00:04<00:00,  4.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 23047/23047 [00:04<00:00,  4.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23048/23048 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23049/23049 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 23050/23050 [00:04<00:00,  4.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 23051/23051 [00:04<00:00,  4.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 23052/23052 [00:05<00:00,  5.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 23053/23053 [00:04<00:00,  4.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 23054/23054 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23055/23055 [00:04<00:00,  4.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 23056/23056 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 23057/23057 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 23058/23058 [00:04<00:00,  4.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 23059/23059 [00:04<00:00,  4.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 23060/23060 [00:04<00:00,  4.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 23061/23061 [00:04<00:00,  4.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 23062/23062 [00:04<00:00,  4.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23063/23063 [00:05<00:00,  5.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 23064/23064 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23065/23065 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 23066/23066 [00:04<00:00,  4.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23067/23067 [00:04<00:00,  4.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 23068/23068 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 23069/23069 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 23070/23070 [00:04<00:00,  4.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 23071/23071 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 23072/23072 [00:04<00:00,  4.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 23073/23073 [00:04<00:00,  4.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 23074/23074 [00:05<00:00,  5.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 23075/23075 [00:04<00:00,  4.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 23076/23076 [00:04<00:00,  4.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 23077/23077 [00:04<00:00,  4.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 23078/23078 [00:04<00:00,  4.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 23079/23079 [00:04<00:00,  4.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 23080/23080 [00:04<00:00,  4.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 23081/23081 [00:04<00:00,  4.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 23082/23082 [00:04<00:00,  4.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 23083/23083 [00:04<00:00,  4.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 23084/23084 [00:04<00:00,  4.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 23085/23085 [00:04<00:00,  4.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 23086/23086 [00:05<00:00,  5.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 23087/23087 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 23088/23088 [00:04<00:00,  4.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 23089/23089 [00:04<00:00,  4.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 23090/23090 [00:04<00:00,  4.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 23091/23091 [00:04<00:00,  4.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 23092/23092 [00:04<00:00,  4.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 23093/23093 [00:04<00:00,  4.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 23094/23094 [00:04<00:00,  4.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 23095/23095 [00:04<00:00,  4.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 23096/23096 [00:04<00:00,  4.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 23097/23097 [00:05<00:00,  5.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 23098/23098 [00:04<00:00,  4.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 23099/23099 [00:04<00:00,  4.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 23100/23100 [00:04<00:00,  4.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 23101/23101 [00:05<00:00,  5.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 23102/23102 [00:05<00:00,  5.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 23103/23103 [00:04<00:00,  4.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 23104/23104 [00:04<00:00,  4.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 23105/23105 [00:04<00:00,  5.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 23106/23106 [00:05<00:00,  5.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 23107/23107 [00:04<00:00,  4.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 23108/23108 [00:06<00:00,  6.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23109/23109 [00:05<00:00,  5.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 23110/23110 [00:05<00:00,  5.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 23111/23111 [00:05<00:00,  5.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 23112/23112 [00:04<00:00,  4.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 23113/23113 [00:04<00:00,  4.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 23114/23114 [00:05<00:00,  5.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 23115/23115 [00:04<00:00,  4.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 23116/23116 [00:05<00:00,  5.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 23117/23117 [00:05<00:00,  5.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 23118/23118 [00:05<00:00,  5.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 23119/23119 [00:05<00:00,  5.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 23120/23120 [00:05<00:00,  5.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 23121/23121 [00:05<00:00,  5.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 23122/23122 [00:04<00:00,  4.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 23123/23123 [00:05<00:00,  5.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23124/23124 [00:05<00:00,  5.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 23125/23125 [00:05<00:00,  5.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 23126/23126 [00:05<00:00,  5.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23127/23127 [00:05<00:00,  5.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23128/23128 [00:05<00:00,  5.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23129/23129 [00:05<00:00,  5.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23130/23130 [00:06<00:00,  6.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 23131/23131 [00:05<00:00,  5.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23132/23132 [00:05<00:00,  5.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23133/23133 [00:05<00:00,  5.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 23134/23134 [00:05<00:00,  5.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23135/23135 [00:05<00:00,  5.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 23136/23136 [00:05<00:00,  5.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 23137/23137 [00:05<00:00,  5.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 23138/23138 [00:05<00:00,  5.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 23139/23139 [00:05<00:00,  5.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 23140/23140 [00:05<00:00,  5.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23141/23141 [00:06<00:00,  6.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 23142/23142 [00:05<00:00,  5.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23143/23143 [00:05<00:00,  5.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 23144/23144 [00:05<00:00,  5.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 23145/23145 [00:05<00:00,  5.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 23146/23146 [00:05<00:00,  5.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 23147/23147 [00:05<00:00,  5.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 23148/23148 [00:05<00:00,  5.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 23149/23149 [00:05<00:00,  5.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 23150/23150 [00:05<00:00,  5.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 23151/23151 [00:05<00:00,  5.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 23152/23152 [00:05<00:00,  5.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 23153/23153 [00:05<00:00,  5.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 23154/23154 [00:05<00:00,  5.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 23155/23155 [00:05<00:00,  5.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 23156/23156 [00:05<00:00,  5.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 23157/23157 [00:05<00:00,  5.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 23158/23158 [00:05<00:00,  5.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 23159/23159 [00:05<00:00,  5.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 23160/23160 [00:05<00:00,  5.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 23161/23161 [00:05<00:00,  5.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 23162/23162 [00:05<00:00,  5.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 23163/23163 [00:05<00:00,  5.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 23164/23164 [00:06<00:00,  6.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 23165/23165 [00:05<00:00,  5.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 23166/23166 [00:05<00:00,  5.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 23167/23167 [00:05<00:00,  5.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 23168/23168 [00:05<00:00,  5.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 23169/23169 [00:05<00:00,  5.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 23170/23170 [00:05<00:00,  5.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 23171/23171 [00:05<00:00,  5.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 23172/23172 [00:05<00:00,  5.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 23173/23173 [00:05<00:00,  5.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 23174/23174 [00:05<00:00,  5.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 23175/23175 [00:07<00:00,  7.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 23176/23176 [00:05<00:00,  5.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 23177/23177 [00:05<00:00,  5.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 23178/23178 [00:05<00:00,  5.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 23179/23179 [00:05<00:00,  5.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 23180/23180 [00:05<00:00,  5.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 23181/23181 [00:05<00:00,  5.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 23182/23182 [00:06<00:00,  6.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 23183/23183 [00:06<00:00,  6.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 23184/23184 [00:05<00:00,  5.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 23185/23185 [00:06<00:00,  6.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 23186/23186 [00:06<00:00,  6.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 23187/23187 [00:05<00:00,  5.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 23188/23188 [00:06<00:00,  6.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 23189/23189 [00:06<00:00,  6.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23190/23190 [00:06<00:00,  6.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23191/23191 [00:05<00:00,  5.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 23192/23192 [00:06<00:00,  6.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23193/23193 [00:06<00:00,  6.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23194/23194 [00:06<00:00,  6.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 23195/23195 [00:06<00:00,  6.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23196/23196 [00:06<00:00,  6.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 23197/23197 [00:07<00:00,  7.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 23198/23198 [00:06<00:00,  6.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 23199/23199 [00:06<00:00,  6.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 23200/23200 [00:06<00:00,  6.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23201/23201 [00:06<00:00,  6.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 23202/23202 [00:06<00:00,  6.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 23203/23203 [00:06<00:00,  6.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23204/23204 [00:06<00:00,  6.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23205/23205 [00:06<00:00,  6.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23206/23206 [00:06<00:00,  6.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 23207/23207 [00:06<00:00,  6.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23208/23208 [00:07<00:00,  7.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 23209/23209 [00:06<00:00,  6.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23210/23210 [00:06<00:00,  6.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 23211/23211 [00:06<00:00,  6.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 23212/23212 [00:06<00:00,  6.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 23213/23213 [00:06<00:00,  6.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23214/23214 [00:06<00:00,  6.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 23215/23215 [00:06<00:00,  6.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 23216/23216 [00:06<00:00,  6.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 23217/23217 [00:06<00:00,  6.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 23218/23218 [00:06<00:00,  6.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 23219/23219 [00:07<00:00,  7.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 23220/23220 [00:06<00:00,  6.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 23221/23221 [00:06<00:00,  6.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 23222/23222 [00:06<00:00,  6.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 23223/23223 [00:06<00:00,  6.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 23224/23224 [00:06<00:00,  6.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 23225/23225 [00:06<00:00,  6.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 23226/23226 [00:06<00:00,  6.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 23227/23227 [00:07<00:00,  7.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 23228/23228 [00:07<00:00,  7.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 23229/23229 [00:07<00:00,  7.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 23230/23230 [00:06<00:00,  6.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 23231/23231 [00:07<00:00,  7.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 23232/23232 [00:07<00:00,  7.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 23233/23233 [00:07<00:00,  7.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 23234/23234 [00:06<00:00,  6.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 23235/23235 [00:06<00:00,  6.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 23236/23236 [00:06<00:00,  6.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 23237/23237 [00:06<00:00,  6.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 23238/23238 [00:06<00:00,  6.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 23239/23239 [00:07<00:00,  7.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 23240/23240 [00:06<00:00,  6.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 23241/23241 [00:07<00:00,  7.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 23242/23242 [00:08<00:00,  8.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 23243/23243 [00:07<00:00,  7.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23244/23244 [00:07<00:00,  7.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23245/23245 [00:07<00:00,  7.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 23246/23246 [00:06<00:00,  6.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 23247/23247 [00:07<00:00,  7.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23248/23248 [00:07<00:00,  7.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 23249/23249 [00:07<00:00,  7.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23250/23250 [00:07<00:00,  7.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 23251/23251 [00:07<00:00,  7.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 23252/23252 [00:07<00:00,  7.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23253/23253 [00:08<00:00,  8.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 23254/23254 [00:07<00:00,  7.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 23255/23255 [00:07<00:00,  7.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23256/23256 [00:07<00:00,  7.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 23257/23257 [00:07<00:00,  7.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 23258/23258 [00:07<00:00,  7.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 23259/23259 [00:07<00:00,  7.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 23260/23260 [00:07<00:00,  7.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 23261/23261 [00:07<00:00,  7.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23262/23262 [00:07<00:00,  7.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 23263/23263 [00:07<00:00,  7.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 23264/23264 [00:08<00:00,  8.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 23265/23265 [00:07<00:00,  7.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 23266/23266 [00:07<00:00,  7.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 23267/23267 [00:07<00:00,  7.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 23268/23268 [00:07<00:00,  7.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 23269/23269 [00:07<00:00,  7.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23270/23270 [00:08<00:00,  8.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 23271/23271 [00:07<00:00,  7.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 23272/23272 [00:07<00:00,  7.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 23273/23273 [00:07<00:00,  7.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 23274/23274 [00:08<00:00,  8.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 23275/23275 [00:08<00:00,  8.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 23276/23276 [00:09<00:00,  9.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 23277/23277 [00:07<00:00,  7.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 23278/23278 [00:07<00:00,  7.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 23279/23279 [00:08<00:00,  8.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 23280/23280 [00:08<00:00,  8.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 23281/23281 [00:08<00:00,  8.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 23282/23282 [00:08<00:00,  8.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 23283/23283 [00:08<00:00,  8.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 23284/23284 [00:08<00:00,  8.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 23285/23285 [00:07<00:00,  7.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 23286/23286 [00:08<00:00,  8.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 23287/23287 [00:09<00:00,  9.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 23288/23288 [00:08<00:00,  8.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 23289/23289 [00:08<00:00,  8.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23290/23290 [00:08<00:00,  8.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 23291/23291 [00:08<00:00,  8.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 23292/23292 [00:08<00:00,  8.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 23293/23293 [00:08<00:00,  8.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 23294/23294 [00:08<00:00,  8.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 23295/23295 [00:08<00:00,  8.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 23296/23296 [00:08<00:00,  8.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 23297/23297 [00:08<00:00,  8.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 23298/23298 [00:09<00:00,  9.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 23299/23299 [00:08<00:00,  8.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 23300/23300 [00:08<00:00,  8.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 23301/23301 [00:08<00:00,  8.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 23302/23302 [00:08<00:00,  8.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 23303/23303 [00:08<00:00,  8.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 23304/23304 [00:08<00:00,  8.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 23305/23305 [00:08<00:00,  8.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 23306/23306 [00:08<00:00,  8.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 23307/23307 [00:08<00:00,  8.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 23308/23308 [00:08<00:00,  8.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 23309/23309 [00:09<00:00,  9.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 23310/23310 [00:08<00:00,  8.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 23311/23311 [00:08<00:00,  8.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 23312/23312 [00:08<00:00,  8.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 23313/23313 [00:08<00:00,  8.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 23314/23314 [00:08<00:00,  8.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 23315/23315 [00:08<00:00,  8.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 23316/23316 [00:08<00:00,  8.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 23317/23317 [00:08<00:00,  8.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 23318/23318 [00:08<00:00,  8.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 23319/23319 [00:08<00:00,  8.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 23320/23320 [00:09<00:00,  9.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 23321/23321 [00:09<00:00,  9.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 23322/23322 [00:08<00:00,  8.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 23323/23323 [00:08<00:00,  8.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 23324/23324 [00:08<00:00,  8.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 23325/23325 [00:08<00:00,  8.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 23326/23326 [00:08<00:00,  8.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 23327/23327 [00:08<00:00,  8.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23328/23328 [00:08<00:00,  8.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23329/23329 [00:08<00:00,  8.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 23330/23330 [00:08<00:00,  8.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 23331/23331 [00:08<00:00,  8.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 23332/23332 [00:09<00:00,  9.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 23333/23333 [00:08<00:00,  8.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 23334/23334 [00:08<00:00,  8.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 23335/23335 [00:08<00:00,  8.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 23336/23336 [00:08<00:00,  8.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 23337/23337 [00:07<00:00,  7.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 23338/23338 [00:08<00:00,  8.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 23339/23339 [00:08<00:00,  8.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 23340/23340 [00:07<00:00,  7.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 23341/23341 [00:08<00:00,  8.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 23342/23342 [00:08<00:00,  8.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 23343/23343 [00:09<00:00,  9.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 23344/23344 [00:07<00:00,  7.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 23345/23345 [00:07<00:00,  7.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 23346/23346 [00:07<00:00,  7.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 23347/23347 [00:08<00:00,  8.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 23348/23348 [00:07<00:00,  7.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 23349/23349 [00:07<00:00,  7.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 23350/23350 [00:07<00:00,  7.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 23351/23351 [00:08<00:00,  8.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 23352/23352 [00:07<00:00,  7.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 23353/23353 [00:07<00:00,  7.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 23354/23354 [00:08<00:00,  8.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 23355/23355 [00:07<00:00,  7.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 23356/23356 [00:07<00:00,  7.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 23357/23357 [00:07<00:00,  7.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 23358/23358 [00:07<00:00,  7.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 23359/23359 [00:07<00:00,  7.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 23360/23360 [00:07<00:00,  7.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 23361/23361 [00:07<00:00,  7.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 23362/23362 [00:07<00:00,  7.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 23363/23363 [00:07<00:00,  7.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 23364/23364 [00:07<00:00,  7.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23365/23365 [00:08<00:00,  8.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 23366/23366 [00:08<00:00,  8.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 23367/23367 [00:07<00:00,  7.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23368/23368 [00:07<00:00,  7.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23369/23369 [00:07<00:00,  7.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 23370/23370 [00:07<00:00,  7.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 23371/23371 [00:07<00:00,  7.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23372/23372 [00:07<00:00,  7.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 23373/23373 [00:07<00:00,  7.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23374/23374 [00:07<00:00,  7.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23375/23375 [00:07<00:00,  7.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23376/23376 [00:07<00:00,  7.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23377/23377 [00:08<00:00,  8.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 23378/23378 [00:07<00:00,  7.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23379/23379 [00:07<00:00,  7.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23380/23380 [00:07<00:00,  7.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23381/23381 [00:07<00:00,  7.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 23382/23382 [00:07<00:00,  7.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23383/23383 [00:07<00:00,  7.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 23384/23384 [00:07<00:00,  7.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23385/23385 [00:07<00:00,  7.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23386/23386 [00:07<00:00,  7.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 23387/23387 [00:07<00:00,  7.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 23388/23388 [00:08<00:00,  8.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 23389/23389 [00:07<00:00,  7.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 23390/23390 [00:07<00:00,  7.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 23391/23391 [00:06<00:00,  6.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 23392/23392 [00:07<00:00,  7.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 23393/23393 [00:07<00:00,  7.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 23394/23394 [00:07<00:00,  7.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 23395/23395 [00:07<00:00,  7.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 23396/23396 [00:06<00:00,  6.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 23397/23397 [00:06<00:00,  6.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 23398/23398 [00:07<00:00,  7.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 23399/23399 [00:08<00:00,  8.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 23400/23400 [00:07<00:00,  7.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23401/23401 [00:07<00:00,  7.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 23402/23402 [00:06<00:00,  6.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 23403/23403 [00:06<00:00,  6.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 23404/23404 [00:06<00:00,  6.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 23405/23405 [00:06<00:00,  6.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 23406/23406 [00:06<00:00,  6.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 23407/23407 [00:06<00:00,  6.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 23408/23408 [00:06<00:00,  6.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 23409/23409 [00:06<00:00,  6.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 23410/23410 [00:08<00:00,  8.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 23411/23411 [00:06<00:00,  6.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 23412/23412 [00:06<00:00,  6.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 23413/23413 [00:06<00:00,  6.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 23414/23414 [00:06<00:00,  6.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 23415/23415 [00:06<00:00,  6.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 23416/23416 [00:06<00:00,  6.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 23417/23417 [00:06<00:00,  6.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 23418/23418 [00:06<00:00,  6.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 23419/23419 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23420/23420 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23421/23421 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23422/23422 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 23423/23423 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23424/23424 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23425/23425 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23426/23426 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23427/23427 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23428/23428 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23429/23429 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23430/23430 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23431/23431 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23432/23432 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 23433/23433 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23434/23434 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23435/23435 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23436/23436 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23437/23437 [00:02<00:00,  2.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 23438/23438 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23439/23439 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23440/23440 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23441/23441 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23442/23442 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23443/23443 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23444/23444 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 23445/23445 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23446/23446 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23447/23447 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23448/23448 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23449/23449 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23450/23450 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23451/23451 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23452/23452 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23453/23453 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23454/23454 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23455/23455 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 23456/23456 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23457/23457 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23458/23458 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23459/23459 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23460/23460 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23461/23461 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23462/23462 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23463/23463 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23464/23464 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23465/23465 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23466/23466 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 23467/23467 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23468/23468 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23469/23469 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23470/23470 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23471/23471 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23472/23472 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23473/23473 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23474/23474 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23475/23475 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 23476/23476 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23477/23477 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 23478/23478 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23479/23479 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23480/23480 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23481/23481 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23482/23482 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23483/23483 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23484/23484 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23485/23485 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23486/23486 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23487/23487 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23488/23488 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23489/23489 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23490/23490 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23491/23491 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23492/23492 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23493/23493 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23494/23494 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23495/23495 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23496/23496 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23497/23497 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23498/23498 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23499/23499 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23500/23500 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23501/23501 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23502/23502 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23503/23503 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23504/23504 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23505/23505 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23506/23506 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23507/23507 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23508/23508 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23509/23509 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23510/23510 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23511/23511 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 23512/23512 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23513/23513 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23514/23514 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23515/23515 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23516/23516 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23517/23517 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23518/23518 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23519/23519 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23520/23520 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23521/23521 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23522/23522 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 23523/23523 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23524/23524 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23525/23525 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23526/23526 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23527/23527 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23528/23528 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23529/23529 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23530/23530 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23531/23531 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 23532/23532 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23533/23533 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 23534/23534 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23535/23535 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23536/23536 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23537/23537 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23538/23538 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23539/23539 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23540/23540 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23541/23541 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23542/23542 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23543/23543 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 23544/23544 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 23545/23545 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23546/23546 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23547/23547 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23548/23548 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23549/23549 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23550/23550 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23551/23551 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23552/23552 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23553/23553 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 23554/23554 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23555/23555 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23556/23556 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 23557/23557 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23558/23558 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23559/23559 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23560/23560 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23561/23561 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23562/23562 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23563/23563 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23564/23564 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23565/23565 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23566/23566 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23567/23567 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 23568/23568 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23569/23569 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23570/23570 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23571/23571 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23572/23572 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23573/23573 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23574/23574 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23575/23575 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23576/23576 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23577/23577 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23578/23578 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 23579/23579 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23580/23580 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23581/23581 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23582/23582 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23583/23583 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23584/23584 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23585/23585 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23586/23586 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23587/23587 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 23588/23588 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23589/23589 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 23590/23590 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23591/23591 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23592/23592 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23593/23593 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23594/23594 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23595/23595 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23596/23596 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23597/23597 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23598/23598 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23599/23599 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23600/23600 [00:02<00:00,  2.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 23601/23601 [00:02<00:00,  2.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 23602/23602 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23603/23603 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23604/23604 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23605/23605 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 23606/23606 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 23607/23607 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23608/23608 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 23609/23609 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23610/23610 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 23611/23611 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23612/23612 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 23613/23613 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 23614/23614 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 23615/23615 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 23616/23616 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 23617/23617 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23618/23618 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23619/23619 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23620/23620 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23621/23621 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23622/23622 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23623/23623 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 23624/23624 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23625/23625 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23626/23626 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23627/23627 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23628/23628 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23629/23629 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23630/23630 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23631/23631 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23632/23632 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23633/23633 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23634/23634 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 23635/23635 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23636/23636 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23637/23637 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23638/23638 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23639/23639 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23640/23640 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23641/23641 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23642/23642 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23643/23643 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23644/23644 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23645/23645 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 23646/23646 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23647/23647 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23648/23648 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23649/23649 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23650/23650 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23651/23651 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23652/23652 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23653/23653 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23654/23654 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23655/23655 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23656/23656 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23657/23657 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 23658/23658 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23659/23659 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23660/23660 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23661/23661 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23662/23662 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23663/23663 [00:02<00:00,  2.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 23664/23664 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23665/23665 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23666/23666 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23667/23667 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23668/23668 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 23669/23669 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23670/23670 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23671/23671 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23672/23672 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23673/23673 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23674/23674 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23675/23675 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23676/23676 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23677/23677 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23678/23678 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23679/23679 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 23680/23680 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23681/23681 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23682/23682 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23683/23683 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23684/23684 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23685/23685 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23686/23686 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23687/23687 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23688/23688 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23689/23689 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23690/23690 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23691/23691 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23692/23692 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23693/23693 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23694/23694 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23695/23695 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23696/23696 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23697/23697 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23698/23698 [00:02<00:00,  2.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 23699/23699 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23700/23700 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23701/23701 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23702/23702 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 23703/23703 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23704/23704 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23705/23705 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23706/23706 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23707/23707 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23708/23708 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23709/23709 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23710/23710 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23711/23711 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23712/23712 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23713/23713 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 23714/23714 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23715/23715 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23716/23716 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23717/23717 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23718/23718 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23719/23719 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23720/23720 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23721/23721 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23722/23722 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23723/23723 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23724/23724 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 23725/23725 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23726/23726 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23727/23727 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23728/23728 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23729/23729 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23730/23730 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23731/23731 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23732/23732 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 23733/23733 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23734/23734 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23735/23735 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23736/23736 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 23737/23737 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23738/23738 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23739/23739 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23740/23740 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23741/23741 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23742/23742 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23743/23743 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23744/23744 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 23745/23745 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23746/23746 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23747/23747 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 23748/23748 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23749/23749 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23750/23750 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23751/23751 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23752/23752 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23753/23753 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23754/23754 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23755/23755 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23756/23756 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23757/23757 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23758/23758 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 23759/23759 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23760/23760 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23761/23761 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23762/23762 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23763/23763 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23764/23764 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23765/23765 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23766/23766 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23767/23767 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23768/23768 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23769/23769 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 23770/23770 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23771/23771 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23772/23772 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23773/23773 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23774/23774 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23775/23775 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23776/23776 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23777/23777 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23778/23778 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23779/23779 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23780/23780 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 23781/23781 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23782/23782 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23783/23783 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23784/23784 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23785/23785 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23786/23786 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23787/23787 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23788/23788 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23789/23789 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23790/23790 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23791/23791 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 23792/23792 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23793/23793 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23794/23794 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23795/23795 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23796/23796 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23797/23797 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23798/23798 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23799/23799 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23800/23800 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23801/23801 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23802/23802 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 23803/23803 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23804/23804 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23805/23805 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23806/23806 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23807/23807 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23808/23808 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23809/23809 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23810/23810 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23811/23811 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 23812/23812 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 23813/23813 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 23814/23814 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 23815/23815 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23816/23816 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23817/23817 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 23818/23818 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23819/23819 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23820/23820 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23821/23821 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23822/23822 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23823/23823 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23824/23824 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 23825/23825 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 23826/23826 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 23827/23827 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 23828/23828 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23829/23829 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 23830/23830 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23831/23831 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 23832/23832 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23833/23833 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23834/23834 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23835/23835 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23836/23836 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 23837/23837 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23838/23838 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23839/23839 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23840/23840 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23841/23841 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23842/23842 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23843/23843 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23844/23844 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23845/23845 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 23846/23846 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23847/23847 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 23848/23848 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23849/23849 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23850/23850 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23851/23851 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23852/23852 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 23853/23853 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23854/23854 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23855/23855 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23856/23856 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23857/23857 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23858/23858 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23859/23859 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 23860/23860 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23861/23861 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23862/23862 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23863/23863 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23864/23864 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23865/23865 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23866/23866 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23867/23867 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23868/23868 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23869/23869 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23870/23870 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23871/23871 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23872/23872 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23873/23873 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23874/23874 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23875/23875 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23876/23876 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23877/23877 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23878/23878 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23879/23879 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23880/23880 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23881/23881 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23882/23882 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23883/23883 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23884/23884 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23885/23885 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23886/23886 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23887/23887 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23888/23888 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23889/23889 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23890/23890 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23891/23891 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23892/23892 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 23893/23893 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23894/23894 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23895/23895 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23896/23896 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23897/23897 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23898/23898 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23899/23899 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23900/23900 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23901/23901 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23902/23902 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23903/23903 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 23904/23904 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23905/23905 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23906/23906 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23907/23907 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23908/23908 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23909/23909 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 23910/23910 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23911/23911 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 23912/23912 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 23913/23913 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23914/23914 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23915/23915 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23916/23916 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23917/23917 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23918/23918 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 23919/23919 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23920/23920 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23921/23921 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23922/23922 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23923/23923 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23924/23924 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23925/23925 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23926/23926 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23927/23927 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23928/23928 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23929/23929 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23930/23930 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23931/23931 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23932/23932 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23933/23933 [00:02<00:00,  2.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 23934/23934 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23935/23935 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23936/23936 [00:02<00:00,  2.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 23937/23937 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 23938/23938 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23939/23939 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23940/23940 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23941/23941 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23942/23942 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23943/23943 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23944/23944 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23945/23945 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23946/23946 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23947/23947 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23948/23948 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 23949/23949 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23950/23950 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23951/23951 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23952/23952 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23953/23953 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23954/23954 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23955/23955 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23956/23956 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23957/23957 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23958/23958 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23959/23959 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23960/23960 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 23961/23961 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23962/23962 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23963/23963 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23964/23964 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23965/23965 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23966/23966 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23967/23967 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 23968/23968 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23969/23969 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23970/23970 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23971/23971 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 23972/23972 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 23973/23973 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 23974/23974 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23975/23975 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 23976/23976 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 23977/23977 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23978/23978 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 23979/23979 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23980/23980 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23981/23981 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23982/23982 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 23983/23983 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 23984/23984 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23985/23985 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23986/23986 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23987/23987 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 23988/23988 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23989/23989 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23990/23990 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23991/23991 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 23992/23992 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 23993/23993 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 23994/23994 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23995/23995 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23996/23996 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 23997/23997 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 23998/23998 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 23999/23999 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24000/24000 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24001/24001 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24002/24002 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24003/24003 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24004/24004 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24005/24005 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 24006/24006 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24007/24007 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24008/24008 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24009/24009 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24010/24010 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24011/24011 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 24012/24012 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24013/24013 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24014/24014 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24015/24015 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24016/24016 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 24017/24017 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24018/24018 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24019/24019 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24020/24020 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24021/24021 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24022/24022 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24023/24023 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24024/24024 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24025/24025 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24026/24026 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24027/24027 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24028/24028 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 24029/24029 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24030/24030 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24031/24031 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 24032/24032 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24033/24033 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24034/24034 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24035/24035 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24036/24036 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24037/24037 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24038/24038 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 24039/24039 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 24040/24040 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24041/24041 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24042/24042 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24043/24043 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24044/24044 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24045/24045 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24046/24046 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24047/24047 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24048/24048 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24049/24049 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24050/24050 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 24051/24051 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24052/24052 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24053/24053 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24054/24054 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24055/24055 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24056/24056 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24057/24057 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24058/24058 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24059/24059 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24060/24060 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24061/24061 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 24062/24062 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24063/24063 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24064/24064 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24065/24065 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24066/24066 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 24067/24067 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24068/24068 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 24069/24069 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24070/24070 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24071/24071 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24072/24072 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 24073/24073 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24074/24074 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24075/24075 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24076/24076 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24077/24077 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24078/24078 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24079/24079 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24080/24080 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24081/24081 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24082/24082 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24083/24083 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 24084/24084 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24085/24085 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24086/24086 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24087/24087 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24088/24088 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24089/24089 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24090/24090 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24091/24091 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24092/24092 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24093/24093 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24094/24094 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24095/24095 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 24096/24096 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24097/24097 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24098/24098 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24099/24099 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24100/24100 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24101/24101 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24102/24102 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24103/24103 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24104/24104 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24105/24105 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24106/24106 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24107/24107 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24108/24108 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24109/24109 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24110/24110 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24111/24111 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24112/24112 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24113/24113 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24114/24114 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24115/24115 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24116/24116 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24117/24117 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24118/24118 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24119/24119 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24120/24120 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24121/24121 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24122/24122 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24123/24123 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24124/24124 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24125/24125 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24126/24126 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24127/24127 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24128/24128 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24129/24129 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 24130/24130 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24131/24131 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24132/24132 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24133/24133 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24134/24134 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24135/24135 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24136/24136 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 24137/24137 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24138/24138 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24139/24139 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24140/24140 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 24141/24141 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24142/24142 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24143/24143 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24144/24144 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24145/24145 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24146/24146 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24147/24147 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24148/24148 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24149/24149 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24150/24150 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24151/24151 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24152/24152 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24153/24153 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24154/24154 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24155/24155 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24156/24156 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24157/24157 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24158/24158 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24159/24159 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24160/24160 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24161/24161 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24162/24162 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24163/24163 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24164/24164 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24165/24165 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24166/24166 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24167/24167 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24168/24168 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24169/24169 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24170/24170 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24171/24171 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24172/24172 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24173/24173 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24174/24174 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24175/24175 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24176/24176 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24177/24177 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24178/24178 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24179/24179 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24180/24180 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24181/24181 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24182/24182 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24183/24183 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24184/24184 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24185/24185 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24186/24186 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24187/24187 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24188/24188 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24189/24189 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24190/24190 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24191/24191 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24192/24192 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24193/24193 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24194/24194 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24195/24195 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24196/24196 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24197/24197 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24198/24198 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24199/24199 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24200/24200 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24201/24201 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24202/24202 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24203/24203 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24204/24204 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24205/24205 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24206/24206 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24207/24207 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 24208/24208 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 24209/24209 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24210/24210 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24211/24211 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24212/24212 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24213/24213 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24214/24214 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24215/24215 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24216/24216 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 24217/24217 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24218/24218 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 24219/24219 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24220/24220 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24221/24221 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24222/24222 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 24223/24223 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24224/24224 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24225/24225 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24226/24226 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24227/24227 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24228/24228 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 24229/24229 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24230/24230 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24231/24231 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24232/24232 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24233/24233 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24234/24234 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24235/24235 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24236/24236 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24237/24237 [00:02<00:00,  2.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 24238/24238 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24239/24239 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 24240/24240 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24241/24241 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 24242/24242 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24243/24243 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 24244/24244 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24245/24245 [00:02<00:00,  2.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 24246/24246 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24247/24247 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24248/24248 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24249/24249 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24250/24250 [00:02<00:00,  2.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 24251/24251 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24252/24252 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 24253/24253 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 24254/24254 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24255/24255 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 24256/24256 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 24257/24257 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24258/24258 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24259/24259 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24260/24260 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24261/24261 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24262/24262 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24263/24263 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 24264/24264 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24265/24265 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24266/24266 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24267/24267 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24268/24268 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24269/24269 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24270/24270 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24271/24271 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24272/24272 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24273/24273 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24274/24274 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 24275/24275 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 24276/24276 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24277/24277 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 24278/24278 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 24279/24279 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24280/24280 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 24281/24281 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24282/24282 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 24283/24283 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24284/24284 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24285/24285 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24286/24286 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 24287/24287 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 24288/24288 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24289/24289 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24290/24290 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 24291/24291 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24292/24292 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24293/24293 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24294/24294 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24295/24295 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24296/24296 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24297/24297 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 24298/24298 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24299/24299 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24300/24300 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24301/24301 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24302/24302 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24303/24303 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24304/24304 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24305/24305 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24306/24306 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24307/24307 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24308/24308 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24309/24309 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 24310/24310 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24311/24311 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24312/24312 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24313/24313 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24314/24314 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24315/24315 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24316/24316 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24317/24317 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24318/24318 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24319/24319 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24320/24320 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 24321/24321 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24322/24322 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24323/24323 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24324/24324 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24325/24325 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24326/24326 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24327/24327 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24328/24328 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24329/24329 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24330/24330 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24331/24331 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 24332/24332 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24333/24333 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24334/24334 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24335/24335 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24336/24336 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24337/24337 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24338/24338 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24339/24339 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24340/24340 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24341/24341 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24342/24342 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 24343/24343 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24344/24344 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24345/24345 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24346/24346 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24347/24347 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24348/24348 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24349/24349 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24350/24350 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24351/24351 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24352/24352 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24353/24353 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24354/24354 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 24355/24355 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24356/24356 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24357/24357 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24358/24358 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24359/24359 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24360/24360 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24361/24361 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24362/24362 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24363/24363 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24364/24364 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24365/24365 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 24366/24366 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24367/24367 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24368/24368 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24369/24369 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24370/24370 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24371/24371 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24372/24372 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24373/24373 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24374/24374 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24375/24375 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24376/24376 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 24377/24377 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24378/24378 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24379/24379 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24380/24380 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24381/24381 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24382/24382 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24383/24383 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24384/24384 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24385/24385 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24386/24386 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24387/24387 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 24388/24388 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24389/24389 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24390/24390 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24391/24391 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24392/24392 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24393/24393 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24394/24394 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24395/24395 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24396/24396 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24397/24397 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24398/24398 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24399/24399 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24400/24400 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24401/24401 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24402/24402 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24403/24403 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24404/24404 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24405/24405 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24406/24406 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24407/24407 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24408/24408 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24409/24409 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24410/24410 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24411/24411 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24412/24412 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24413/24413 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24414/24414 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24415/24415 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24416/24416 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24417/24417 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24418/24418 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24419/24419 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24420/24420 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24421/24421 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 24422/24422 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24423/24423 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24424/24424 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24425/24425 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24426/24426 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24427/24427 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24428/24428 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24429/24429 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24430/24430 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24431/24431 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24432/24432 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 24433/24433 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24434/24434 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24435/24435 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24436/24436 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24437/24437 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24438/24438 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24439/24439 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24440/24440 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24441/24441 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24442/24442 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24443/24443 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24444/24444 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 24445/24445 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24446/24446 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24447/24447 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24448/24448 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24449/24449 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24450/24450 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24451/24451 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24452/24452 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24453/24453 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24454/24454 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24455/24455 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 24456/24456 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24457/24457 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24458/24458 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24459/24459 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24460/24460 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24461/24461 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24462/24462 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24463/24463 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24464/24464 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24465/24465 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24466/24466 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 24467/24467 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24468/24468 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24469/24469 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24470/24470 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24471/24471 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24472/24472 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24473/24473 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24474/24474 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24475/24475 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24476/24476 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24477/24477 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 24478/24478 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24479/24479 [00:02<00:00,  2.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 24480/24480 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24481/24481 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24482/24482 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24483/24483 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24484/24484 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24485/24485 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24486/24486 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24487/24487 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24488/24488 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24489/24489 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 24490/24490 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24491/24491 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24492/24492 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24493/24493 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24494/24494 [00:02<00:00,  2.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 24495/24495 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24496/24496 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24497/24497 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24498/24498 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24499/24499 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24500/24500 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 24501/24501 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24502/24502 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24503/24503 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24504/24504 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24505/24505 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24506/24506 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24507/24507 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24508/24508 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24509/24509 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24510/24510 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24511/24511 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 24512/24512 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24513/24513 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24514/24514 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24515/24515 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24516/24516 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24517/24517 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24518/24518 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24519/24519 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24520/24520 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24521/24521 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24522/24522 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24523/24523 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24524/24524 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24525/24525 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24526/24526 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24527/24527 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24528/24528 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24529/24529 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24530/24530 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24531/24531 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24532/24532 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24533/24533 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24534/24534 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24535/24535 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24536/24536 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24537/24537 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24538/24538 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24539/24539 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24540/24540 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24541/24541 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24542/24542 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24543/24543 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24544/24544 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24545/24545 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 24546/24546 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24547/24547 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24548/24548 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24549/24549 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24550/24550 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24551/24551 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24552/24552 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24553/24553 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24554/24554 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24555/24555 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24556/24556 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 24557/24557 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24558/24558 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24559/24559 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24560/24560 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24561/24561 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24562/24562 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 24563/24563 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 24564/24564 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 24565/24565 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 24566/24566 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 24567/24567 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 24568/24568 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 24569/24569 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24570/24570 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 24571/24571 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 24572/24572 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 24573/24573 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 24574/24574 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 24575/24575 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 24576/24576 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 24577/24577 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 24578/24578 [00:02<00:00,  2.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 24579/24579 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24580/24580 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24581/24581 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24582/24582 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24583/24583 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24584/24584 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24585/24585 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24586/24586 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24587/24587 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24588/24588 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24589/24589 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24590/24590 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 24591/24591 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24592/24592 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24593/24593 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24594/24594 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24595/24595 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 24596/24596 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24597/24597 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24598/24598 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24599/24599 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24600/24600 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24601/24601 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 24602/24602 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24603/24603 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24604/24604 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24605/24605 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24606/24606 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24607/24607 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24608/24608 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24609/24609 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24610/24610 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24611/24611 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24612/24612 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 24613/24613 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24614/24614 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24615/24615 [00:02<00:00,  2.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 24616/24616 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24617/24617 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24618/24618 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24619/24619 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24620/24620 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24621/24621 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24622/24622 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24623/24623 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 24624/24624 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24625/24625 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24626/24626 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24627/24627 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24628/24628 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24629/24629 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24630/24630 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24631/24631 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24632/24632 [00:02<00:00,  2.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 24633/24633 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24634/24634 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24635/24635 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24636/24636 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24637/24637 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24638/24638 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 24639/24639 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 24640/24640 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 24641/24641 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 24642/24642 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 24643/24643 [00:02<00:00,  2.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 24644/24644 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24645/24645 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24646/24646 [00:02<00:00,  2.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 24647/24647 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24648/24648 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24649/24649 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24650/24650 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24651/24651 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24652/24652 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24653/24653 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24654/24654 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24655/24655 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24656/24656 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24657/24657 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 24658/24658 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24659/24659 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24660/24660 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24661/24661 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24662/24662 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24663/24663 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24664/24664 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24665/24665 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24666/24666 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24667/24667 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24668/24668 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 24669/24669 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24670/24670 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24671/24671 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24672/24672 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24673/24673 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24674/24674 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24675/24675 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24676/24676 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24677/24677 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24678/24678 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24679/24679 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 24680/24680 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24681/24681 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24682/24682 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24683/24683 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24684/24684 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 24685/24685 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24686/24686 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24687/24687 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24688/24688 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24689/24689 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24690/24690 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24691/24691 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24692/24692 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24693/24693 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24694/24694 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24695/24695 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24696/24696 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24697/24697 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24698/24698 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24699/24699 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24700/24700 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24701/24701 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24702/24702 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 24703/24703 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24704/24704 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24705/24705 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24706/24706 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24707/24707 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24708/24708 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24709/24709 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24710/24710 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24711/24711 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24712/24712 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24713/24713 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 24714/24714 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24715/24715 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24716/24716 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24717/24717 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24718/24718 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24719/24719 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24720/24720 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24721/24721 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24722/24722 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24723/24723 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24724/24724 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 24725/24725 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24726/24726 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24727/24727 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24728/24728 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24729/24729 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24730/24730 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24731/24731 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24732/24732 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24733/24733 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24734/24734 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24735/24735 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 24736/24736 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24737/24737 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24738/24738 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24739/24739 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24740/24740 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24741/24741 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24742/24742 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24743/24743 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24744/24744 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24745/24745 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24746/24746 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24747/24747 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 24748/24748 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24749/24749 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24750/24750 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24751/24751 [00:02<00:00,  2.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 24752/24752 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24753/24753 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24754/24754 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24755/24755 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24756/24756 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24757/24757 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24758/24758 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 24759/24759 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24760/24760 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24761/24761 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24762/24762 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24763/24763 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24764/24764 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24765/24765 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24766/24766 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24767/24767 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24768/24768 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 24769/24769 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 24770/24770 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24771/24771 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24772/24772 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24773/24773 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24774/24774 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24775/24775 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24776/24776 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24777/24777 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24778/24778 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24779/24779 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24780/24780 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24781/24781 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 24782/24782 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24783/24783 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24784/24784 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24785/24785 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24786/24786 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24787/24787 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24788/24788 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24789/24789 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24790/24790 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24791/24791 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24792/24792 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 24793/24793 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24794/24794 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24795/24795 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24796/24796 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24797/24797 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24798/24798 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24799/24799 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24800/24800 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24801/24801 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24802/24802 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24803/24803 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 24804/24804 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24805/24805 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24806/24806 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24807/24807 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24808/24808 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24809/24809 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24810/24810 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24811/24811 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24812/24812 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24813/24813 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24814/24814 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 24815/24815 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24816/24816 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24817/24817 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24818/24818 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24819/24819 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24820/24820 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24821/24821 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24822/24822 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24823/24823 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24824/24824 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24825/24825 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24826/24826 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24827/24827 [00:02<00:00,  2.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 24828/24828 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24829/24829 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 24830/24830 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24831/24831 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24832/24832 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24833/24833 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24834/24834 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24835/24835 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24836/24836 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24837/24837 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 24838/24838 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24839/24839 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24840/24840 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24841/24841 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24842/24842 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24843/24843 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24844/24844 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24845/24845 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24846/24846 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24847/24847 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24848/24848 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 24849/24849 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24850/24850 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24851/24851 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24852/24852 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24853/24853 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24854/24854 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24855/24855 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24856/24856 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24857/24857 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24858/24858 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24859/24859 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 24860/24860 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24861/24861 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24862/24862 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24863/24863 [00:02<00:00,  2.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 24864/24864 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24865/24865 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24866/24866 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24867/24867 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24868/24868 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24869/24869 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24870/24870 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 24871/24871 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24872/24872 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24873/24873 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24874/24874 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24875/24875 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24876/24876 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24877/24877 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24878/24878 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24879/24879 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24880/24880 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24881/24881 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 24882/24882 [00:02<00:00,  2.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 24883/24883 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24884/24884 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24885/24885 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24886/24886 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24887/24887 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24888/24888 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24889/24889 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24890/24890 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24891/24891 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24892/24892 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24893/24893 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 24894/24894 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24895/24895 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24896/24896 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24897/24897 [00:02<00:00,  2.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 24898/24898 [00:02<00:00,  2.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 24899/24899 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24900/24900 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24901/24901 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24902/24902 [00:02<00:00,  2.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 24903/24903 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24904/24904 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 24905/24905 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24906/24906 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24907/24907 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24908/24908 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24909/24909 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24910/24910 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24911/24911 [00:02<00:00,  2.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 24912/24912 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24913/24913 [00:02<00:00,  2.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 24914/24914 [00:02<00:00,  2.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 24915/24915 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 24916/24916 [00:02<00:00,  2.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 24917/24917 [00:02<00:00,  2.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 24918/24918 [00:02<00:00,  2.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 24919/24919 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24920/24920 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24921/24921 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 24922/24922 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 24923/24923 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24924/24924 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24925/24925 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 24926/24926 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 24927/24927 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 24928/24928 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 24929/24929 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24930/24930 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 24931/24931 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 24932/24932 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24933/24933 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 24934/24934 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 24935/24935 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 24936/24936 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 24937/24937 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 24938/24938 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 24939/24939 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 24940/24940 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 24941/24941 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 24942/24942 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 24943/24943 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 24944/24944 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 24945/24945 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 24946/24946 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 24947/24947 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 24948/24948 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 24949/24949 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 24950/24950 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 24951/24951 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 24952/24952 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 24953/24953 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 24954/24954 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 24955/24955 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 24956/24956 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 24957/24957 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 24958/24958 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24959/24959 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 24960/24960 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 24961/24961 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 24962/24962 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 24963/24963 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 24964/24964 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 24965/24965 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 24966/24966 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24967/24967 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 24968/24968 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 24969/24969 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 24970/24970 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24971/24971 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 24972/24972 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 24973/24973 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 24974/24974 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 24975/24975 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 24976/24976 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 24977/24977 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 24978/24978 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 24979/24979 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 24980/24980 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 24981/24981 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 24982/24982 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 24983/24983 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 24984/24984 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 24985/24985 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 24986/24986 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 24987/24987 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 24988/24988 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 24989/24989 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 24990/24990 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 24991/24991 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 24992/24992 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 24993/24993 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24994/24994 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 24995/24995 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 24996/24996 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 24997/24997 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 24998/24998 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 24999/24999 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25000/25000 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 25001/25001 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 25002/25002 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 25003/25003 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 25004/25004 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 25005/25005 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 25006/25006 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 25007/25007 [00:02<00:00,  2.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 25008/25008 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25009/25009 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25010/25010 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25011/25011 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 25012/25012 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 25013/25013 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 25014/25014 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 25015/25015 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 25016/25016 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 25017/25017 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25018/25018 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25019/25019 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25020/25020 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 25021/25021 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 25022/25022 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25023/25023 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25024/25024 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 25025/25025 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25026/25026 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25027/25027 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 25028/25028 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25029/25029 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25030/25030 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25031/25031 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25032/25032 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25033/25033 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25034/25034 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25035/25035 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25036/25036 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25037/25037 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25038/25038 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25039/25039 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 25040/25040 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 25041/25041 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25042/25042 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25043/25043 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25044/25044 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25045/25045 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25046/25046 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25047/25047 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25048/25048 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25049/25049 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25050/25050 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 25051/25051 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25052/25052 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25053/25053 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25054/25054 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25055/25055 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25056/25056 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 25057/25057 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25058/25058 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25059/25059 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 25060/25060 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25061/25061 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 25062/25062 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 25063/25063 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25064/25064 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25065/25065 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 25066/25066 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25067/25067 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 25068/25068 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25069/25069 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 25070/25070 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 25071/25071 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 25072/25072 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25073/25073 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25074/25074 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25075/25075 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 25076/25076 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 25077/25077 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25078/25078 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 25079/25079 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25080/25080 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25081/25081 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25082/25082 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 25083/25083 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25084/25084 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 25085/25085 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25086/25086 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25087/25087 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25088/25088 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25089/25089 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25090/25090 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 25091/25091 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25092/25092 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25093/25093 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25094/25094 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25095/25095 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 25096/25096 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 25097/25097 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25098/25098 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 25099/25099 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25100/25100 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 25101/25101 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 25102/25102 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 25103/25103 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 25104/25104 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 25105/25105 [00:02<00:00,  2.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 25106/25106 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 25107/25107 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 25108/25108 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25109/25109 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 25110/25110 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 25111/25111 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25112/25112 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25113/25113 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 25114/25114 [00:02<00:00,  2.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25115/25115 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 25116/25116 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25117/25117 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 25118/25118 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 25119/25119 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25120/25120 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25121/25121 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25122/25122 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 25123/25123 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25124/25124 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25125/25125 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 25126/25126 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 25127/25127 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 25128/25128 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25129/25129 [00:04<00:00,  4.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25130/25130 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 25131/25131 [00:02<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 25132/25132 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 25133/25133 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 25134/25134 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 25135/25135 [00:02<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 25136/25136 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 25137/25137 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 25138/25138 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25139/25139 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 25140/25140 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 25141/25141 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 25142/25142 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25143/25143 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 25144/25144 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 25145/25145 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 25146/25146 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 25147/25147 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 25148/25148 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 25149/25149 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 25150/25150 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 25151/25151 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 25152/25152 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 25153/25153 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 25154/25154 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 25155/25155 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 25156/25156 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 25157/25157 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 25158/25158 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25159/25159 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 25160/25160 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 25161/25161 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25162/25162 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25163/25163 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 25164/25164 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 25165/25165 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 25166/25166 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 25167/25167 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 25168/25168 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25169/25169 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 25170/25170 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25171/25171 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 25172/25172 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 25173/25173 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 25174/25174 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 25175/25175 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 25176/25176 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25177/25177 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25178/25178 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 25179/25179 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 25180/25180 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 25181/25181 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 25182/25182 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 25183/25183 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 25184/25184 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25185/25185 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25186/25186 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 25187/25187 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25188/25188 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25189/25189 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 25190/25190 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 25191/25191 [00:02<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 25192/25192 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 25193/25193 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 25194/25194 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 25195/25195 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 25196/25196 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 25197/25197 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 25198/25198 [00:04<00:00,  4.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 25199/25199 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 25200/25200 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25201/25201 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 25202/25202 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 25203/25203 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 25204/25204 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 25205/25205 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 25206/25206 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 25207/25207 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 25208/25208 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 25209/25209 [00:02<00:00,  2.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 25210/25210 [00:04<00:00,  4.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 25211/25211 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 25212/25212 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 25213/25213 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 25214/25214 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 25215/25215 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 25216/25216 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 25217/25217 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 25218/25218 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 25219/25219 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 25220/25220 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 25221/25221 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 25222/25222 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 25223/25223 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25224/25224 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 25225/25225 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 25226/25226 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25227/25227 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25228/25228 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 25229/25229 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 25230/25230 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25231/25231 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 25232/25232 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 25233/25233 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 25234/25234 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 25235/25235 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 25236/25236 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 25237/25237 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 25238/25238 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 25239/25239 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 25240/25240 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 25241/25241 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 25242/25242 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 25243/25243 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 25244/25244 [00:04<00:00,  4.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 25245/25245 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 25246/25246 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 25247/25247 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 25248/25248 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 25249/25249 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 25250/25250 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 25251/25251 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 25252/25252 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 25253/25253 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 25254/25254 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 25255/25255 [00:04<00:00,  4.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 25256/25256 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 25257/25257 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 25258/25258 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 25259/25259 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 25260/25260 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 25261/25261 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 25262/25262 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 25263/25263 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 25264/25264 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 25265/25265 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25266/25266 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 25267/25267 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 25268/25268 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 25269/25269 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 25270/25270 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 25271/25271 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 25272/25272 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 25273/25273 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 25274/25274 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 25275/25275 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 25276/25276 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 25277/25277 [00:04<00:00,  4.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 25278/25278 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25279/25279 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 25280/25280 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 25281/25281 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25282/25282 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25283/25283 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25284/25284 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 25285/25285 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25286/25286 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 25287/25287 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 25288/25288 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 25289/25289 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 25290/25290 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25291/25291 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 25292/25292 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 25293/25293 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 25294/25294 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 25295/25295 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 25296/25296 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 25297/25297 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25298/25298 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25299/25299 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 25300/25300 [00:04<00:00,  4.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 25301/25301 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 25302/25302 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 25303/25303 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 25304/25304 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 25305/25305 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 25306/25306 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 25307/25307 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 25308/25308 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 25309/25309 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 25310/25310 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 25311/25311 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 25312/25312 [00:04<00:00,  4.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 25313/25313 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25314/25314 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 25315/25315 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 25316/25316 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 25317/25317 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 25318/25318 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25319/25319 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 25320/25320 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 25321/25321 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 25322/25322 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25323/25323 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 25324/25324 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25325/25325 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25326/25326 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 25327/25327 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 25328/25328 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 25329/25329 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 25330/25330 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 25331/25331 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 25332/25332 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25333/25333 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25334/25334 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 25335/25335 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 25336/25336 [00:04<00:00,  4.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 25337/25337 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 25338/25338 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 25339/25339 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 25340/25340 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 25341/25341 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 25342/25342 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 25343/25343 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 25344/25344 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 25345/25345 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25346/25346 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 25347/25347 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 25348/25348 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25349/25349 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 25350/25350 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 25351/25351 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 25352/25352 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 25353/25353 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 25354/25354 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 25355/25355 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 25356/25356 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25357/25357 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25358/25358 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25359/25359 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 25360/25360 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 25361/25361 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 25362/25362 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 25363/25363 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 25364/25364 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25365/25365 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25366/25366 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 25367/25367 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 25368/25368 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25369/25369 [00:04<00:00,  4.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 25370/25370 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 25371/25371 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25372/25372 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 25373/25373 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25374/25374 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 25375/25375 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25376/25376 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 25377/25377 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 25378/25378 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 25379/25379 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 25380/25380 [00:04<00:00,  4.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25381/25381 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 25382/25382 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 25383/25383 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 25384/25384 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25385/25385 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 25386/25386 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 25387/25387 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25388/25388 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 25389/25389 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 25390/25390 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 25391/25391 [00:04<00:00,  4.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 25392/25392 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25393/25393 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 25394/25394 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 25395/25395 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 25396/25396 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 25397/25397 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 25398/25398 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 25399/25399 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25400/25400 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 25401/25401 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 25402/25402 [00:04<00:00,  4.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 25403/25403 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25404/25404 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 25405/25405 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25406/25406 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 25407/25407 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25408/25408 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 25409/25409 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 25410/25410 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 25411/25411 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 25412/25412 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 25413/25413 [00:04<00:00,  4.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 25414/25414 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25415/25415 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 25416/25416 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 25417/25417 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 25418/25418 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 25419/25419 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25420/25420 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 25421/25421 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 25422/25422 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 25423/25423 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25424/25424 [00:04<00:00,  4.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25425/25425 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25426/25426 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 25427/25427 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 25428/25428 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 25429/25429 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25430/25430 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 25431/25431 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25432/25432 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 25433/25433 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 25434/25434 [00:03<00:00,  3.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 25435/25435 [00:04<00:00,  4.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 25436/25436 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 25437/25437 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 25438/25438 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 25439/25439 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 25440/25440 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 25441/25441 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 25442/25442 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 25443/25443 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 25444/25444 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 25445/25445 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 25446/25446 [00:04<00:00,  4.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 25447/25447 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 25448/25448 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 25449/25449 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25450/25450 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25451/25451 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 25452/25452 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 25453/25453 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 25454/25454 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 25455/25455 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 25456/25456 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 25457/25457 [00:04<00:00,  4.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 25458/25458 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25459/25459 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 25460/25460 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 25461/25461 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 25462/25462 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 25463/25463 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 25464/25464 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 25465/25465 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 25466/25466 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 25467/25467 [00:03<00:00,  3.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 25468/25468 [00:04<00:00,  4.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25469/25469 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 25470/25470 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 25471/25471 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 25472/25472 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 25473/25473 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25474/25474 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 25475/25475 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 25476/25476 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 25477/25477 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 25478/25478 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 25479/25479 [00:04<00:00,  4.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 25480/25480 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 25481/25481 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 25482/25482 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25483/25483 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 25484/25484 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 25485/25485 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 25486/25486 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 25487/25487 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 25488/25488 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 25489/25489 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 25490/25490 [00:04<00:00,  4.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 25491/25491 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 25492/25492 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 25493/25493 [00:03<00:00,  3.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 25494/25494 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 25495/25495 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 25496/25496 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 25497/25497 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 25498/25498 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 25499/25499 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 25500/25500 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 25501/25501 [00:04<00:00,  4.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25502/25502 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 25503/25503 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 25504/25504 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 25505/25505 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 25506/25506 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 25507/25507 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 25508/25508 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 25509/25509 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 25510/25510 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 25511/25511 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 25512/25512 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 25513/25513 [00:04<00:00,  4.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 25514/25514 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 25515/25515 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 25516/25516 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 25517/25517 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 25518/25518 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25519/25519 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 25520/25520 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 25521/25521 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 25522/25522 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 25523/25523 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 25524/25524 [00:04<00:00,  4.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 25525/25525 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25526/25526 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 25527/25527 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 25528/25528 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 25529/25529 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 25530/25530 [00:03<00:00,  3.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 25531/25531 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 25532/25532 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 25533/25533 [00:03<00:00,  3.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 25534/25534 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 25535/25535 [00:04<00:00,  4.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 25536/25536 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 25537/25537 [00:03<00:00,  3.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 25538/25538 [00:03<00:00,  3.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 25539/25539 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 25540/25540 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25541/25541 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 25542/25542 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 25543/25543 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 25544/25544 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 25545/25545 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 25546/25546 [00:04<00:00,  4.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 25547/25547 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 25548/25548 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 25549/25549 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 25550/25550 [00:03<00:00,  3.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 25551/25551 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 25552/25552 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 25553/25553 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 25554/25554 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25555/25555 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 25556/25556 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 25557/25557 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 25558/25558 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 25559/25559 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 25560/25560 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 25561/25561 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 25562/25562 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 25563/25563 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 25564/25564 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 25565/25565 [00:03<00:00,  3.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 25566/25566 [00:03<00:00,  3.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 25567/25567 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 25568/25568 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25569/25569 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 25570/25570 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 25571/25571 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 25572/25572 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 25573/25573 [00:03<00:00,  3.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 25574/25574 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 25575/25575 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 25576/25576 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 25577/25577 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 25578/25578 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25579/25579 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 25580/25580 [00:04<00:00,  4.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 25581/25581 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 25582/25582 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 25583/25583 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25584/25584 [00:03<00:00,  3.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 25585/25585 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 25586/25586 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 25587/25587 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 25588/25588 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 25589/25589 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 25590/25590 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 25591/25591 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 25592/25592 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 25593/25593 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 25594/25594 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 25595/25595 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 25596/25596 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 25597/25597 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 25598/25598 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 25599/25599 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 25600/25600 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 25601/25601 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 25602/25602 [00:04<00:00,  4.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 25603/25603 [00:03<00:00,  3.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 25604/25604 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 25605/25605 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 25606/25606 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 25607/25607 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 25608/25608 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 25609/25609 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 25610/25610 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25611/25611 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 25612/25612 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 25613/25613 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 25614/25614 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 25615/25615 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 25616/25616 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 25617/25617 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 25618/25618 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 25619/25619 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 25620/25620 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 25621/25621 [00:03<00:00,  3.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 25622/25622 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 25623/25623 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 25624/25624 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 25625/25625 [00:04<00:00,  4.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 25626/25626 [00:03<00:00,  3.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 25627/25627 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 25628/25628 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 25629/25629 [00:03<00:00,  3.45s/trial, best loss: 1.0]\n",
      "100%|██████████| 25630/25630 [00:03<00:00,  3.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 25631/25631 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 25632/25632 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 25633/25633 [00:03<00:00,  3.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 25634/25634 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 25635/25635 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 25636/25636 [00:04<00:00,  4.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 25637/25637 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 25638/25638 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 25639/25639 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25640/25640 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25641/25641 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25642/25642 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 25643/25643 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25644/25644 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25645/25645 [00:03<00:00,  3.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 25646/25646 [00:03<00:00,  3.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 25647/25647 [00:04<00:00,  4.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 25648/25648 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 25649/25649 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 25650/25650 [00:03<00:00,  3.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 25651/25651 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25652/25652 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 25653/25653 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 25654/25654 [00:03<00:00,  3.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 25655/25655 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 25656/25656 [00:03<00:00,  3.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 25657/25657 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 25658/25658 [00:04<00:00,  4.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25659/25659 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 25660/25660 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25661/25661 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 25662/25662 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 25663/25663 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25664/25664 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 25665/25665 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25666/25666 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 25667/25667 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 25668/25668 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25669/25669 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 25670/25670 [00:04<00:00,  4.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 25671/25671 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 25672/25672 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 25673/25673 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25674/25674 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 25675/25675 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25676/25676 [00:03<00:00,  3.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 25677/25677 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25678/25678 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 25679/25679 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 25680/25680 [00:03<00:00,  3.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 25681/25681 [00:04<00:00,  4.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25682/25682 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 25683/25683 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25684/25684 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 25685/25685 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 25686/25686 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 25687/25687 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 25688/25688 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 25689/25689 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 25690/25690 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25691/25691 [00:03<00:00,  3.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 25692/25692 [00:04<00:00,  4.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25693/25693 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25694/25694 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 25695/25695 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25696/25696 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25697/25697 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 25698/25698 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25699/25699 [00:03<00:00,  3.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 25700/25700 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25701/25701 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25702/25702 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 25703/25703 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 25704/25704 [00:03<00:00,  3.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 25705/25705 [00:03<00:00,  3.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 25706/25706 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 25707/25707 [00:03<00:00,  3.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 25708/25708 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 25709/25709 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 25710/25710 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25711/25711 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 25712/25712 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 25713/25713 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 25714/25714 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 25715/25715 [00:04<00:00,  4.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25716/25716 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 25717/25717 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25718/25718 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 25719/25719 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 25720/25720 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 25721/25721 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25722/25722 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 25723/25723 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 25724/25724 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 25725/25725 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 25726/25726 [00:05<00:00,  5.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 25727/25727 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 25728/25728 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 25729/25729 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25730/25730 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 25731/25731 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25732/25732 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25733/25733 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25734/25734 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 25735/25735 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25736/25736 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 25737/25737 [00:05<00:00,  5.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25738/25738 [00:03<00:00,  3.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 25739/25739 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 25740/25740 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 25741/25741 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 25742/25742 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25743/25743 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 25744/25744 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 25745/25745 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 25746/25746 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 25747/25747 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 25748/25748 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25749/25749 [00:04<00:00,  4.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25750/25750 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 25751/25751 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 25752/25752 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 25753/25753 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 25754/25754 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 25755/25755 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 25756/25756 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 25757/25757 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 25758/25758 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25759/25759 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 25760/25760 [00:04<00:00,  4.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 25761/25761 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25762/25762 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 25763/25763 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 25764/25764 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 25765/25765 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 25766/25766 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 25767/25767 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 25768/25768 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 25769/25769 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 25770/25770 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 25771/25771 [00:05<00:00,  5.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 25772/25772 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 25773/25773 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 25774/25774 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 25775/25775 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 25776/25776 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25777/25777 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 25778/25778 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 25779/25779 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 25780/25780 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 25781/25781 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 25782/25782 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 25783/25783 [00:05<00:00,  5.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 25784/25784 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 25785/25785 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25786/25786 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 25787/25787 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 25788/25788 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 25789/25789 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 25790/25790 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 25791/25791 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 25792/25792 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 25793/25793 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 25794/25794 [00:05<00:00,  5.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 25795/25795 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 25796/25796 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 25797/25797 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 25798/25798 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 25799/25799 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 25800/25800 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 25801/25801 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 25802/25802 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 25803/25803 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25804/25804 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 25805/25805 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25806/25806 [00:05<00:00,  5.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25807/25807 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 25808/25808 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 25809/25809 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 25810/25810 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 25811/25811 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 25812/25812 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25813/25813 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 25814/25814 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25815/25815 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 25816/25816 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 25817/25817 [00:05<00:00,  5.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 25818/25818 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 25819/25819 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 25820/25820 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25821/25821 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 25822/25822 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25823/25823 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 25824/25824 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25825/25825 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 25826/25826 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25827/25827 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25828/25828 [00:05<00:00,  5.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 25829/25829 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25830/25830 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 25831/25831 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 25832/25832 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 25833/25833 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25834/25834 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 25835/25835 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 25836/25836 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 25837/25837 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25838/25838 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 25839/25839 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 25840/25840 [00:05<00:00,  5.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 25841/25841 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25842/25842 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 25843/25843 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 25844/25844 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25845/25845 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 25846/25846 [00:03<00:00,  3.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25847/25847 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25848/25848 [00:03<00:00,  3.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 25849/25849 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25850/25850 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25851/25851 [00:05<00:00,  5.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 25852/25852 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 25853/25853 [00:03<00:00,  3.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 25854/25854 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 25855/25855 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25856/25856 [00:03<00:00,  3.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25857/25857 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25858/25858 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25859/25859 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 25860/25860 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25861/25861 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 25862/25862 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25863/25863 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25864/25864 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 25865/25865 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25866/25866 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 25867/25867 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25868/25868 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25869/25869 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 25870/25870 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25871/25871 [00:03<00:00,  3.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25872/25872 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25873/25873 [00:03<00:00,  3.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 25874/25874 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25875/25875 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25876/25876 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25877/25877 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 25878/25878 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25879/25879 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25880/25880 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25881/25881 [00:03<00:00,  3.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25882/25882 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25883/25883 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 25884/25884 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 25885/25885 [00:05<00:00,  5.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 25886/25886 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 25887/25887 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25888/25888 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25889/25889 [00:03<00:00,  3.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 25890/25890 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25891/25891 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 25892/25892 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25893/25893 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 25894/25894 [00:03<00:00,  3.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 25895/25895 [00:03<00:00,  3.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 25896/25896 [00:05<00:00,  5.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 25897/25897 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 25898/25898 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 25899/25899 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25900/25900 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 25901/25901 [00:03<00:00,  3.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 25902/25902 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25903/25903 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25904/25904 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25905/25905 [00:03<00:00,  3.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25906/25906 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25907/25907 [00:05<00:00,  5.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 25908/25908 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25909/25909 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 25910/25910 [00:03<00:00,  3.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 25911/25911 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 25912/25912 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25913/25913 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 25914/25914 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 25915/25915 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 25916/25916 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 25917/25917 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 25918/25918 [00:05<00:00,  5.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 25919/25919 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 25920/25920 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 25921/25921 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 25922/25922 [00:03<00:00,  3.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 25923/25923 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 25924/25924 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 25925/25925 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 25926/25926 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 25927/25927 [00:03<00:00,  3.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 25928/25928 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 25929/25929 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 25930/25930 [00:05<00:00,  5.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 25931/25931 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 25932/25932 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 25933/25933 [00:03<00:00,  3.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 25934/25934 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 25935/25935 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 25936/25936 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 25937/25937 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 25938/25938 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25939/25939 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 25940/25940 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 25941/25941 [00:04<00:00,  4.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 25942/25942 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 25943/25943 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 25944/25944 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25945/25945 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25946/25946 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 25947/25947 [00:03<00:00,  3.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 25948/25948 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 25949/25949 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 25950/25950 [00:03<00:00,  3.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 25951/25951 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 25952/25952 [00:05<00:00,  5.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 25953/25953 [00:03<00:00,  3.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 25954/25954 [00:03<00:00,  3.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 25955/25955 [00:03<00:00,  3.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 25956/25956 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25957/25957 [00:03<00:00,  3.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 25958/25958 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25959/25959 [00:03<00:00,  3.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 25960/25960 [00:03<00:00,  3.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 25961/25961 [00:03<00:00,  3.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 25962/25962 [00:03<00:00,  3.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 25963/25963 [00:03<00:00,  3.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 25964/25964 [00:03<00:00,  3.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 25965/25965 [00:03<00:00,  3.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 25966/25966 [00:03<00:00,  3.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 25967/25967 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25968/25968 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 25969/25969 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 25970/25970 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25971/25971 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 25972/25972 [00:03<00:00,  3.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 25973/25973 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 25974/25974 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 25975/25975 [00:05<00:00,  5.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 25976/25976 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 25977/25977 [00:03<00:00,  3.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 25978/25978 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25979/25979 [00:03<00:00,  3.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 25980/25980 [00:03<00:00,  3.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 25981/25981 [00:03<00:00,  3.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 25982/25982 [00:03<00:00,  3.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 25983/25983 [00:03<00:00,  3.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 25984/25984 [00:03<00:00,  3.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 25985/25985 [00:03<00:00,  3.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 25986/25986 [00:05<00:00,  5.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 25987/25987 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 25988/25988 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 25989/25989 [00:03<00:00,  3.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 25990/25990 [00:03<00:00,  3.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 25991/25991 [00:03<00:00,  3.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 25992/25992 [00:03<00:00,  3.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 25993/25993 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 25994/25994 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 25995/25995 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 25996/25996 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 25997/25997 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 25998/25998 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 25999/25999 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26000/26000 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26001/26001 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 26002/26002 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26003/26003 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26004/26004 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26005/26005 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 26006/26006 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 26007/26007 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 26008/26008 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 26009/26009 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 26010/26010 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 26011/26011 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26012/26012 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 26013/26013 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 26014/26014 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 26015/26015 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26016/26016 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 26017/26017 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 26018/26018 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 26019/26019 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 26020/26020 [00:03<00:00,  3.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 26021/26021 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26022/26022 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26023/26023 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 26024/26024 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26025/26025 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26026/26026 [00:02<00:00,  2.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 26027/26027 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 26028/26028 [00:02<00:00,  2.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 26029/26029 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 26030/26030 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 26031/26031 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 26032/26032 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26033/26033 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 26034/26034 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26035/26035 [00:02<00:00,  2.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 26036/26036 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26037/26037 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26038/26038 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 26039/26039 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26040/26040 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 26041/26041 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26042/26042 [00:03<00:00,  3.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 26043/26043 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26044/26044 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26045/26045 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26046/26046 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26047/26047 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26048/26048 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26049/26049 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 26050/26050 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26051/26051 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26052/26052 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26053/26053 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26054/26054 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26055/26055 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26056/26056 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26057/26057 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26058/26058 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26059/26059 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 26060/26060 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26061/26061 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 26062/26062 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26063/26063 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26064/26064 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 26065/26065 [00:03<00:00,  3.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 26066/26066 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26067/26067 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26068/26068 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26069/26069 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26070/26070 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26071/26071 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26072/26072 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26073/26073 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26074/26074 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26075/26075 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26076/26076 [00:03<00:00,  3.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 26077/26077 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26078/26078 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 26079/26079 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26080/26080 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26081/26081 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 26082/26082 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26083/26083 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 26084/26084 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26085/26085 [00:02<00:00,  2.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 26086/26086 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26087/26087 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 26088/26088 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 26089/26089 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26090/26090 [00:02<00:00,  2.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 26091/26091 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26092/26092 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26093/26093 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26094/26094 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26095/26095 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 26096/26096 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26097/26097 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26098/26098 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 26099/26099 [00:03<00:00,  3.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 26100/26100 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26101/26101 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26102/26102 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26103/26103 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26104/26104 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26105/26105 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26106/26106 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26107/26107 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26108/26108 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26109/26109 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26110/26110 [00:03<00:00,  3.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 26111/26111 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26112/26112 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26113/26113 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26114/26114 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26115/26115 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 26116/26116 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26117/26117 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26118/26118 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 26119/26119 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26120/26120 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26121/26121 [00:03<00:00,  3.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 26122/26122 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26123/26123 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26124/26124 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26125/26125 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26126/26126 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26127/26127 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26128/26128 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26129/26129 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26130/26130 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26131/26131 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 26132/26132 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26133/26133 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26134/26134 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26135/26135 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26136/26136 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26137/26137 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26138/26138 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26139/26139 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26140/26140 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26141/26141 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 26142/26142 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26143/26143 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 26144/26144 [00:03<00:00,  3.95s/trial, best loss: 1.0]\n",
      "100%|██████████| 26145/26145 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26146/26146 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26147/26147 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26148/26148 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26149/26149 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26150/26150 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26151/26151 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26152/26152 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26153/26153 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26154/26154 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26155/26155 [00:03<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 26156/26156 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26157/26157 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26158/26158 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26159/26159 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26160/26160 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26161/26161 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26162/26162 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26163/26163 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26164/26164 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26165/26165 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26166/26166 [00:03<00:00,  3.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 26167/26167 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26168/26168 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26169/26169 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26170/26170 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26171/26171 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26172/26172 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26173/26173 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26174/26174 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26175/26175 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26176/26176 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26177/26177 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26178/26178 [00:03<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 26179/26179 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26180/26180 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26181/26181 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26182/26182 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26183/26183 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26184/26184 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26185/26185 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26186/26186 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26187/26187 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26188/26188 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26189/26189 [00:03<00:00,  3.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 26190/26190 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26191/26191 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26192/26192 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26193/26193 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26194/26194 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26195/26195 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26196/26196 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26197/26197 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26198/26198 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26199/26199 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26200/26200 [00:03<00:00,  3.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 26201/26201 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26202/26202 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26203/26203 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26204/26204 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26205/26205 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26206/26206 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26207/26207 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26208/26208 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26209/26209 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26210/26210 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26211/26211 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26212/26212 [00:03<00:00,  3.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 26213/26213 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26214/26214 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26215/26215 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26216/26216 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26217/26217 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26218/26218 [00:02<00:00,  2.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 26219/26219 [00:02<00:00,  2.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 26220/26220 [00:02<00:00,  2.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 26221/26221 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26222/26222 [00:02<00:00,  2.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 26223/26223 [00:03<00:00,  3.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 26224/26224 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26225/26225 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26226/26226 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26227/26227 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26228/26228 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26229/26229 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26230/26230 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26231/26231 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26232/26232 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26233/26233 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26234/26234 [00:03<00:00,  3.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 26235/26235 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26236/26236 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26237/26237 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26238/26238 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26239/26239 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26240/26240 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26241/26241 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26242/26242 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26243/26243 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26244/26244 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26245/26245 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26246/26246 [00:03<00:00,  3.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 26247/26247 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26248/26248 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26249/26249 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26250/26250 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26251/26251 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26252/26252 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26253/26253 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26254/26254 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26255/26255 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26256/26256 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26257/26257 [00:03<00:00,  3.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 26258/26258 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26259/26259 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26260/26260 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26261/26261 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26262/26262 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26263/26263 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26264/26264 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26265/26265 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26266/26266 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26267/26267 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26268/26268 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26269/26269 [00:03<00:00,  3.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 26270/26270 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26271/26271 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26272/26272 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26273/26273 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26274/26274 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26275/26275 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26276/26276 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26277/26277 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26278/26278 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26279/26279 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26280/26280 [00:04<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 26281/26281 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26282/26282 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26283/26283 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26284/26284 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26285/26285 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26286/26286 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26287/26287 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26288/26288 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26289/26289 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26290/26290 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26291/26291 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26292/26292 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 26293/26293 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26294/26294 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26295/26295 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26296/26296 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26297/26297 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26298/26298 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26299/26299 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26300/26300 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26301/26301 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26302/26302 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26303/26303 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 26304/26304 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26305/26305 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26306/26306 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26307/26307 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26308/26308 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26309/26309 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26310/26310 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26311/26311 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26312/26312 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26313/26313 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26314/26314 [00:03<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 26315/26315 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26316/26316 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26317/26317 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26318/26318 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26319/26319 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26320/26320 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26321/26321 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26322/26322 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26323/26323 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26324/26324 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26325/26325 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26326/26326 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26327/26327 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26328/26328 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26329/26329 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26330/26330 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26331/26331 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26332/26332 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26333/26333 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26334/26334 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26335/26335 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26336/26336 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26337/26337 [00:04<00:00,  4.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 26338/26338 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26339/26339 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26340/26340 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26341/26341 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26342/26342 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26343/26343 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26344/26344 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26345/26345 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26346/26346 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26347/26347 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26348/26348 [00:03<00:00,  3.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 26349/26349 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26350/26350 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26351/26351 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26352/26352 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26353/26353 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26354/26354 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26355/26355 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26356/26356 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26357/26357 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26358/26358 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26359/26359 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26360/26360 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 26361/26361 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26362/26362 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26363/26363 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26364/26364 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26365/26365 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26366/26366 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26367/26367 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26368/26368 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26369/26369 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26370/26370 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26371/26371 [00:04<00:00,  4.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 26372/26372 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26373/26373 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26374/26374 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26375/26375 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26376/26376 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26377/26377 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26378/26378 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26379/26379 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26380/26380 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26381/26381 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26382/26382 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26383/26383 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 26384/26384 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26385/26385 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26386/26386 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26387/26387 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26388/26388 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26389/26389 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26390/26390 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26391/26391 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26392/26392 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26393/26393 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26394/26394 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 26395/26395 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26396/26396 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26397/26397 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26398/26398 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26399/26399 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 26400/26400 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 26401/26401 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26402/26402 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 26403/26403 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26404/26404 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26405/26405 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 26406/26406 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 26407/26407 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 26408/26408 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 26409/26409 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 26410/26410 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 26411/26411 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 26412/26412 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26413/26413 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26414/26414 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 26415/26415 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 26416/26416 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26417/26417 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 26418/26418 [00:02<00:00,  2.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 26419/26419 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 26420/26420 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26421/26421 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 26422/26422 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 26423/26423 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26424/26424 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 26425/26425 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 26426/26426 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26427/26427 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26428/26428 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26429/26429 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 26430/26430 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 26431/26431 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 26432/26432 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 26433/26433 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26434/26434 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26435/26435 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 26436/26436 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 26437/26437 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 26438/26438 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 26439/26439 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26440/26440 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 26441/26441 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26442/26442 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26443/26443 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 26444/26444 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 26445/26445 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 26446/26446 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26447/26447 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26448/26448 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 26449/26449 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26450/26450 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 26451/26451 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 26452/26452 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 26453/26453 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26454/26454 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26455/26455 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26456/26456 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26457/26457 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26458/26458 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26459/26459 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26460/26460 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26461/26461 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26462/26462 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26463/26463 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26464/26464 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26465/26465 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26466/26466 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26467/26467 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26468/26468 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26469/26469 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26470/26470 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26471/26471 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26472/26472 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26473/26473 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26474/26474 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 26475/26475 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26476/26476 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26477/26477 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26478/26478 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26479/26479 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26480/26480 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26481/26481 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26482/26482 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26483/26483 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26484/26484 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26485/26485 [00:03<00:00,  3.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 26486/26486 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26487/26487 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26488/26488 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26489/26489 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26490/26490 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26491/26491 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26492/26492 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26493/26493 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26494/26494 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26495/26495 [00:02<00:00,  2.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 26496/26496 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26497/26497 [00:04<00:00,  4.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 26498/26498 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26499/26499 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26500/26500 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26501/26501 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26502/26502 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26503/26503 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26504/26504 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26505/26505 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26506/26506 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26507/26507 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26508/26508 [00:04<00:00,  4.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 26509/26509 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26510/26510 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26511/26511 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26512/26512 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26513/26513 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26514/26514 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26515/26515 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26516/26516 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26517/26517 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26518/26518 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26519/26519 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 26520/26520 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26521/26521 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26522/26522 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26523/26523 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26524/26524 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26525/26525 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26526/26526 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26527/26527 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26528/26528 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26529/26529 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26530/26530 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26531/26531 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 26532/26532 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26533/26533 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26534/26534 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26535/26535 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26536/26536 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26537/26537 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26538/26538 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26539/26539 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26540/26540 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26541/26541 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26542/26542 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 26543/26543 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26544/26544 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26545/26545 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26546/26546 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 26547/26547 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26548/26548 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26549/26549 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26550/26550 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26551/26551 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26552/26552 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26553/26553 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26554/26554 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26555/26555 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26556/26556 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26557/26557 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26558/26558 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26559/26559 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26560/26560 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26561/26561 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26562/26562 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26563/26563 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26564/26564 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26565/26565 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26566/26566 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26567/26567 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26568/26568 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26569/26569 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26570/26570 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26571/26571 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26572/26572 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 26573/26573 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26574/26574 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26575/26575 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26576/26576 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26577/26577 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 26578/26578 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26579/26579 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26580/26580 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 26581/26581 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 26582/26582 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26583/26583 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26584/26584 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26585/26585 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26586/26586 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26587/26587 [00:04<00:00,  4.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 26588/26588 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26589/26589 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26590/26590 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26591/26591 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26592/26592 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26593/26593 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 26594/26594 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26595/26595 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 26596/26596 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26597/26597 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26598/26598 [00:04<00:00,  4.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 26599/26599 [00:02<00:00,  2.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 26600/26600 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26601/26601 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26602/26602 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26603/26603 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26604/26604 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26605/26605 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26606/26606 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26607/26607 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26608/26608 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26609/26609 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26610/26610 [00:03<00:00,  4.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 26611/26611 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26612/26612 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26613/26613 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26614/26614 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26615/26615 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26616/26616 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26617/26617 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26618/26618 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26619/26619 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26620/26620 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26621/26621 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 26622/26622 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 26623/26623 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 26624/26624 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 26625/26625 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 26626/26626 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 26627/26627 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26628/26628 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 26629/26629 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 26630/26630 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 26631/26631 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 26632/26632 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 26633/26633 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26634/26634 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 26635/26635 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 26636/26636 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 26637/26637 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26638/26638 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 26639/26639 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26640/26640 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26641/26641 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 26642/26642 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 26643/26643 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 26644/26644 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 26645/26645 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26646/26646 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 26647/26647 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26648/26648 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 26649/26649 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26650/26650 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26651/26651 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26652/26652 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26653/26653 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26654/26654 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26655/26655 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 26656/26656 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26657/26657 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26658/26658 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26659/26659 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26660/26660 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26661/26661 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26662/26662 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26663/26663 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26664/26664 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26665/26665 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26666/26666 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26667/26667 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 26668/26668 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26669/26669 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26670/26670 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26671/26671 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26672/26672 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26673/26673 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26674/26674 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26675/26675 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26676/26676 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26677/26677 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26678/26678 [00:04<00:00,  4.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 26679/26679 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26680/26680 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26681/26681 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26682/26682 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26683/26683 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26684/26684 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26685/26685 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26686/26686 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26687/26687 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26688/26688 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26689/26689 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26690/26690 [00:04<00:00,  4.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 26691/26691 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26692/26692 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26693/26693 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26694/26694 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26695/26695 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 26696/26696 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26697/26697 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26698/26698 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26699/26699 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26700/26700 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26701/26701 [00:04<00:00,  4.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 26702/26702 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26703/26703 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26704/26704 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26705/26705 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26706/26706 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26707/26707 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26708/26708 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26709/26709 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26710/26710 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26711/26711 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26712/26712 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26713/26713 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 26714/26714 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26715/26715 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26716/26716 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26717/26717 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26718/26718 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26719/26719 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26720/26720 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26721/26721 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26722/26722 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26723/26723 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26724/26724 [00:04<00:00,  4.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 26725/26725 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26726/26726 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26727/26727 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26728/26728 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26729/26729 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26730/26730 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26731/26731 [00:02<00:00,  2.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 26732/26732 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26733/26733 [00:02<00:00,  2.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 26734/26734 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26735/26735 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26736/26736 [00:04<00:00,  4.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 26737/26737 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26738/26738 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26739/26739 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26740/26740 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26741/26741 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26742/26742 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26743/26743 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26744/26744 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26745/26745 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26746/26746 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26747/26747 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 26748/26748 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26749/26749 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26750/26750 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26751/26751 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26752/26752 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26753/26753 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26754/26754 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26755/26755 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26756/26756 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26757/26757 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26758/26758 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 26759/26759 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 26760/26760 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26761/26761 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26762/26762 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26763/26763 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26764/26764 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26765/26765 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26766/26766 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26767/26767 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 26768/26768 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26769/26769 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26770/26770 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 26771/26771 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26772/26772 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 26773/26773 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 26774/26774 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26775/26775 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 26776/26776 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26777/26777 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 26778/26778 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 26779/26779 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 26780/26780 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26781/26781 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 26782/26782 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26783/26783 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 26784/26784 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 26785/26785 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26786/26786 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26787/26787 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26788/26788 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 26789/26789 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 26790/26790 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 26791/26791 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 26792/26792 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 26793/26793 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 26794/26794 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 26795/26795 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 26796/26796 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26797/26797 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 26798/26798 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26799/26799 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 26800/26800 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 26801/26801 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26802/26802 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26803/26803 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26804/26804 [00:04<00:00,  4.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 26805/26805 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26806/26806 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26807/26807 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26808/26808 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26809/26809 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26810/26810 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26811/26811 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26812/26812 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26813/26813 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26814/26814 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26815/26815 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 26816/26816 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26817/26817 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26818/26818 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26819/26819 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26820/26820 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26821/26821 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26822/26822 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26823/26823 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26824/26824 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26825/26825 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26826/26826 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 26827/26827 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26828/26828 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26829/26829 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26830/26830 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26831/26831 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26832/26832 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 26833/26833 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26834/26834 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 26835/26835 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 26836/26836 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26837/26837 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 26838/26838 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26839/26839 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26840/26840 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26841/26841 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26842/26842 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26843/26843 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26844/26844 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26845/26845 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 26846/26846 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26847/26847 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26848/26848 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26849/26849 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 26850/26850 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26851/26851 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26852/26852 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26853/26853 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26854/26854 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26855/26855 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26856/26856 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26857/26857 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26858/26858 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26859/26859 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26860/26860 [00:04<00:00,  4.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 26861/26861 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26862/26862 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26863/26863 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26864/26864 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26865/26865 [00:02<00:00,  2.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 26866/26866 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26867/26867 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26868/26868 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26869/26869 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26870/26870 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26871/26871 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 26872/26872 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26873/26873 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26874/26874 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26875/26875 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26876/26876 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26877/26877 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26878/26878 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26879/26879 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 26880/26880 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26881/26881 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26882/26882 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 26883/26883 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26884/26884 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26885/26885 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26886/26886 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26887/26887 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26888/26888 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26889/26889 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26890/26890 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26891/26891 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26892/26892 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26893/26893 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26894/26894 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26895/26895 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26896/26896 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 26897/26897 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26898/26898 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 26899/26899 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 26900/26900 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26901/26901 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26902/26902 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26903/26903 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26904/26904 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26905/26905 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 26906/26906 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 26907/26907 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26908/26908 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26909/26909 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 26910/26910 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 26911/26911 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26912/26912 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 26913/26913 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26914/26914 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26915/26915 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26916/26916 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 26917/26917 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26918/26918 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26919/26919 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26920/26920 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26921/26921 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 26922/26922 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26923/26923 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26924/26924 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 26925/26925 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 26926/26926 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 26927/26927 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26928/26928 [00:04<00:00,  4.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 26929/26929 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 26930/26930 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 26931/26931 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26932/26932 [00:02<00:00,  2.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 26933/26933 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 26934/26934 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 26935/26935 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 26936/26936 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 26937/26937 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 26938/26938 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 26939/26939 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 26940/26940 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 26941/26941 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 26942/26942 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26943/26943 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 26944/26944 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26945/26945 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26946/26946 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 26947/26947 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 26948/26948 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 26949/26949 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26950/26950 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 26951/26951 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26952/26952 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26953/26953 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26954/26954 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26955/26955 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26956/26956 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26957/26957 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26958/26958 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26959/26959 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 26960/26960 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 26961/26961 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26962/26962 [00:04<00:00,  4.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 26963/26963 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 26964/26964 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26965/26965 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 26966/26966 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26967/26967 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26968/26968 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 26969/26969 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26970/26970 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 26971/26971 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 26972/26972 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 26973/26973 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 26974/26974 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 26975/26975 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 26976/26976 [00:02<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 26977/26977 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 26978/26978 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 26979/26979 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 26980/26980 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 26981/26981 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 26982/26982 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 26983/26983 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 26984/26984 [00:04<00:00,  4.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 26985/26985 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 26986/26986 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 26987/26987 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 26988/26988 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 26989/26989 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 26990/26990 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 26991/26991 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 26992/26992 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 26993/26993 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 26994/26994 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 26995/26995 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 26996/26996 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 26997/26997 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 26998/26998 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 26999/26999 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27000/27000 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 27001/27001 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27002/27002 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 27003/27003 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27004/27004 [00:03<00:00,  3.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 27005/27005 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27006/27006 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27007/27007 [00:04<00:00,  4.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 27008/27008 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27009/27009 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 27010/27010 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27011/27011 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27012/27012 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 27013/27013 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27014/27014 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27015/27015 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 27016/27016 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27017/27017 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27018/27018 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 27019/27019 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27020/27020 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27021/27021 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27022/27022 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27023/27023 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27024/27024 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27025/27025 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27026/27026 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27027/27027 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27028/27028 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27029/27029 [00:04<00:00,  4.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 27030/27030 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27031/27031 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27032/27032 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27033/27033 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27034/27034 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27035/27035 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27036/27036 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27037/27037 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27038/27038 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27039/27039 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 27040/27040 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27041/27041 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27042/27042 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27043/27043 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 27044/27044 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 27045/27045 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27046/27046 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27047/27047 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27048/27048 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27049/27049 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27050/27050 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27051/27051 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27052/27052 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 27053/27053 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27054/27054 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27055/27055 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27056/27056 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27057/27057 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27058/27058 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27059/27059 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27060/27060 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27061/27061 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 27062/27062 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27063/27063 [00:04<00:00,  4.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 27064/27064 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27065/27065 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27066/27066 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27067/27067 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 27068/27068 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27069/27069 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27070/27070 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27071/27071 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27072/27072 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 27073/27073 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27074/27074 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 27075/27075 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27076/27076 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27077/27077 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27078/27078 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27079/27079 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 27080/27080 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27081/27081 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27082/27082 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27083/27083 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27084/27084 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27085/27085 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27086/27086 [00:04<00:00,  4.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 27087/27087 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27088/27088 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27089/27089 [00:03<00:00,  3.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 27090/27090 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27091/27091 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27092/27092 [00:03<00:00,  3.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 27093/27093 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27094/27094 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27095/27095 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27096/27096 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27097/27097 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 27098/27098 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27099/27099 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27100/27100 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27101/27101 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27102/27102 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27103/27103 [00:03<00:00,  3.01s/trial, best loss: 1.0]\n",
      "100%|██████████| 27104/27104 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27105/27105 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27106/27106 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27107/27107 [00:02<00:00,  2.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 27108/27108 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27109/27109 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 27110/27110 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27111/27111 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27112/27112 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27113/27113 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27114/27114 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27115/27115 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27116/27116 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27117/27117 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27118/27118 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27119/27119 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 27120/27120 [00:04<00:00,  4.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 27121/27121 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27122/27122 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27123/27123 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27124/27124 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27125/27125 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27126/27126 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27127/27127 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27128/27128 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27129/27129 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27130/27130 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27131/27131 [00:04<00:00,  4.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 27132/27132 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27133/27133 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27134/27134 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27135/27135 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27136/27136 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27137/27137 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27138/27138 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27139/27139 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27140/27140 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27141/27141 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27142/27142 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27143/27143 [00:04<00:00,  4.54s/trial, best loss: 1.0]\n",
      "100%|██████████| 27144/27144 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27145/27145 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27146/27146 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 27147/27147 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27148/27148 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27149/27149 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27150/27150 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27151/27151 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27152/27152 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27153/27153 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27154/27154 [00:04<00:00,  4.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 27155/27155 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27156/27156 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27157/27157 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27158/27158 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27159/27159 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27160/27160 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27161/27161 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27162/27162 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27163/27163 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27164/27164 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27165/27165 [00:04<00:00,  4.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 27166/27166 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27167/27167 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27168/27168 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 27169/27169 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27170/27170 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27171/27171 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27172/27172 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27173/27173 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27174/27174 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27175/27175 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27176/27176 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27177/27177 [00:04<00:00,  4.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 27178/27178 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27179/27179 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27180/27180 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27181/27181 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 27182/27182 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27183/27183 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27184/27184 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27185/27185 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27186/27186 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27187/27187 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27188/27188 [00:04<00:00,  4.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 27189/27189 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27190/27190 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27191/27191 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27192/27192 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27193/27193 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27194/27194 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27195/27195 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27196/27196 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27197/27197 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27198/27198 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27199/27199 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27200/27200 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27201/27201 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27202/27202 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27203/27203 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27204/27204 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27205/27205 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 27206/27206 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 27207/27207 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 27208/27208 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 27209/27209 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 27210/27210 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27211/27211 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 27212/27212 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27213/27213 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27214/27214 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27215/27215 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27216/27216 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27217/27217 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27218/27218 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27219/27219 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27220/27220 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27221/27221 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27222/27222 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 27223/27223 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27224/27224 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27225/27225 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27226/27226 [00:03<00:00,  3.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27227/27227 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27228/27228 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27229/27229 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27230/27230 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27231/27231 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27232/27232 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 27233/27233 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27234/27234 [00:04<00:00,  4.47s/trial, best loss: 1.0]\n",
      "100%|██████████| 27235/27235 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27236/27236 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 27237/27237 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27238/27238 [00:03<00:00,  3.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27239/27239 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27240/27240 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27241/27241 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27242/27242 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27243/27243 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27244/27244 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27245/27245 [00:04<00:00,  4.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 27246/27246 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27247/27247 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27248/27248 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27249/27249 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27250/27250 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27251/27251 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27252/27252 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27253/27253 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27254/27254 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27255/27255 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27256/27256 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27257/27257 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27258/27258 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27259/27259 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27260/27260 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27261/27261 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27262/27262 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27263/27263 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 27264/27264 [00:03<00:00,  3.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 27265/27265 [00:03<00:00,  3.30s/trial, best loss: 1.0]\n",
      "100%|██████████| 27266/27266 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 27267/27267 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27268/27268 [00:04<00:00,  4.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 27269/27269 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27270/27270 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27271/27271 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 27272/27272 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 27273/27273 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27274/27274 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27275/27275 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 27276/27276 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 27277/27277 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27278/27278 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27279/27279 [00:04<00:00,  4.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 27280/27280 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27281/27281 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 27282/27282 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 27283/27283 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27284/27284 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 27285/27285 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 27286/27286 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27287/27287 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27288/27288 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27289/27289 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27290/27290 [00:03<00:00,  3.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 27291/27291 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27292/27292 [00:03<00:00,  3.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 27293/27293 [00:03<00:00,  3.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 27294/27294 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 27295/27295 [00:03<00:00,  3.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 27296/27296 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 27297/27297 [00:03<00:00,  3.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 27298/27298 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27299/27299 [00:03<00:00,  3.02s/trial, best loss: 1.0]\n",
      "100%|██████████| 27300/27300 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27301/27301 [00:03<00:00,  3.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 27302/27302 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27303/27303 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27304/27304 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27305/27305 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27306/27306 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27307/27307 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27308/27308 [00:03<00:00,  3.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27309/27309 [00:03<00:00,  3.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27310/27310 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27311/27311 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27312/27312 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27313/27313 [00:04<00:00,  4.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 27314/27314 [00:03<00:00,  3.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27315/27315 [00:03<00:00,  3.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27316/27316 [00:03<00:00,  3.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 27317/27317 [00:03<00:00,  3.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 27318/27318 [00:03<00:00,  3.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27319/27319 [00:03<00:00,  3.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27320/27320 [00:03<00:00,  3.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27321/27321 [00:03<00:00,  3.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27322/27322 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 27323/27323 [00:03<00:00,  3.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 27324/27324 [00:04<00:00,  4.63s/trial, best loss: 1.0]\n",
      "100%|██████████| 27325/27325 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 27326/27326 [00:03<00:00,  3.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 27327/27327 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 27328/27328 [00:03<00:00,  3.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 27329/27329 [00:07<00:00,  7.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 27330/27330 [00:07<00:00,  7.52s/trial, best loss: 1.0]\n",
      "100%|██████████| 27331/27331 [00:07<00:00,  7.59s/trial, best loss: 1.0]\n",
      "100%|██████████| 27332/27332 [00:07<00:00,  7.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 27333/27333 [00:07<00:00,  7.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 27334/27334 [00:07<00:00,  7.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 27335/27335 [00:08<00:00,  8.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 27336/27336 [00:07<00:00,  7.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 27337/27337 [00:07<00:00,  7.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 27338/27338 [00:07<00:00,  7.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 27339/27339 [00:07<00:00,  7.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 27340/27340 [00:07<00:00,  7.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 27341/27341 [00:07<00:00,  7.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 27342/27342 [00:07<00:00,  7.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 27343/27343 [00:07<00:00,  7.55s/trial, best loss: 1.0]\n",
      "100%|██████████| 27344/27344 [00:07<00:00,  7.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 27345/27345 [00:07<00:00,  7.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 27346/27346 [00:07<00:00,  7.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 27347/27347 [00:08<00:00,  8.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27348/27348 [00:07<00:00,  7.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 27349/27349 [00:07<00:00,  7.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 27350/27350 [00:07<00:00,  7.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 27351/27351 [00:07<00:00,  7.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 27352/27352 [00:07<00:00,  7.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 27353/27353 [00:07<00:00,  7.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 27354/27354 [00:07<00:00,  7.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27355/27355 [00:07<00:00,  7.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 27356/27356 [00:07<00:00,  7.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 27357/27357 [00:06<00:00,  6.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 27358/27358 [00:08<00:00,  8.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 27359/27359 [00:07<00:00,  7.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27360/27360 [00:07<00:00,  7.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27361/27361 [00:07<00:00,  7.51s/trial, best loss: 1.0]\n",
      "100%|██████████| 27362/27362 [00:07<00:00,  7.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27363/27363 [00:06<00:00,  6.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 27364/27364 [00:07<00:00,  7.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 27365/27365 [00:06<00:00,  6.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 27366/27366 [00:07<00:00,  7.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27367/27367 [00:06<00:00,  6.96s/trial, best loss: 1.0]\n",
      "100%|██████████| 27368/27368 [00:06<00:00,  6.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 27369/27369 [00:06<00:00,  6.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 27370/27370 [00:08<00:00,  8.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 27371/27371 [00:06<00:00,  6.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 27372/27372 [00:06<00:00,  6.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 27373/27373 [00:06<00:00,  6.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 27374/27374 [00:06<00:00,  6.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 27375/27375 [00:06<00:00,  6.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27376/27376 [00:06<00:00,  6.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 27377/27377 [00:07<00:00,  7.03s/trial, best loss: 1.0]\n",
      "100%|██████████| 27378/27378 [00:06<00:00,  6.64s/trial, best loss: 1.0]\n",
      "100%|██████████| 27379/27379 [00:06<00:00,  6.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 27380/27380 [00:06<00:00,  6.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27381/27381 [00:08<00:00,  8.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27382/27382 [00:06<00:00,  6.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27383/27383 [00:06<00:00,  6.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27384/27384 [00:06<00:00,  6.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 27385/27385 [00:06<00:00,  6.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 27386/27386 [00:06<00:00,  6.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27387/27387 [00:06<00:00,  6.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 27388/27388 [00:06<00:00,  6.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 27389/27389 [00:06<00:00,  6.67s/trial, best loss: 1.0]\n",
      "100%|██████████| 27390/27390 [00:06<00:00,  6.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 27391/27391 [00:06<00:00,  6.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 27392/27392 [00:06<00:00,  6.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 27393/27393 [00:07<00:00,  8.00s/trial, best loss: 1.0]\n",
      "100%|██████████| 27394/27394 [00:06<00:00,  6.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 27395/27395 [00:06<00:00,  6.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 27396/27396 [00:06<00:00,  6.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 27397/27397 [00:06<00:00,  6.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 27398/27398 [00:06<00:00,  6.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 27399/27399 [00:06<00:00,  6.61s/trial, best loss: 1.0]\n",
      "100%|██████████| 27400/27400 [00:06<00:00,  6.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 27401/27401 [00:06<00:00,  6.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 27402/27402 [00:06<00:00,  6.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 27403/27403 [00:06<00:00,  6.60s/trial, best loss: 1.0]\n",
      "100%|██████████| 27404/27404 [00:06<00:00,  6.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 27405/27405 [00:07<00:00,  7.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27406/27406 [00:06<00:00,  6.36s/trial, best loss: 1.0]\n",
      "100%|██████████| 27407/27407 [00:06<00:00,  6.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 27408/27408 [00:06<00:00,  6.42s/trial, best loss: 1.0]\n",
      "100%|██████████| 27409/27409 [00:06<00:00,  6.44s/trial, best loss: 1.0]\n",
      "100%|██████████| 27410/27410 [00:06<00:00,  6.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 27411/27411 [00:06<00:00,  6.26s/trial, best loss: 1.0]\n",
      "100%|██████████| 27412/27412 [00:06<00:00,  6.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 27413/27413 [00:06<00:00,  6.38s/trial, best loss: 1.0]\n",
      "100%|██████████| 27414/27414 [00:06<00:00,  6.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 27415/27415 [00:06<00:00,  6.46s/trial, best loss: 1.0]\n",
      "100%|██████████| 27416/27416 [00:07<00:00,  7.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 27417/27417 [00:06<00:00,  6.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 27418/27418 [00:06<00:00,  6.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 27419/27419 [00:06<00:00,  6.33s/trial, best loss: 1.0]\n",
      "100%|██████████| 27420/27420 [00:06<00:00,  6.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27421/27421 [00:06<00:00,  6.28s/trial, best loss: 1.0]\n",
      "100%|██████████| 27422/27422 [00:06<00:00,  6.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 27423/27423 [00:06<00:00,  6.41s/trial, best loss: 1.0]\n",
      "100%|██████████| 27424/27424 [00:06<00:00,  6.37s/trial, best loss: 1.0]\n",
      "100%|██████████| 27425/27425 [00:06<00:00,  6.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 27426/27426 [00:06<00:00,  6.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 27427/27427 [00:07<00:00,  7.62s/trial, best loss: 1.0]\n",
      "100%|██████████| 27428/27428 [00:06<00:00,  6.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 27429/27429 [00:06<00:00,  6.35s/trial, best loss: 1.0]\n",
      "100%|██████████| 27430/27430 [00:06<00:00,  6.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 27431/27431 [00:06<00:00,  6.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 27432/27432 [00:06<00:00,  6.57s/trial, best loss: 1.0]\n",
      "100%|██████████| 27433/27433 [00:06<00:00,  6.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 27434/27434 [00:06<00:00,  6.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 27435/27435 [00:06<00:00,  6.58s/trial, best loss: 1.0]\n",
      "100%|██████████| 27436/27436 [00:06<00:00,  6.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 27437/27437 [00:06<00:00,  6.49s/trial, best loss: 1.0]\n",
      "100%|██████████| 27438/27438 [00:08<00:00,  8.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27439/27439 [00:06<00:00,  6.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 27440/27440 [00:06<00:00,  6.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 27441/27441 [00:06<00:00,  6.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 27442/27442 [00:06<00:00,  6.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 27443/27443 [00:06<00:00,  6.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 27444/27444 [00:06<00:00,  6.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 27445/27445 [00:06<00:00,  6.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 27446/27446 [00:06<00:00,  6.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 27447/27447 [00:06<00:00,  6.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 27448/27448 [00:06<00:00,  6.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27449/27449 [00:06<00:00,  6.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27450/27450 [00:07<00:00,  7.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 27451/27451 [00:06<00:00,  6.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 27452/27452 [00:06<00:00,  6.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 27453/27453 [00:06<00:00,  6.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 27454/27454 [00:06<00:00,  6.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27455/27455 [00:06<00:00,  6.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27456/27456 [00:06<00:00,  6.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27457/27457 [00:06<00:00,  6.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27458/27458 [00:06<00:00,  6.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27459/27459 [00:05<00:00,  5.99s/trial, best loss: 1.0]\n",
      "100%|██████████| 27460/27460 [00:06<00:00,  6.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27461/27461 [00:07<00:00,  7.43s/trial, best loss: 1.0]\n",
      "100%|██████████| 27462/27462 [00:05<00:00,  5.97s/trial, best loss: 1.0]\n",
      "100%|██████████| 27463/27463 [00:06<00:00,  6.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27464/27464 [00:05<00:00,  5.98s/trial, best loss: 1.0]\n",
      "100%|██████████| 27465/27465 [00:06<00:00,  6.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27466/27466 [00:06<00:00,  6.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27467/27467 [00:06<00:00,  6.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 27468/27468 [00:06<00:00,  6.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 27469/27469 [00:06<00:00,  6.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 27470/27470 [00:06<00:00,  6.39s/trial, best loss: 1.0]\n",
      "100%|██████████| 27471/27471 [00:06<00:00,  6.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 27472/27472 [00:06<00:00,  6.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 27473/27473 [00:07<00:00,  7.65s/trial, best loss: 1.0]\n",
      "100%|██████████| 27474/27474 [00:06<00:00,  6.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 27475/27475 [00:06<00:00,  6.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 27476/27476 [00:06<00:00,  6.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 27477/27477 [00:06<00:00,  6.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 27478/27478 [00:06<00:00,  6.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 27479/27479 [00:06<00:00,  6.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 27480/27480 [00:06<00:00,  6.29s/trial, best loss: 1.0]\n",
      "100%|██████████| 27481/27481 [00:06<00:00,  6.27s/trial, best loss: 1.0]\n",
      "100%|██████████| 27482/27482 [00:06<00:00,  6.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 27483/27483 [00:06<00:00,  6.32s/trial, best loss: 1.0]\n",
      "100%|██████████| 27484/27484 [00:07<00:00,  7.56s/trial, best loss: 1.0]\n",
      "100%|██████████| 27485/27485 [00:06<00:00,  6.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 27486/27486 [00:06<00:00,  6.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 27487/27487 [00:06<00:00,  6.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 27488/27488 [00:06<00:00,  6.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 27489/27489 [00:06<00:00,  6.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 27490/27490 [00:06<00:00,  6.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 27491/27491 [00:06<00:00,  6.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 27492/27492 [00:06<00:00,  6.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 27493/27493 [00:06<00:00,  6.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 27494/27494 [00:06<00:00,  6.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 27495/27495 [00:07<00:00,  7.50s/trial, best loss: 1.0]\n",
      "100%|██████████| 27496/27496 [00:06<00:00,  6.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 27497/27497 [00:06<00:00,  6.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 27498/27498 [00:06<00:00,  6.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 27499/27499 [00:06<00:00,  6.23s/trial, best loss: 1.0]\n",
      "100%|██████████| 27500/27500 [00:05<00:00,  5.85s/trial, best loss: 1.0]\n",
      "100%|██████████| 27501/27501 [00:05<00:00,  5.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 27502/27502 [00:05<00:00,  5.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 27503/27503 [00:05<00:00,  5.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 27504/27504 [00:06<00:00,  6.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27505/27505 [00:05<00:00,  5.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 27506/27506 [00:05<00:00,  5.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27507/27507 [00:07<00:00,  7.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 27508/27508 [00:06<00:00,  6.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27509/27509 [00:06<00:00,  6.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27510/27510 [00:06<00:00,  6.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 27511/27511 [00:05<00:00,  5.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27512/27512 [00:05<00:00,  5.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27513/27513 [00:05<00:00,  5.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27514/27514 [00:05<00:00,  5.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 27515/27515 [00:05<00:00,  5.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27516/27516 [00:05<00:00,  5.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27517/27517 [00:05<00:00,  5.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 27518/27518 [00:06<00:00,  6.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27519/27519 [00:05<00:00,  5.48s/trial, best loss: 1.0]\n",
      "100%|██████████| 27520/27520 [00:05<00:00,  5.40s/trial, best loss: 1.0]\n",
      "100%|██████████| 27521/27521 [00:05<00:00,  5.53s/trial, best loss: 1.0]\n",
      "100%|██████████| 27522/27522 [00:05<00:00,  5.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27523/27523 [00:05<00:00,  5.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27524/27524 [00:05<00:00,  5.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 27525/27525 [00:05<00:00,  5.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27526/27526 [00:05<00:00,  5.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27527/27527 [00:05<00:00,  5.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27528/27528 [00:05<00:00,  5.08s/trial, best loss: 1.0]\n",
      "100%|██████████| 27529/27529 [00:06<00:00,  6.34s/trial, best loss: 1.0]\n",
      "100%|██████████| 27530/27530 [00:05<00:00,  5.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27531/27531 [00:05<00:00,  5.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27532/27532 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 27533/27533 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27534/27534 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27535/27535 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 27536/27536 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27537/27537 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27538/27538 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27539/27539 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27540/27540 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27541/27541 [00:04<00:00,  4.06s/trial, best loss: 1.0]\n",
      "100%|██████████| 27542/27542 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27543/27543 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27544/27544 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27545/27545 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27546/27546 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27547/27547 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 27548/27548 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27549/27549 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27550/27550 [00:02<00:00,  2.66s/trial, best loss: 1.0]\n",
      "100%|██████████| 27551/27551 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27552/27552 [00:04<00:00,  4.07s/trial, best loss: 1.0]\n",
      "100%|██████████| 27553/27553 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27554/27554 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27555/27555 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 27556/27556 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 27557/27557 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 27558/27558 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27559/27559 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27560/27560 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 27561/27561 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27562/27562 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27563/27563 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27564/27564 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27565/27565 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27566/27566 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 27567/27567 [00:02<00:00,  2.69s/trial, best loss: 1.0]\n",
      "100%|██████████| 27568/27568 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27569/27569 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27570/27570 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27571/27571 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27572/27572 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27573/27573 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27574/27574 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27575/27575 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27576/27576 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27577/27577 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27578/27578 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27579/27579 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27580/27580 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27581/27581 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27582/27582 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27583/27583 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27584/27584 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27585/27585 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27586/27586 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27587/27587 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27588/27588 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27589/27589 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27590/27590 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27591/27591 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27592/27592 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27593/27593 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27594/27594 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27595/27595 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27596/27596 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27597/27597 [00:04<00:00,  4.05s/trial, best loss: 1.0]\n",
      "100%|██████████| 27598/27598 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27599/27599 [00:02<00:00,  2.68s/trial, best loss: 1.0]\n",
      "100%|██████████| 27600/27600 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27601/27601 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27602/27602 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27603/27603 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27604/27604 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27605/27605 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27606/27606 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27607/27607 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 27608/27608 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27609/27609 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27610/27610 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27611/27611 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27612/27612 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27613/27613 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27614/27614 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27615/27615 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27616/27616 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27617/27617 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27618/27618 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27619/27619 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27620/27620 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 27621/27621 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27622/27622 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27623/27623 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 27624/27624 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27625/27625 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27626/27626 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27627/27627 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27628/27628 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27629/27629 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 27630/27630 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27631/27631 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 27632/27632 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27633/27633 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27634/27634 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27635/27635 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27636/27636 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27637/27637 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27638/27638 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27639/27639 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27640/27640 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27641/27641 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27642/27642 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27643/27643 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27644/27644 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27645/27645 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27646/27646 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27647/27647 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27648/27648 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27649/27649 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27650/27650 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27651/27651 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27652/27652 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27653/27653 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27654/27654 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 27655/27655 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27656/27656 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27657/27657 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27658/27658 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27659/27659 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27660/27660 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27661/27661 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27662/27662 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27663/27663 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27664/27664 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27665/27665 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 27666/27666 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27667/27667 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27668/27668 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27669/27669 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27670/27670 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27671/27671 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27672/27672 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27673/27673 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27674/27674 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27675/27675 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 27676/27676 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27677/27677 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27678/27678 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27679/27679 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27680/27680 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 27681/27681 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27682/27682 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27683/27683 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27684/27684 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27685/27685 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27686/27686 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27687/27687 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27688/27688 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27689/27689 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27690/27690 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27691/27691 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27692/27692 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 27693/27693 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27694/27694 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27695/27695 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27696/27696 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27697/27697 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27698/27698 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27699/27699 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 27700/27700 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27701/27701 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27702/27702 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27703/27703 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27704/27704 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 27705/27705 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27706/27706 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27707/27707 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27708/27708 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27709/27709 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27710/27710 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27711/27711 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27712/27712 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 27713/27713 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27714/27714 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27715/27715 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 27716/27716 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27717/27717 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27718/27718 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27719/27719 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27720/27720 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27721/27721 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27722/27722 [00:04<00:00,  4.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 27723/27723 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27724/27724 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27725/27725 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27726/27726 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27727/27727 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27728/27728 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27729/27729 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27730/27730 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27731/27731 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27732/27732 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27733/27733 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 27734/27734 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27735/27735 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27736/27736 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27737/27737 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27738/27738 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 27739/27739 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27740/27740 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27741/27741 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 27742/27742 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27743/27743 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27744/27744 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27745/27745 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27746/27746 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27747/27747 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27748/27748 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27749/27749 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27750/27750 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27751/27751 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27752/27752 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27753/27753 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27754/27754 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27755/27755 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27756/27756 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 27757/27757 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 27758/27758 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27759/27759 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27760/27760 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 27761/27761 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27762/27762 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 27763/27763 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27764/27764 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27765/27765 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27766/27766 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27767/27767 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27768/27768 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27769/27769 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27770/27770 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 27771/27771 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27772/27772 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 27773/27773 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27774/27774 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27775/27775 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27776/27776 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27777/27777 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27778/27778 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 27779/27779 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27780/27780 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27781/27781 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27782/27782 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27783/27783 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27784/27784 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27785/27785 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27786/27786 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 27787/27787 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27788/27788 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27789/27789 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27790/27790 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 27791/27791 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27792/27792 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27793/27793 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27794/27794 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27795/27795 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27796/27796 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27797/27797 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27798/27798 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27799/27799 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27800/27800 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27801/27801 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27802/27802 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27803/27803 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27804/27804 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27805/27805 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27806/27806 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27807/27807 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27808/27808 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27809/27809 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27810/27810 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27811/27811 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27812/27812 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27813/27813 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27814/27814 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 27815/27815 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27816/27816 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 27817/27817 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27818/27818 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27819/27819 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 27820/27820 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27821/27821 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27822/27822 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 27823/27823 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27824/27824 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 27825/27825 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27826/27826 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27827/27827 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27828/27828 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27829/27829 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27830/27830 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27831/27831 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27832/27832 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27833/27833 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27834/27834 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27835/27835 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 27836/27836 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27837/27837 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27838/27838 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27839/27839 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27840/27840 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27841/27841 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27842/27842 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27843/27843 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27844/27844 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27845/27845 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27846/27846 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27847/27847 [00:02<00:00,  2.70s/trial, best loss: 1.0]\n",
      "100%|██████████| 27848/27848 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27849/27849 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 27850/27850 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27851/27851 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27852/27852 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27853/27853 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27854/27854 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27855/27855 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27856/27856 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27857/27857 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27858/27858 [00:04<00:00,  4.18s/trial, best loss: 1.0]\n",
      "100%|██████████| 27859/27859 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27860/27860 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27861/27861 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27862/27862 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27863/27863 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27864/27864 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27865/27865 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27866/27866 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27867/27867 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27868/27868 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27869/27869 [00:04<00:00,  4.31s/trial, best loss: 1.0]\n",
      "100%|██████████| 27870/27870 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 27871/27871 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 27872/27872 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 27873/27873 [00:02<00:00,  2.93s/trial, best loss: 1.0]\n",
      "100%|██████████| 27874/27874 [00:03<00:00,  3.19s/trial, best loss: 1.0]\n",
      "100%|██████████| 27875/27875 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 27876/27876 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 27877/27877 [00:02<00:00,  2.94s/trial, best loss: 1.0]\n",
      "100%|██████████| 27878/27878 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 27879/27879 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 27880/27880 [00:02<00:00,  2.89s/trial, best loss: 1.0]\n",
      "100%|██████████| 27881/27881 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 27882/27882 [00:02<00:00,  2.88s/trial, best loss: 1.0]\n",
      "100%|██████████| 27883/27883 [00:02<00:00,  2.92s/trial, best loss: 1.0]\n",
      "100%|██████████| 27884/27884 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27885/27885 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27886/27886 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27887/27887 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27888/27888 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27889/27889 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27890/27890 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27891/27891 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27892/27892 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27893/27893 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27894/27894 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27895/27895 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 27896/27896 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27897/27897 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27898/27898 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27899/27899 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 27900/27900 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27901/27901 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27902/27902 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27903/27903 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 27904/27904 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27905/27905 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27906/27906 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 27907/27907 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27908/27908 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27909/27909 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27910/27910 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27911/27911 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27912/27912 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27913/27913 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 27914/27914 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27915/27915 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27916/27916 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27917/27917 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27918/27918 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27919/27919 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27920/27920 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27921/27921 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27922/27922 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27923/27923 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27924/27924 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27925/27925 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27926/27926 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 27927/27927 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27928/27928 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27929/27929 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27930/27930 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27931/27931 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 27932/27932 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27933/27933 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27934/27934 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27935/27935 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27936/27936 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27937/27937 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27938/27938 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27939/27939 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27940/27940 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27941/27941 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27942/27942 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27943/27943 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27944/27944 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27945/27945 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27946/27946 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27947/27947 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27948/27948 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27949/27949 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 27950/27950 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27951/27951 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27952/27952 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27953/27953 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27954/27954 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27955/27955 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27956/27956 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27957/27957 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27958/27958 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 27959/27959 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27960/27960 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 27961/27961 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 27962/27962 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27963/27963 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27964/27964 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27965/27965 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27966/27966 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27967/27967 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27968/27968 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27969/27969 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27970/27970 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27971/27971 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 27972/27972 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27973/27973 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27974/27974 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27975/27975 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27976/27976 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27977/27977 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27978/27978 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27979/27979 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27980/27980 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 27981/27981 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27982/27982 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 27983/27983 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 27984/27984 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27985/27985 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27986/27986 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27987/27987 [00:03<00:00,  3.04s/trial, best loss: 1.0]\n",
      "100%|██████████| 27988/27988 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27989/27989 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 27990/27990 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 27991/27991 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 27992/27992 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27993/27993 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27994/27994 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 27995/27995 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 27996/27996 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 27997/27997 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 27998/27998 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 27999/27999 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28000/28000 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28001/28001 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28002/28002 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28003/28003 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 28004/28004 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28005/28005 [00:04<00:00,  4.21s/trial, best loss: 1.0]\n",
      "100%|██████████| 28006/28006 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 28007/28007 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28008/28008 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28009/28009 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28010/28010 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28011/28011 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28012/28012 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28013/28013 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28014/28014 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28015/28015 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28016/28016 [00:04<00:00,  4.24s/trial, best loss: 1.0]\n",
      "100%|██████████| 28017/28017 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 28018/28018 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28019/28019 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 28020/28020 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 28021/28021 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28022/28022 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 28023/28023 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 28024/28024 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 28025/28025 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28026/28026 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 28027/28027 [00:04<00:00,  4.25s/trial, best loss: 1.0]\n",
      "100%|██████████| 28028/28028 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28029/28029 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 28030/28030 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28031/28031 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28032/28032 [00:02<00:00,  2.87s/trial, best loss: 1.0]\n",
      "100%|██████████| 28033/28033 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28034/28034 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28035/28035 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28036/28036 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28037/28037 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28038/28038 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 28039/28039 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 28040/28040 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 28041/28041 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28042/28042 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28043/28043 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28044/28044 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28045/28045 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28046/28046 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28047/28047 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28048/28048 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28049/28049 [00:04<00:00,  4.16s/trial, best loss: 1.0]\n",
      "100%|██████████| 28050/28050 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28051/28051 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28052/28052 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28053/28053 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 28054/28054 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28055/28055 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28056/28056 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28057/28057 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28058/28058 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28059/28059 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28060/28060 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28061/28061 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28062/28062 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28063/28063 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28064/28064 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28065/28065 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28066/28066 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28067/28067 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28068/28068 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28069/28069 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28070/28070 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28071/28071 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28072/28072 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 28073/28073 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28074/28074 [00:02<00:00,  2.90s/trial, best loss: 1.0]\n",
      "100%|██████████| 28075/28075 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28076/28076 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28077/28077 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28078/28078 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 28079/28079 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28080/28080 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28081/28081 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28082/28082 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28083/28083 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 28084/28084 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28085/28085 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 28086/28086 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28087/28087 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28088/28088 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28089/28089 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28090/28090 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28091/28091 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28092/28092 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28093/28093 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28094/28094 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28095/28095 [00:04<00:00,  4.14s/trial, best loss: 1.0]\n",
      "100%|██████████| 28096/28096 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 28097/28097 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28098/28098 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28099/28099 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28100/28100 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28101/28101 [00:02<00:00,  2.91s/trial, best loss: 1.0]\n",
      "100%|██████████| 28102/28102 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28103/28103 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28104/28104 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28105/28105 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28106/28106 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 28107/28107 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28108/28108 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28109/28109 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28110/28110 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28111/28111 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28112/28112 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28113/28113 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28114/28114 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28115/28115 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28116/28116 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28117/28117 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28118/28118 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 28119/28119 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28120/28120 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28121/28121 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28122/28122 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28123/28123 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28124/28124 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28125/28125 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28126/28126 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28127/28127 [00:02<00:00,  2.84s/trial, best loss: 1.0]\n",
      "100%|██████████| 28128/28128 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28129/28129 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 28130/28130 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28131/28131 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28132/28132 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28133/28133 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28134/28134 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28135/28135 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28136/28136 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28137/28137 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28138/28138 [00:02<00:00,  2.71s/trial, best loss: 1.0]\n",
      "100%|██████████| 28139/28139 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28140/28140 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28141/28141 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 28142/28142 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28143/28143 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28144/28144 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28145/28145 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 28146/28146 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28147/28147 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 28148/28148 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28149/28149 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28150/28150 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28151/28151 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28152/28152 [00:04<00:00,  4.15s/trial, best loss: 1.0]\n",
      "100%|██████████| 28153/28153 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28154/28154 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28155/28155 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28156/28156 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28157/28157 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28158/28158 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 28159/28159 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28160/28160 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 28161/28161 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28162/28162 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28163/28163 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28164/28164 [00:04<00:00,  4.09s/trial, best loss: 1.0]\n",
      "100%|██████████| 28165/28165 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28166/28166 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28167/28167 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28168/28168 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28169/28169 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 28170/28170 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28171/28171 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 28172/28172 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28173/28173 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28174/28174 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 28175/28175 [00:04<00:00,  4.10s/trial, best loss: 1.0]\n",
      "100%|██████████| 28176/28176 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28177/28177 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28178/28178 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28179/28179 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28180/28180 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28181/28181 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 28182/28182 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28183/28183 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28184/28184 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28185/28185 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 28186/28186 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28187/28187 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 28188/28188 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28189/28189 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28190/28190 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28191/28191 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28192/28192 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28193/28193 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28194/28194 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28195/28195 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28196/28196 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28197/28197 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28198/28198 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28199/28199 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28200/28200 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28201/28201 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28202/28202 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28203/28203 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28204/28204 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28205/28205 [00:02<00:00,  2.86s/trial, best loss: 1.0]\n",
      "100%|██████████| 28206/28206 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 28207/28207 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28208/28208 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28209/28209 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28210/28210 [00:04<00:00,  4.22s/trial, best loss: 1.0]\n",
      "100%|██████████| 28211/28211 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28212/28212 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28213/28213 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28214/28214 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 28215/28215 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28216/28216 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28217/28217 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 28218/28218 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28219/28219 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 28220/28220 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28221/28221 [00:04<00:00,  4.20s/trial, best loss: 1.0]\n",
      "100%|██████████| 28222/28222 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28223/28223 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 28224/28224 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28225/28225 [00:02<00:00,  2.83s/trial, best loss: 1.0]\n",
      "100%|██████████| 28226/28226 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28227/28227 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28228/28228 [00:02<00:00,  2.82s/trial, best loss: 1.0]\n",
      "100%|██████████| 28229/28229 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28230/28230 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28231/28231 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28232/28232 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28233/28233 [00:04<00:00,  4.12s/trial, best loss: 1.0]\n",
      "100%|██████████| 28234/28234 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28235/28235 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28236/28236 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28237/28237 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28238/28238 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28239/28239 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 28240/28240 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28241/28241 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28242/28242 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 28243/28243 [00:02<00:00,  2.79s/trial, best loss: 1.0]\n",
      "100%|██████████| 28244/28244 [00:04<00:00,  4.13s/trial, best loss: 1.0]\n",
      "100%|██████████| 28245/28245 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28246/28246 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 28247/28247 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28248/28248 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28249/28249 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28250/28250 [00:02<00:00,  2.75s/trial, best loss: 1.0]\n",
      "100%|██████████| 28251/28251 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28252/28252 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 28253/28253 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28254/28254 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28255/28255 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 28256/28256 [00:04<00:00,  4.11s/trial, best loss: 1.0]\n",
      "100%|██████████| 28257/28257 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28258/28258 [00:02<00:00,  2.74s/trial, best loss: 1.0]\n",
      "100%|██████████| 28259/28259 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 28260/28260 [00:02<00:00,  2.73s/trial, best loss: 1.0]\n",
      "100%|██████████| 28261/28261 [00:02<00:00,  2.78s/trial, best loss: 1.0]\n",
      "100%|██████████| 28262/28262 [00:02<00:00,  2.72s/trial, best loss: 1.0]\n",
      "100%|██████████| 28263/28263 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28264/28264 [00:02<00:00,  2.80s/trial, best loss: 1.0]\n",
      "100%|██████████| 28265/28265 [00:02<00:00,  2.76s/trial, best loss: 1.0]\n",
      "100%|██████████| 28266/28266 [00:02<00:00,  2.81s/trial, best loss: 1.0]\n",
      "100%|██████████| 28267/28267 [00:04<00:00,  4.17s/trial, best loss: 1.0]\n",
      "100%|██████████| 28268/28268 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|██████████| 28269/28269 [00:02<00:00,  2.77s/trial, best loss: 1.0]\n",
      "100%|█████████▉| 28269/28270 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    }
   ],
   "source": [
    "estim = HyperoptEstimator( classifier=clf,max_evals=10000000,\n",
    "                            algo=tpe.suggest,seed=1000)\n",
    "\n",
    "estim.fit( X1_train, y1_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c045cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save estime file as pickle\n",
    "import pickle\n",
    "filename = 'estim_modelNew.pkl'\n",
    "pickle.dump(estim, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "46839d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "{'learner': LogisticRegression(C=0.14599006477794893, max_iter=685, n_jobs=1,\n",
      "                   random_state=0, solver='newton-cg',\n",
      "                   tol=0.00031571236607489056), 'preprocs': (Normalizer(norm='l1'),), 'ex_preprocs': ()}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ej/.local/lib/python3.10/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but Normalizer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print( estim.score( X1_test, y1_test ) )\n",
    "# <<show score here>>\n",
    "print( estim.best_model() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76e794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 44.479391,
   "end_time": "2021-12-28T14:42:30.078407",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-28T14:41:45.599016",
   "version": "2.3.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
